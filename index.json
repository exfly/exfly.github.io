[{"categories":null,"content":" ExFly Keep It Simple \u0026 Stupid. ","date":"2014-10-01","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Description Focusing on computer technology, has basic operating system, computer network, data structure and algorithm knowledge, good at back-end, distributed, master certain data analysis knowledge, understand basic Linux server maintenance, understand the basic maintenance of the database, understand Basic network security, convinced that technology changes the world, and hopes to have a place in the computer industry. ","date":"2014-10-01","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Skills Golang \u003e Java \u003e Python \u003e others Distribution, Microservice Linux Docker DevOps k8s ","date":"2014-10-01","objectID":"/about/:2:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Project ","date":"2014-10-01","objectID":"/about/:3:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"newdb Implement RDBMS ","date":"2014-10-01","objectID":"/about/:3:1","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"wiki wor2vec ","date":"2014-10-01","objectID":"/about/:3:2","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"DistributedSystems-MIT6.824 ","date":"2014-10-01","objectID":"/about/:3:3","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"pubsub ","date":"2014-10-01","objectID":"/about/:3:4","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"gracefully ","date":"2014-10-01","objectID":"/about/:3:5","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"money ","date":"2014-10-01","objectID":"/about/:3:6","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"jsonrpc boilerplate ","date":"2014-10-01","objectID":"/about/:3:7","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"dockerc ","date":"2014-10-01","objectID":"/about/:3:8","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"container-lab ","date":"2014-10-01","objectID":"/about/:3:9","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"go-mod-helper ","date":"2014-10-01","objectID":"/about/:3:10","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"ucore 操作系统 ","date":"2014-10-01","objectID":"/about/:3:11","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"ghttp a http server written by pure c ","date":"2014-10-01","objectID":"/about/:3:12","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"awesome blog springboot-ssm ","date":"2014-10-01","objectID":"/about/:3:13","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"graphql-mongo-boilerplate ","date":"2014-10-01","objectID":"/about/:3:14","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"openapi boilerplate ","date":"2014-10-01","objectID":"/about/:3:15","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"go cache in mem ","date":"2014-10-01","objectID":"/about/:3:16","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"Books Computer Science from the Bottom Up 翻译 Computer Science from the Bottom Up 官网 Distributed Query Processing (DQP) Operating System Concepts Operating Systems: Three Easy Pieces R870: Unix System Administration - A Survival Course ","date":"2014-10-01","objectID":"/about/:4:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"文章简介：K3s 自带的 CA 有效期 10 年，其签发的 mTLS 证书只有 1 年，远不能满足“一次部署、长期免运维”的生产诉求。本文给出零侵入式方案，让 K3s 一次性使用 100 年有效期的 CA 与终端实体证书，真正做到“证书一生只装一次”。 ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:0:0","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"背景 K3s 默认证书策略 CA 有效期：10 年（写死，不可通过参数调整） Server / Client 证书：1 年（hard-coded 365 d，可通过环境变量微调） 对于边缘、离线或无人值守场景，1 年一换证书意味着巨大的运维风险。社区在 issue #3253 中早已反馈该痛点，官方脚本已提供“自定义 CA”能力，但未给出完整百年方案。 ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:1:0","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"整体思路 提前生成超长寿命 CA（100 年） K3s 启动时生成 CATTLE_NEW_SIGNED_CERT_EXPIRATION_DAYS 有效期的 server/client cert 同步调整 Kubernetes 控制平面 CSR 有效期（100 年） ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:2:0","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"步骤 1：生成 100 年 CA 及中间证书 k3s 使用 certutil.NewSelfSignedCACert 签发 CA 证书，有效期是写死的 10 年，这个值不是很生产可用。社区也遇到了这个问题，其中 CA 部分采用的解决办法是自签名一个自定义证书，脚本默认 20 年证书，这里我们改成 100 年： --- \"ansible/files/generate-custom-ca-certs copy.sh\" 2024-08-06 17:43:32.663442036 +0800 +++ ansible/files/generate-custom-ca-certs.sh 2024-08-06 17:53:07.285515777 +0800 @@ -30,6 +30,11 @@ PRODUCT=\"${PRODUCT:-k3s}\" DATA_DIR=\"${DATA_DIR:-/var/lib/rancher/${PRODUCT}}\" +if [[ -d \"${DATA_DIR}/server/tls\" ]]; then + echo \"SKIPING: ${DATA_DIR}/server/tls is exists\" + exit 0 +fi + if type -t openssl-3 \u0026\u003e/dev/null; then OPENSSL=openssl-3 else @@ -93,7 +98,7 @@ else echo \"Generating root certificate authority RSA key and certificate\" ${OPENSSL} genrsa ${OPENSSL_GENRSA_FLAGS:-} -out root-ca.key 4096 - ${OPENSSL} req -x509 -new -nodes -sha256 -days 7300 \\ + ${OPENSSL} req -x509 -new -nodes -sha256 -days 37000 \\ -subj \"/CN=${PRODUCT}-root-ca@${TIMESTAMP}\" \\ -key root-ca.key \\ -out root-ca.pem \\ @@ -116,7 +121,7 @@ ${OPENSSL} req -new -nodes \\ -subj \"/CN=${PRODUCT}-intermediate-ca@${TIMESTAMP}\" \\ -key intermediate-ca.key | - ${OPENSSL} ca -batch -notext -days 3700 \\ + ${OPENSSL} ca -batch -notext -days 37000 \\ -in /dev/stdin \\ -out intermediate-ca.pem \\ -keyfile root-ca.key \\ @@ -139,7 +144,7 @@ ${OPENSSL} req -new -nodes \\ -subj \"/CN=${CERT_NAME}@${TIMESTAMP}\" \\ -key ${TYPE}-ca.key | - ${OPENSSL} ca -batch -notext -days 3700 \\ + ${OPENSSL} ca -batch -notext -days 37000 \\ -in /dev/stdin \\ -out ${TYPE}-ca.pem \\ -keyfile intermediate-ca.key \\ 修改后执行即可： bash k3s-generate-custom-ca-certs.sh ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:2:1","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"步骤 2：调整 K3s Server / Client 证书有效期 k3s 创建 tls 证书，其 server 证书有效期是 365 天，可以通过环境变量 CATTLE_NEW_SIGNED_CERT_EXPIRATION_DAYS 修改为希望的值，单位天： echo 'CATTLE_NEW_SIGNED_CERT_EXPIRATION_DAYS=36500' \u003e\u003e /etc/systemd/system/k3s.service.env ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:2:2","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"步骤 3：让 Kubernetes CSR 也一次签 100 年 Kubernetes 控制平面自带的 kube-controller-manager 默认对 CSR 签发 1 年，可以覆盖这个配置 /etc/rancher/k3s/config.yaml： kube-controller-manager-arg: - cluster-signing-duration=876000h # 100year ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:2:3","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"最终验证 # 查看 CA 有效期 openssl x509 -in /var/lib/rancher/k3s/server/tls/root-ca.pem -noout -dates # 查看 server 证书有效期 openssl x509 -in /var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt -noout -dates # 查看 kubelet client 证书有效期 openssl x509 -in /var/lib/rancher/k3s/agent/client-kubelet.crt -noout -dates ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:2:4","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"结语 CA 100 年（脚本一次性生成，K3s 直接复用） Server / Client 证书 100 年（环境变量控制） Kubernetes CSR 100 年（controller-manager 参数控制） 至此，一次部署，证书终身免运维的 K3s 集群正式就绪。 ","date":"2025-07-27","objectID":"/k3s-100year-ca-certs/:3:0","tags":["k3s","devops","ops"],"title":"K3s 百年证书","uri":"/k3s-100year-ca-certs/"},{"categories":null,"content":"文章简介：每周六 11:00 定时出现的 IO 延迟，最终定位到 Raid 卡的后台一致性检查（CC \u0026 PR）。关闭后延迟消失，复盘记录在此。 机器机器 raid 信息 调整前磁盘 etcd 的 grep -E '(apply request took too lon)|(slow fdatasyn)' took 趋势： 客户现场的机器是浪潮的整机，信息如下： TODO: 确认下客户机器信息 根据 io 延迟的出现时间，判断每周六的 11:00 出现的定时任务； 业务层排查：无对应定时任务 宿主机排查：无异常进程 怀疑硬件与厂商确认：Raid 卡默认每周六 11:00 运行 Consistency Check（CC）与 Patrol Read（PR） 关闭后 etcd 的延迟几乎消失： 在内网找了 3 台同型号机器（Raid 卡：AVAGOMegaRAIDSAS9361-8i），400 GB SSD RAID 1 平均 Disk Average Wait Time： [root@TODO storcli]# ./storcli64 /c0 show ---------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------------------- 252:0 14 Onln 0 446.625 GB SATA SSD N N 512B SAMSUNG MZ7LH480HAHQ-00005 U - 252:1 15 Onln 0 446.625 GB SATA SSD N N 512B SAMSUNG MZ7LH480HAHQ-00005 U - 252:2 8 Onln 1 7.276 TB SATA HDD N N 512B ST8000NM000A-2KE101 U - 252:3 9 Onln 1 7.276 TB SATA HDD N N 512B ST8000NM000A-2KE101 U - 252:5 11 Onln 1 7.276 TB SATA HDD N N 512B ST8000NM000A-2KE101 U - 252:6 12 Onln 1 7.276 TB SATA HDD N N 512B ST8000NM000A-2KE101 U - 252:7 13 Onln 1 7.276 TB SATA HDD N N 512B ST8000NM000A-2KE101 U - ---------------------------------------------------------------------------------------- [root@TODO storcli]# ./storcli64 /c0 show cc CLI Version = 007.3404.0000.0000 April 18, 2025 Operating system = Linux 3.10.0-1160.el7.x86_64 Controller = 0 Status = Success Description = None Controller Properties : ===================== ----------------------------------------------- Ctrl_Prop Value ----------------------------------------------- CC Operation Mode Concurrent CC Execution Delay 168 hours CC Next Starttime 07/26/2025, 03:00:00 CC Current State Stopped CC Number of iterations 164 CC Number of VD completed 2 CC Excluded VDs None ----------------------------------------------- [root@TODO storcli]# ./storcli64 /c0 show pr CLI Version = 007.3404.0000.0000 April 18, 2025 Operating system = Linux 3.10.0-1160.el7.x86_64 Controller = 0 Status = Success Description = None Controller Properties : ===================== --------------------------------------------- Ctrl_Prop Value --------------------------------------------- PR Mode Auto PR Execution Delay 168 hours PR iterations completed 120 PR Next Start time 07/26/2025, 03:00:00 PR on SSD Disabled PR Current State Stopped PR Excluded VDs None PR MaxConcurrentPd 248 --------------------------------------------- ssd 上已经 disable 了 PR，只有 cc，看起来 cc 给磁盘带来了大约 5ms ± 15ms 左右的 iowait，而客户这边 io wait 跟比这要高非常多。 参考：浅谈 Raid 卡的两种一致性校验方式（PR\u0026CC） 找了硬件厂商的人，反馈回来： 因 CC 和 PR 校验会影响硬盘 IO，不同业务环境对延迟敏感程度不同，系统下受影响的现象不同，目前推荐如下： 1. CC 周期推荐 28 天执行一次，默认 Cc rate 为 30%，这个 30% 是一个大约的 CPU 时间占用比例，由这个 rate 决定多久进行一次 Cc 的后台动作，但是即使降到 0 也会占用一点 CPU，因为 CC 必须要做。（目前已调整至 5%，已经最低） 2. 如实际业务对延迟较敏感，请评估是否关闭 CC。关闭 CC 后，因 CC 带来的 IO 性能和延迟影响得到改善。建议参考 CC 对性能和数据安全的影响进行综合评估。关闭 CC 后，在遇到盘端有介质错误的时候，RAID 卡端会花费更多的时间来尝试读取数据并标记修复异常数据，可能造成延迟升高，严重情况下，如果同条带中多个数据错误可能会导致数据丢失。 接下来继续等待硬件厂商的 debug 结果吧。 ","date":"2025-07-20","objectID":"/raid-cc-causes-disk-io-lantency-increase/:0:0","tags":["k8s","debug","raid","io"],"title":"Raid 卡 Consistency Check 导致 io 延迟变高","uri":"/raid-cc-causes-disk-io-lantency-increase/"},{"categories":null,"content":"后续 等待厂商进一步 debug 对 etcd 场景而言，已有 snapshot + 12 h 备份，可直接关闭 CC 以换取确定性低延迟 是否可将 etcd 数据目录直接放 tmpfs？需评估内存容量与断电风险 ","date":"2025-07-20","objectID":"/raid-cc-causes-disk-io-lantency-increase/:1:0","tags":["k8s","debug","raid","io"],"title":"Raid 卡 Consistency Check 导致 io 延迟变高","uri":"/raid-cc-causes-disk-io-lantency-increase/"},{"categories":null,"content":"结论 Raid 卡不是免费的午餐，后台维护任务同样消耗资源。在延迟敏感型业务中，需要显式权衡数据安全与性能。 ","date":"2025-07-20","objectID":"/raid-cc-causes-disk-io-lantency-increase/:2:0","tags":["k8s","debug","raid","io"],"title":"Raid 卡 Consistency Check 导致 io 延迟变高","uri":"/raid-cc-causes-disk-io-lantency-increase/"},{"categories":null,"content":"文章简介：netstat -s 已经废弃，一些机器已经不内置，但 ss 和 nstat 是内置的。本文通过代码描述他俩之间迁移的方法。 netstat -s 读取 /proc/net/netstat，所有参数会由这个 table 对应起来，进而展示 右侧人可读信息。代码见 这里。 说结论吧： 例如： netstat -s | grep -i 'invalid SYN cookies received' 87441 invalid SYN cookies received 在 这里 搜索 invalid SYN cookies received 可以看到如下内容： {\"SyncookiesFailed\", N_(\"%llu invalid SYN cookies received\"), opt_number}, 可以看到 SyncookiesFailed，则对应的更现代的 nstat 对应起来就是： nstat -s -za | grep SyncookiesFailed TcpExtSyncookiesFailed 87441 0.0 结果 87441 可以对应上。 详细 table 见： https://github.com/ecki/net-tools/blob/master/statistics.c#L223 ","date":"2025-04-10","objectID":"/netstat-migrate-to-ss-nstat/:0:0","tags":["linux","ops","network"],"title":"Netstat Migrate to Ss Nstat","uri":"/netstat-migrate-to-ss-nstat/"},{"categories":null,"content":"nstat -s -za nstat -s -za 会直接根据 /proc/net/netstat 中的上下级关系直接拼接 比如： cat /proc/net/netstat TcpExt: SyncookiesSent SyncookiesRecv SyncookiesFailed TcpExt: 0 0 6248048 netstat -s | grep -i 'SYN cookies received' 6248058 invalid SYN cookies received nstat -s -za | grep SyncookiesFailed TcpExtSyncookiesFailed 6248058 0.0 其中 TcpExtSyncookiesFailed 部分恰好为 TcpExt+SyncookiesFailed 另外 /proc/net/netstat 对应的问题再这里 对应的内核代码在这里 ","date":"2025-04-10","objectID":"/netstat-migrate-to-ss-nstat/:1:0","tags":["linux","ops","network"],"title":"Netstat Migrate to Ss Nstat","uri":"/netstat-migrate-to-ss-nstat/"},{"categories":null,"content":"ref https://blog.csdn.net/yscisco/article/details/121443965 ","date":"2025-04-10","objectID":"/netstat-migrate-to-ss-nstat/:2:0","tags":["linux","ops","network"],"title":"Netstat Migrate to Ss Nstat","uri":"/netstat-migrate-to-ss-nstat/"},{"categories":null,"content":"文章简介：Gitea Earthly 构建时间从 1m30s 优化到 11s ","date":"2025-04-06","objectID":"/gitea-earthly-speedup-from-1m30s-to-11s/:0:0","tags":["homelab","docker","gitea","actions"],"title":"Gitea Earthly 构建时间从 1m30s 优化到 11s","uri":"/gitea-earthly-speedup-from-1m30s-to-11s/"},{"categories":null,"content":"背景 在我的 homelab server 上搭建了一套 gitea + actions cicd 平台，使用 docker + earthly 构建 golang 程序。workflow 如下： name: Gitea Actions Demo run-name: ${{ gitea.actor }} is testing out Gitea Actions 🚀 on: push: schedule: - cron: '30 5 * * */2' # 每两天 jobs: Explore-Gitea-Actions: runs-on: ubuntu-latest steps: - run: echo \"🎉 The job was automatically triggered by a ${{ gitea.event_name }} event.\" - run: echo \"🐧 This job is now running on a ${{ runner.os }} server hosted by Gitea!\" - run: echo \"🔎 The name of your branch is ${{ gitea.ref }} and your repository is ${{ gitea.repository }}.\" - name: Check out repository code # uses: actions/checkout@v3 uses: https://gitea.com/actions/checkout@v4 with: fetch-depth: 0 - name: install docker client run: | sed -E -i -e 's/(archive|ports|security).ubuntu.com/mirrors.dev.domain/g' /etc/apt/sources.list.d/ubuntu.sources apt update \u0026\u0026 apt install sudo sh .gitea/workflows/get-docker.sh --mirror Exfly ls -alh /var/run/ # truenas 中不支持 安装 qemu，故限制了 crosscompile # - name: Set up QEMU # id: qemu # # docker/setup-qemu-action@v1 # uses: https://gitee.com/exfly/setupqemuaction@v1 # with: # image: docker.m.daocloud.io/tonistiigi/binfmt:latest # platforms: all # - run: /bin/sh -c \"wget https://mirror.ghproxy.com/https://github.com/earthly/earthly/releases/latest/download/earthly-linux-amd64 -O /usr/local/bin/earthly \u0026\u0026 chmod +x /usr/local/bin/earthly \u0026\u0026 /usr/local/bin/earthly bootstrap --with-autocomplete\" - uses: https://gitee.com/exfly/earthly-actions-setup@main with: version: \"v0.8.1\" # version: v0.7.2 - name: Earthly version run: earthly --version - name: Run build run: earthly -V --buildkit-image=docker.1ms.run/earthly/buildkitd:v0.8.3 +build - run: echo \"💡 The ${{ gitea.repository }} repository has been cloned to the runner.\" - run: echo \"🖥️ The workflow is now ready to test your code on the runner.\" - name: List files in the repository run: | ls ${{ gitea.workspace }} - run: echo \"🍏 This job's status is ${{ gitea.status }}.\" 分析下 docker 的安装占用了大量时间；另一部分由于 earthly-actions-setup 从 github 中下载 earthly，众所周知的原因，非常不稳定，还好有正常可以访问时候的 cache，消耗时间比较短。 可以基于 ubuntu-latest 添加 earthly + docker-client，看看效果： ","date":"2025-04-06","objectID":"/gitea-earthly-speedup-from-1m30s-to-11s/:1:0","tags":["homelab","docker","gitea","actions"],"title":"Gitea Earthly 构建时间从 1m30s 优化到 11s","uri":"/gitea-earthly-speedup-from-1m30s-to-11s/"},{"categories":null,"content":"总结 ci 的优化空间从几分钟已经优化到 11s，方法就是将实时安装依赖转为放到基础镜像中。 ","date":"2025-04-06","objectID":"/gitea-earthly-speedup-from-1m30s-to-11s/:2:0","tags":["homelab","docker","gitea","actions"],"title":"Gitea Earthly 构建时间从 1m30s 优化到 11s","uri":"/gitea-earthly-speedup-from-1m30s-to-11s/"},{"categories":null,"content":"文章简介：通过实验，探索 BDP、tcp buffer、RT 三者之间的关系；探讨如何控制吞吐；实验 curl 限速是如何工作的； 按照：TCP 性能和发送接收窗口、Buffer 的关系 | plantegg 实验调整 RT、控制死内核 tcp buffer，让速度随着 RT 增加慢下来。实验大致过程是先创建一个比较大的文件，使用 python 开一个 http server，调整 tcp 的配置信息，在另一台机器上使用 curl 下载这个大文件，使用 tcpdump 抓包后使用 wireshark 分析结果。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:0:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"搭建环境 我的 Macbook M1 下使用 vagrant + vmware 创建虚拟机如下： # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| (1..2).each do |i| #定义节点变量 config.vm.define \"node#{i}\" do |node| # box 配置 node.vm.box = \"bento/rockylinux-9\" # 设置虚拟机的主机名 node.vm.hostname = \"node#{i}\" # 设置虚拟机的 IP # node.vm.network \"private_network\", ip: \"192.168.60.#{10+i}\" node.vm.synced_folder \".\", \"/vagrant\", disabled: true # vagrant rsync-auto --debug # https://developer.hashicorp.com/vagrant/docs/synced-folders/rsync node.vm.synced_folder \".\", \"/vagrant\", type: \"rsync\", rsync__exclude: [\".git/\", \"docs/\", \".vagrant/\"], rsync__args: [\"--verbose\", \"--rsync-path='sudo rsync'\", \"--archive\", \"-z\"] node.vm.provider :vmware_desktop do |vmware| vmware.vmx[\"ethernet0.pcislotnumber\"] = \"160\" end # 设置主机与虚拟机的共享目录 # VirtaulBox 相关配置 node.vm.provision \"shell\", inline: \u003c\u003c-SHELL echo \"hello\" sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.ustc.edu.cn/rocky|g' \\ -i.bak \\ /etc/yum.repos.d/rocky-extras.repo \\ /etc/yum.repos.d/rocky.repo yum install -y yum-utils yum-config-manager --add-repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo sed -i 's+https://download.docker.com+https://mirrors.ustc.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo dnf makecache # dnf install -y epel-release \u0026\u0026 dnf makecache yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin tmux nc nmap systemctl enable --now docker # https://github.com/iovisor/bcc/blob/master/INSTALL.md#rhel---binary # export PATH=\"/usr/share/bcc/tools:$PATH\" yum install -y bcc-tools # https://tshark.dev/setup/install/ yum install -y tcpdump net-tools gdb dstat zip wireshark-cli SHELL end # end config.vm.define node end # end each 3 node end 共创建了两个 vm，一个是 VM1 和 VM2 export VM1=192.168.245.151 export VM2=192.168.245.152 # 生成 2g 的文件 test.txt fallocate -l 2G test.txt # 生成 2g 文件，ls \u0026 du 都是 2g python3 -m http.server 8089 # 在 VM2 中访问 vm 的这个文件 curl -v --output /tmp/test.txt http://{VM1}:8089/test.txt 当前的 sysctl 参数如下： sysctl -a | egrep \"rmem|wmem|tcp_mem|adv_win|moderate\" net.core.rmem_default = 212992 net.core.rmem_max = 212992 net.core.wmem_default = 212992 net.core.wmem_max = 212992 net.ipv4.tcp_adv_win_scale = 1 net.ipv4.tcp_mem = 18852 25136 37704 net.ipv4.tcp_moderate_rcvbuf = 1 net.ipv4.tcp_rmem = 4096 131072 6291456 net.ipv4.tcp_wmem = 4096 4096 4096 net.ipv4.udp_rmem_min = 4096 net.ipv4.udp_wmem_min = 4096 vm.lowmem_reserve_ratio = 256 256 32 0 0 下面的实现会调整 rmem 和 wmem 的大小做相关的实验 先整体看下实验的结果： group rmem(bytes) window(bytes) rtt(ms) throughput(KB/s) dup ack(%) retransmission(%) outoforder(%) packetcount benchmark 4096 1040 2 3,079 0 0 0 66086 10ms 4096 1040 10 108 0 0 0 2239 20ms 4096 1040 20 58 0 0 0 1263 40ms 4096 1040 40 30 0 0 0 644 80ms 4096 1040 80 15 0 0 0 348 100ms 4096 1040 100 12 0 0 0 283 1%lose 4096 1040 2 1651 0.63% 0.50% 0.12% 36264 10%lose 4096 1040 2 202 250/5057=5.00% 97/5057=1.90% 152/5057=3.00% 5057 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:1:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"实验 1 固定 rmem 和 wmem，增加延迟 sysctl -w \"net.ipv4.tcp_rmem=4096 4096 4096\" //最小值 默认值 最大值 sysctl -w \"net.ipv4.tcp_wmem=4096 4096 4096\" ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:2:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"控制 rmem 和 wmem 在 4096，无延迟 吞吐为：2.110**7 bits/s = python3 -c ‘print(2.110**7/8/1024)’ = 2563.48 KB/s TODO: 这个值跟 wireshark Capture File Properties 显示的 3,079KB/s 有出入，需要看看 RT 非常小 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:2:1","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"现在增加延迟到 10ms tc qdisc add dev eth0 root netem delay 10ms [root@node2 vagrant]# ping $VM1 PING 192.168.245.151 (192.168.245.151) 56(84) bytes of data. 64 bytes from 192.168.245.151: icmp_seq=1 ttl=64 time=24.5 ms 64 bytes from 192.168.245.151: icmp_seq=2 ttl=64 time=11.9 ms 64 bytes from 192.168.245.151: icmp_seq=3 ttl=64 time=10.7 ms 64 bytes from 192.168.245.151: icmp_seq=4 ttl=64 time=11.7 ms 64 bytes from 192.168.245.151: icmp_seq=5 ttl=64 time=11.4 ms curl -v --output test.txt http://$VM1:8089/test.txt 92104bytes/s 每个台阶的时间大概是 10ms，说明 10ms 生效了 可以看到吞吐不到 700000bits/s = python3 -c ‘print(700000/8/1024)’ = 85.45 KB/s 看报文中 Calculated window size: 1040 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:2:2","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"小结 固定 rmem、wmem，随着延迟的增加，吞吐也随之降低。 做了几个实验，分别是 10ms、20ms、30ms、40ms，可以看到发包规律，假设发包大小及延迟的固定的（即排除网络波动），可以得到数学公式，使用 python 实现为： '''window 单位 bytes, rtt 单位 ms''' def throughput(window: int, rtt: int): return window * (1000/rtt)/1024 如在 10ms、window size 1040bytes 下的吞吐为 print(throughput(1040, 10)) 101.5625 101 相较 85.45 有一些误差，网络传输是动态变化的，故会有差距；可以使用此公式估算网络吞吐。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:2:3","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"实验 2 固定 rmem 和 wmem，增加丢包率 seq 曲线非常弯，发包不稳定 丢包会让吞吐变的不稳定 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:3:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"小结 丢包会出现重传，重传很慢，吞吐波动比较大。随着丢包率的提高，波动会进一步变大，吞吐在减小。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:3:1","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"实验 3 分别设置 rmem 和 wmem TODO: 抓包试一下 可以看到对 client 的 rmem 限制的时候，会出现大量的 TCP Window Full，而对 server 的 wmem 限制则不会有非常大的影响。client rmem 读取后，需要一个 rtt 时间才能通知到 server 可以发送，故对吞吐的影响更大；而 server wmem 使用完后，只需等待发送完重新申请内存即可，延迟为内存延迟，对吞吐影响小。 如下图，可以更直观的展示 rmem/wmem 对吞吐影响的原因。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:4:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"实验 4 curl 限速 基于版本 8.12.1 核心数据结构、关键函数、算法逻辑、调用流程 4 个方面分析 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"数据结构 struct pgrs_measure { struct curltime start; /* when measure started */ curl_off_t start_size; /* the 'cur_size' the measure started at */ }; struct pgrs_dir { curl_off_t total_size; /* total expected bytes */ curl_off_t cur_size; /* transferred bytes so far */ curl_off_t speed; /* bytes per second transferred */ struct pgrs_measure limit; }; ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:1","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"关键代码 Curl_pgrsStartNow 函数初始化限速相关的初始状态 void Curl_pgrsStartNow(struct Curl_easy *data) { data-\u003eprogress.speeder_c = 0; /* reset the progress meter display */ data-\u003eprogress.start = Curl_now(); data-\u003eprogress.is_t_startransfer_set = FALSE; data-\u003eprogress.ul.limit.start = data-\u003eprogress.start; data-\u003eprogress.dl.limit.start = data-\u003eprogress.start; data-\u003eprogress.ul.limit.start_size = 0; data-\u003eprogress.dl.limit.start_size = 0; data-\u003eprogress.dl.cur_size = 0; data-\u003eprogress.ul.cur_size = 0; /* clear all bits except HIDE and HEADERS_OUT */ data-\u003eprogress.flags \u0026= PGRS_HIDE|PGRS_HEADERS_OUT; Curl_ratelimit(data, data-\u003eprogress.start); } Curl_pgrsLimitWaitTime 计算在需要等待多少毫秒才能回到速度限制以下 /* * This is used to handle speed limits, calculating how many milliseconds to * wait until we are back under the speed limit, if needed. * * The way it works is by having a \"starting point\" (time \u0026 amount of data * transferred by then) used in the speed computation, to be used instead of * the start of the transfer. This starting point is regularly moved as * transfer goes on, to keep getting accurate values (instead of average over * the entire transfer). * * This function takes the current amount of data transferred, the amount at * the starting point, the limit (in bytes/s), the time of the starting point * and the current time. * * Returns 0 if no waiting is needed or when no waiting is needed but the * starting point should be reset (to current); or the number of milliseconds * to wait to get back under the speed limit. */ timediff_t Curl_pgrsLimitWaitTime(struct pgrs_dir *d, curl_off_t speed_limit, struct curltime now) { curl_off_t size = d-\u003ecur_size - d-\u003elimit.start_size; timediff_t minimum; timediff_t actual; if(!speed_limit || !size) return 0; /* * 'minimum' is the number of milliseconds 'size' should take to download to * stay below 'limit'. */ if(size \u003c CURL_OFF_T_MAX/1000) minimum = (timediff_t) (CURL_OFF_T_C(1000) * size / speed_limit); else { minimum = (timediff_t) (size / speed_limit); if(minimum \u003c TIMEDIFF_T_MAX/1000) minimum *= 1000; else minimum = TIMEDIFF_T_MAX; } /* * 'actual' is the time in milliseconds it took to actually download the * last 'size' bytes. */ actual = Curl_timediff_ceil(now, d-\u003elimit.start); if(actual \u003c minimum) { /* if it downloaded the data faster than the limit, make it wait the difference */ return minimum - actual; } return 0; } /* * Update the timestamp and sizestamp to use for rate limit calculations. */ void Curl_ratelimit(struct Curl_easy *data, struct curltime now) { /* do not set a new stamp unless the time since last update is long enough */ if(data-\u003eset.max_recv_speed) { if(Curl_timediff(now, data-\u003eprogress.dl.limit.start) \u003e= MIN_RATE_LIMIT_PERIOD) { data-\u003eprogress.dl.limit.start = now; data-\u003eprogress.dl.limit.start_size = data-\u003eprogress.dl.cur_size; } } if(data-\u003eset.max_send_speed) { if(Curl_timediff(now, data-\u003eprogress.ul.limit.start) \u003e= MIN_RATE_LIMIT_PERIOD) { data-\u003eprogress.ul.limit.start = now; data-\u003eprogress.ul.limit.start_size = data-\u003eprogress.ul.cur_size; } } } state_performing 实际执行读写 socket，到达限速要求，跳转到限速 static CURLMcode state_performing(struct Curl_easy *data, struct curltime *nowp, bool *stream_errorp, CURLcode *resultp) { char *newurl = NULL; bool retry = FALSE; timediff_t recv_timeout_ms = 0; timediff_t send_timeout_ms = 0; CURLMcode rc = CURLM_OK; CURLcode result = *resultp = CURLE_OK; *stream_errorp = FALSE; /* check if over send speed */ if(data-\u003eset.max_send_speed) send_timeout_ms = Curl_pgrsLimitWaitTime(\u0026data-\u003eprogress.ul, data-\u003eset.max_send_speed, *nowp); /* check if over recv speed */ if(data-\u003eset.max_recv_speed) recv_timeout_ms = Curl_pgrsLimitWaitTime(\u0026data-\u003eprogress.dl, data-\u003eset.max_recv_speed, *nowp); if(send_timeout_ms || recv_timeout_ms) { Curl_ratelimit(data, *nowp); multistate(data, MSTATE_RATELIMITING); if(send_timeout_ms \u003e= recv_timeout_ms) Curl_expire(data, send_timeout_ms, EXPIRE_TOOFAST); else Curl_expire(data, recv_timeout_ms, EXPIRE_TOOFAST); return CURLM_OK; } /* read/wr","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:2","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"算法逻辑 curl 的限速使用滑动窗口算法实现。Curl_pgrsStartNow 初始化滑动窗口相关算法。Curl_pgrsLimitWaitTime 计算了需要等待多久可以达到预期的限速。实际执行过程中 (state_performing) 计算是否需要限速，当需要限速时等待 Curl_pgrsLimitWaitTime 返回的预期等待时间。然后恢复读写 socket。 大概的伪代码如下： limit = \"10kb/s\" Curl_pgrsStartNow(...) for { if limitWait := Curl_pgrsLimitWaitTime(...); limitWait \u003e 0 { sleep(limitWait) } state_performing(...) } 全局搜索 SO_RCVBUF 未找到，可以说明 curl 未使用设置发送/接收窗口的方式做限速。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:3","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"实际抓包看看 执行如下命令抓包看看实际的限速行为是什么样的： curl http://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/24.04.2/ubuntu-24.04.2-live-server-amd64.iso -k -v --output t.iso --limit-rate 200K 开始瞬间的吞吐非常高 4638 KB/s，后续几个峰值比较高，1220 KB/s 到 2441KB/s；整体平均下来 233 KB/s，瞬时吞吐非常高 4638 KB/s。进一步证实，没有通过设置 SO_RCVBUF/SO_SNDBUF 控制发送/接收窗口的方式控制网速。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:4","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"调用流程 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:5","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"自己实现一个类似的限速 package main import ( \"context\" \"fmt\" \"io\" \"net\" \"net/http\" \"os\" \"time\" ) const ( BufferSize = 1024 ) // GOARCH=arm64 GOOS=linux go build -o golimitclient labs/network/BDP-buffer-RT/golimitclient/main.go func main() { tcpdialer := \u0026net.Dialer{ Timeout: 30 * time.Second, KeepAlive: 30 * time.Second, } transport := \u0026http.Transport{ Proxy: http.ProxyFromEnvironment, DialContext: func(ctx context.Context, network, addr string) (net.Conn, error) { ret, err := tcpdialer.DialContext(ctx, network, addr) if err != nil { return nil, err } tcpConn, ok := ret.(*net.TCPConn) if ok { err := tcpConn.SetReadBuffer(BufferSize) if err != nil { return nil, err } } return ret, err }, ForceAttemptHTTP2: true, MaxIdleConns: 100, IdleConnTimeout: 90 * time.Second, TLSHandshakeTimeout: 10 * time.Second, ExpectContinueTimeout: 1 * time.Second, // ReadBufferSize: BufferSize, } client := \u0026http.Client{Transport: transport} response, err := client.Get(fmt.Sprintf(\"http://%s:8089/test.txt\", os.Getenv(\"VM1\"))) check(err) defer response.Body.Close() var readTimes int64 = 2000 var readTimesPerSecond int64 = 20 _ = readTimesPerSecond for i := 0; i \u003c int(readTimes); i++ { io.CopyN(io.Discard, response.Body, BufferSize) println(i) time.Sleep(time.Second / time.Duration(readTimesPerSecond)) } } func check(err error) { if err != nil { panic(err) } } 如图，基本可以限速限制在 20k 左右。 在实践中发现，设置 tcp readbuf（tcpConn.SetReadBuffer(BufferSize)）是有作用的，不设置最终也会吞吐收敛到 20k 的限速。 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:5:6","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"结论 RT 越大，速度越慢 丢包越高，速度越慢 curl 的限速是通过动态限制某段时间内读取到最多多少数据达到限速的效果 ","date":"2025-02-23","objectID":"/the-relationship-between-bdp-buffer-rt/:6:0","tags":["linux","curl","network","实验"],"title":"通过实验，探索 BDP、tcp buffer、RT 三者之间的关系","uri":"/the-relationship-between-bdp-buffer-rt/"},{"categories":null,"content":"文章简介：一次业务逻辑问题导致的 kafka 连接泄露问题排查 生产出了问题，再一次被拉来救场。 问题现象：golang 程序所有使用 kafka 连接的地方报错 cannot assign requested address，业务不可用。 错误的含意是客户端无法申请新的端口。 先看 pedestal 底层连接数： netstat -nt | grep 9092 结果非常多，命令执行了很久，疯狂刷各种 kafka 连接信息。对于 kafka 来说是不太合理的，看起来是一次 kafka 连接泄露问题，即业务疯狂创建 kafka 连接，但未关闭连接。 先看看调用栈： grep 'sarama@v1.38.1/client.go:209' 'goroutine' | wc -l 23311 看起来有很多 sarama kafka 后台 goroutine 在跑，也印证了有连接泄露。 然后看 profile 也印证了同样的结论。 但是为什么会这样呢？kafka 函数调用会创建 goroutine 后退出，调用栈不携带调用方调用栈，所以无法在 profile 或者 goroutine 中看到实际有问题的代码逻辑。 到此问题卡住了。 尝试看看内存分配是否支持，确认哪里的内存分配对象个数有问题呢。 heap： 可以看到堆调用了 InitClient，这个函数中调用了很多的 sarama.NewClient，暂时猜测是这个代码的问题。 详细看过代码后发现类似这种逻辑： func (s *service) InitClient() error { s.producer, err := NewProducer() if err != nil { return err } err = doSomeInit() if err != nil { return err } return nil } func do() { s := newService() err := s.InitClient() if err != nil { return } defer s.producer.Close() } 正常情况下 InitClient 不会报错，defer 逻辑会被执行，producer 会被正确的关闭。当 InitClient 的 doSomeInit 失败后，do 函数会没有执行 s.producer.Close() 直接退出了，导致连接泄露了。 修复方法也比较朴素 func (s *service) InitClient() error { s.producer, err := NewProducer() if err != nil { return err } err = doSomeInit() if err != nil { + _ = s.producer.Close() return err } return nil } func do() { s := newService() err := s.InitClient() if err != nil { return } defer s.producer.Close() } ","date":"2025-02-07","objectID":"/connect-kafka-failed-with-cannot-assign-requested-address/:0:0","tags":["debug","golang","linux","tcp","kafka"],"title":"connect kafka failed with cannot assign requested address","uri":"/connect-kafka-failed-with-cannot-assign-requested-address/"},{"categories":null,"content":"总结 问题现象非常难对应到代码上，需要不断探索，解决的过程中一度想要放弃了。还好，再一次发现了问题的原因，并解决了。 ","date":"2025-02-07","objectID":"/connect-kafka-failed-with-cannot-assign-requested-address/:1:0","tags":["debug","golang","linux","tcp","kafka"],"title":"connect kafka failed with cannot assign requested address","uri":"/connect-kafka-failed-with-cannot-assign-requested-address/"},{"categories":null,"content":"文章简介：程序因为 tcp window full hang 住的一次排查过程 ","date":"2024-12-29","objectID":"/why-golang-prog-hang-tcp-window-full/:0:0","tags":["debug","golang","linux","tcp"],"title":"程序因为 tcp window full hang 住的一次排查过程","uri":"/why-golang-prog-hang-tcp-window-full/"},{"categories":null,"content":"背景 生产出了问题，被拉来救场。 业务流程是 server1 会从 kafka1 中消费数据，重新整理数据后通过 tcp 发送给 logstash，logstash 再将数据发送给下游的 kafka2；为了确保不丢数据，logstach 开了某种幂等插件。 问题现象是，kafka1 中有 1000 条数据，tcp 确认发送成功 500 条，剩下的 500 条数据未发送或者发送失败了；kafka2 中未收到数据； ","date":"2024-12-29","objectID":"/why-golang-prog-hang-tcp-window-full/:1:0","tags":["debug","golang","linux","tcp"],"title":"程序因为 tcp window full hang 住的一次排查过程","uri":"/why-golang-prog-hang-tcp-window-full/"},{"categories":null,"content":"debug 过程 首先从源头看 server 1 中 tcp 是未发送还是发送失败了。从日志中未发现发送失败的日志。 从程序中的 goroutine 调用栈可以确认，程序卡在 syscall write 27 minutes，怀疑 tcp 连接出了问题； 在 logstash 这个机器上 ss 查看连接状态，可以看到，logstash 没有将 socket 中的数据 copy 到用户空间。 Recv-Q: The count of bytes not copied by the user program connected to this socket. man 8 netstat 查看 logstash 发现发送到 kafka 失败了： 看下抓包，可以看到再 keepalive + zerowindow，确认为 tcp read buf 已经满了，tcp 无法继续发送新的数据，golang 的 socket.write 只能死等。 ","date":"2024-12-29","objectID":"/why-golang-prog-hang-tcp-window-full/:2:0","tags":["debug","golang","linux","tcp"],"title":"程序因为 tcp window full hang 住的一次排查过程","uri":"/why-golang-prog-hang-tcp-window-full/"},{"categories":null,"content":"解决 修复 kafka TopicAuthorizationException: not authorized to aceess topics, 业务即恢复。 ","date":"2024-12-29","objectID":"/why-golang-prog-hang-tcp-window-full/:3:0","tags":["debug","golang","linux","tcp"],"title":"程序因为 tcp window full hang 住的一次排查过程","uri":"/why-golang-prog-hang-tcp-window-full/"},{"categories":null,"content":"综上 tcp write 的成功，不意味着数据真的被接收到，如果业务需要确保下游被接收到，需要有业务 ack 机制； 改进方案可以考虑添加一个 kafka topic 或者中间添加一个 mq，整体数据流变更为 kafka1 -\u003e server1 -\u003e mq -\u003e kafka2 ","date":"2024-12-29","objectID":"/why-golang-prog-hang-tcp-window-full/:4:0","tags":["debug","golang","linux","tcp"],"title":"程序因为 tcp window full hang 住的一次排查过程","uri":"/why-golang-prog-hang-tcp-window-full/"},{"categories":null,"content":"文章简介：Mac m1 机器上安装的 rust 编译运行程序遇到问题 zsh: killed ./main，问题解决 在 Mac M1 机器上安装完 rust 后运行程序，报错 zsh: killed ./main。 先看下我的环境 fastfetch ..' xxx@hostname ,xNMM. -------------------- .OMMMMo OS: macOS Ventura 13.4 arm64 lMM\" Host: MacBook Pro (14-inch, 2021) .;loddo:. .olloddol;. Kernel: Darwin 22.5.0 cKMMMMMMMMMMNWMMMMMMMMMM0: Uptime: 22 days, 22 hours, 40 mins .KMMMMMMMMMMMMMMMMMMMMMMMWd. Packages: 220 (nix-system), 45 (nix-default), 158 (brew), 48 (brew-cask) XMMMMMMMMMMMMMMMMMMMMMMMX. Shell: zsh 5.9 ;MMMMMMMMMMMMMMMMMMMMMMMM: Display (Color LCD): 3024x1964 @ 120 Hz (as 1512x982) in 14\" [Built-in] :MMMMMMMMMMMMMMMMMMMMMMMM: DE: Aqua .MMMMMMMMMMMMMMMMMMMMMMMMX. WM: Quartz Compositor kMMMMMMMMMMMMMMMMMMMMMMMMWd. WM Theme: Multicolor (Dark) 'XMMMMMMMMMMMMMMMMMMMMMMMMMMk Font: .AppleSystemUIFont [System], Helvetica [User] 'XMMMMMMMMMMMMMMMMMMMMMMMMK. Cursor: Fill - Black, Outline - White (32px) kMMMMMMMMMMMMMMMMMMMMMMd Terminal: tmux 3.4 ;KMMMMMMMWXXWMMMMMMMk. CPU: Apple M1 Pro (8) @ 3.23 GHz \"cooc*\" \"*coo'\" GPU: Apple M1 Pro (14) @ 1.30 GHz [Integrated] Memory: 13.42 GiB / 16.00 GiB (84%) Swap: 17.39 GiB / 18.00 GiB (97%) Disk (/): 425.64 GiB / 460.43 GiB (92%) - apfs [Read-only] Disk (/Volumes/code): 425.64 GiB / 460.43 GiB (92%) - apfs Local IP (en0): 192.167.1.4/24 Battery (bq40z651): 84% (3 hours, 52 mins remaining) [Discharging] Locale: en_US.UTF-8 rustc --version -v rustc 1.83.0 (90b35a623 2024-11-26) binary: rustc commit-hash: 90b35a6239c3d8bdabc530a6a0816f7ff89a0aaf commit-date: 2024-11-26 host: aarch64-apple-darwin release: 1.83.0 LLVM version: 19.1.1 执行如下代码会报错 cat \u003c\u003cEOF \u003e main.rs fn main() { println!(\"Hello, world!\"); } EOF rm -rf main \u0026\u0026 rustc main.rs \u0026\u0026 ./main \u0026\u0026 rm -rf main.rs main zsh: killed ./main 开了新的 shell 中运行则正常输出。 Hello, world! 坏的程序在好的 shell 中也报相同的错误。 多次尝试后发现新开的 zsh 下正常，tmux 下的 zsh 中有问题，开始怀疑是否是环境变量的问题。diff 了两个环境的环境变量如下： --- 1good.env 2024-12-13 21:56:57.329383153 +0800 +++ 1bad.env 2024-12-13 21:56:45.666162367 +0800 @@ -1,19 +1,29 @@ +AR= +AS= AUTO_NOTIFY_EXPIRE_TIME=8000 AUTO_NOTIFY_THRESHOLD=10 AUTO_NOTIFY_VERSION=0.10.0 +CC= CLICOLOR=true COLORTERM=truecolor COMMAND_MODE=unix2003 CONDA_PROMPT_MODIFIER=false +CONFIG_SHELL= +CXX= +DIRENV_DIFF= +DIRENV_DIR= +DIRENV_FILE= +DIRENV_WATCHES= EDITOR=vim +HOST_PATH= +IN_NIX_SHELL= JENV_LOADED=1 JENV_SHELL=zsh +LANG=en_US.UTF-8 LC_CTYPE=UTF-8 +LD= +LD_DYLD_PATH= zsh 非 tmux 下尝试不断设置不同的环境变量后测试发现好的环境下设置了 LD_DYLD_PATH 变量就会有问题。 最小重现代码如下： export LD_DYLD_PATH= cat \u003c\u003cEOF \u003e main.rs fn main() { println!(\"Hello, world!\"); } EOF rm -rf main \u0026\u0026 rustc main.rs \u0026\u0026 ./main \u0026\u0026 rm -rf main.rs main 检查了当前我在使用的 dotfiles 中没有设置过 LD_DYLD_PATH，尝试一下 workaround： unset LD_DYLD_PATH 尝试在 linux 下看看是否会有同样的问题： docker run --rm -it docker.m.daocloud.io/library/rust:1-bookworm bash Hello, world! 看起来是 macos 下特有的问题。 sudo dtrace ./main dtrace: system integrity protection is on, some features will not be available dtrace: failed to execute /tmp/main: Malformed Mach-o file 提了 issue ，看看社区怎么说。 ","date":"2024-12-14","objectID":"/rustc-compiled-bin-killed-on-mac-m1/:0:0","tags":["rust","debug"],"title":"rustc 编译的程序直接执行报错 `zsh: killed     ./main`","uri":"/rustc-compiled-bin-killed-on-mac-m1/"},{"categories":null,"content":"文章简介：通过一个实验理解超线程是如何工作的 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:0:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"背景 学习 CPU 超线程的时候，实践一下 plantegg 大佬的实验。实验大概是这样的： 写一个能把 IPC（instructions per cycle）跑到最高的代码 (可以试试跑一段死循环的 IPC 能到多少)；写一个能把 IPC 跑到最低的程序。然后用 perf 去看他们的 IPC，用 top 去看他们的 CPU 使用率。进一步同时把这样的程序跑两份，但是将他们绑到一对超线程上，然后再看他们的 IPC 以及 top。 nop.c void main() { while (1) { __asm__( \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\\n\\t\" \"nop\"); } } pause.c void main() { while (1) { __asm__( \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\\n\\t\" \"pause\"); } } ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:1:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"实验环境信息 root@localhost# cat /etc/redhat-release Rocky Linux release 8.10 (Green Obsidian) root@localhost# uname -a Linux localhost.localdomain 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux root@localhost# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 80 On-line CPU(s) list: 0-79 Thread(s) per core: 2 Core(s) per socket: 20 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel BIOS Vendor ID: Intel(R) Corporation CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz BIOS Model name: Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz Stepping: 7 CPU MHz: 799.932 CPU max MHz: 4000.0000 CPU min MHz: 800.0000 BogoMIPS: 4200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 28160K NUMA node0 CPU(s): 0-19,40-59 NUMA node1 CPU(s): 20-39,60-79 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities numactl -H available: 2 nodes (0-1) node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 node 0 size: 128410 MB node 0 free: 52082 MB node 1 cpus: 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 node 1 size: 128985 MB node 1 free: 120673 MB node distances: node 0 1 0: 10 21 1: 21 10 lstopo-no-graphics --output-format svg \u003e IntelRXeonGold5218R.svg 我的环境比较奇怪，宿主机的系统是 centos7.9，重装系统会相对麻烦一些。centos 已经 EOL，直接宿主机安装各种需要的软件、搭建编译环境相对麻烦，且我的电脑是 M1 arm，搭建交叉编译环境会涉及修改编译命令，入门门槛相对较高。所以尝试在 docker 中搭建运行、编译环境。环境如下： ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:2:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"编译运行环境 Dockerfile 见如下，文件保存到 tools/Dockerfile.labs： FROM --platform=linux/amd64 rockylinux/rockylinux:8 RUN dnf groupinstall -y 'Development Tools' \\ \u0026\u0026 dnf --enablerepo=devel -y install bison byacc flex patch glibc-static git libtirpc libtirpc-devel numactl numa* hwloc perf tmux 编译 docker 镜像 docker build -f tools/Dockerfile.labs -t cpulabs:dev . 只使用镜像中的 linux 发行版 rootfs，其他所有的隔离手段不使用，尽可能匹配宿主机裸机执行的环境： docker run --name casestudy --rm -it --privileged --userns=host --network=host --pid=host -v $(pwd):/host -w /host cpulabs:dev bash ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:2:1","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"实验主体 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:3:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"测试 nop 指令 理论上 intel 4 条流水线并行，nop 执行需要一个时钟周期，cpu 完全跑 nop 指令的 IPC 大约是 4。接下来验证一下。 gcc ./nop.c -o nop 执行测试 perf stat timeout 10 ./nop Performance counter stats for 'timeout 10 ./nop': 9977.71 msec task-clock # 0.997 CPUs utilized 8 context-switches # 0.802 /sec 2 cpu-migrations # 0.200 /sec 511 page-faults # 51.214 /sec 38351130635 cycles # 3.844 GHz 149695103720 instructions # 3.90 insn per cycle 1168639909 branches # 117.125 M/sec 465623 branch-misses # 0.04% of all branches 10.002747608 seconds time elapsed 9.977095000 seconds user 0.001750000 seconds sys 现象： IPC 跑到 3.90 CPUs util 0.997 问题： IPC 跟 perf 执行多久无关，但是执行时间过短，IPC 计数会小一点点，猜测可能是时间太短，while 循环也需要执行执行，跳转指令让指令流水线受阻导致的，长时间的执行令分支预测更加准确，IPC 趋于稳定 perf stat timeout 0.1 ./nop IPC 在 3.84 perf stat timeout 1 ./nop IPC 在 3.90 IPC 是否可以接近 4? 据说 intel 流水线可达 4 条并行，理论值是 4，while 循环也会转换成其他命令，会稀释一部分导致 IPC 降低，可以考虑增加 asm 中的 nop 的数量 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:3:1","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"测试 pause 指令 理论上 intel 共 4 条流水线并行，一个 pause 指令大概 140 时钟周期，4/140=0.02857142857142857，接下来验证一下。 gcc ./pause.c -o pause 执行测试 perf stat timeout 10 ./pause Performance counter stats for 'timeout 10 ./pause': 9980.35 msec task-clock # 0.997 CPUs utilized 6 context-switches # 0.601 /sec 3 cpu-migrations # 0.301 /sec 510 page-faults # 51.100 /sec 35500797866 cycles # 3.557 GHz 929191714 instructions # 0.03 insn per cycle 15430712 branches # 1.546 M/sec 373753 branch-misses # 2.42% of all branches 10.005685906 seconds time elapsed 9.979564000 seconds user 0.002129000 seconds sys 现象： IPC 大约 0.03，估计是精度问题，与计算出的理论值接近。 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:3:2","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"测试 nop 指令跑两份各自绑定到同一物理核 cat /proc/cpuinfo 中 希望找到相同物理核心的两个超线程，即寻找 core id 相同 且 physical id 相同的 两个 processor 即可。 perf stat timeout 3 taskset -c 0 ./nop \u0026 perf stat timeout 3 taskset -c 40 ./nop Performance counter stats for 'taskset -c 40 ./nop': 6668.00 msec task-clock # 0.997 CPUs utilized 2 context-switches # 0.300 /sec 1 cpu-migrations # 0.150 /sec 343 page-faults # 51.440 /sec 21645830628 cycles # 3.246 GHz 32415527643 instructions # 1.50 insn per cycle 256383559 branches # 38.450 M/sec 273069 branch-misses # 0.11% of all branches 6.684805024 seconds time elapsed 6.666732000 seconds user 0.001996000 seconds sys Performance counter stats for 'timeout 3 taskset -c 40 ./nop': 3000.46 msec task-clock # 0.996 CPUs utilized 6 context-switches # 2.000 /sec 1 cpu-migrations # 0.333 /sec 740 page-faults # 246.629 /sec 10749571656 cycles # 3.583 GHz 19580067188 instructions # 1.82 insn per cycle 155065143 branches # 51.680 M/sec 159560 branch-misses # 0.10% of all branches 3.013625515 seconds time elapsed 2.995919000 seconds user 0.008434000 seconds sys 可以看到两个 nop 跑在相同物理核的两个超线程下，IPC 下降到原来的一半以下了。 (1.50+1.82)/(3.90*2)=0.42 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:3:3","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"测试 pause 指令跑两份各自绑定到同一物理核 perf stat timeout 3 taskset -c 0 ./pause \u0026 perf stat timeout 3 taskset -c 40 ./pause Performance counter stats for 'timeout 3 taskset -c 40 ./pause': 3000.71 msec task-clock # 0.997 CPUs utilized 5 context-switches # 1.666 /sec 1 cpu-migrations # 0.333 /sec 740 page-faults # 246.608 /sec 9127896305 cycles # 3.042 GHz 232869301 instructions # 0.03 insn per cycle 5153047 branches # 1.717 M/sec 149048 branch-misses # 2.89% of all branches 3.008989678 seconds time elapsed 2.996138000 seconds user 0.005710000 seconds sys Performance counter stats for 'timeout 3 taskset -c 0 ./pause': 3002.08 msec task-clock # 0.997 CPUs utilized 8 context-switches # 2.665 /sec 1 cpu-migrations # 0.333 /sec 740 page-faults # 246.496 /sec 9036205346 cycles # 3.010 GHz 228681050 instructions # 0.03 insn per cycle 4910490 branches # 1.636 M/sec 151013 branch-misses # 3.08% of all branches 3.010698825 seconds time elapsed 3.000847000 seconds user 0.002973000 seconds sys 现象： 绑定在 0 号 CPU 上的 IPC 为 0.03，绑定到 40 号 CPU 上的 IPC 为 0.03，与原始跑一个 pause 时一样 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:3:4","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"测试 pause/nop 指令各自绑定到同一物理核 perf stat timeout 3 taskset -c 0 ./nop \u0026 perf stat timeout 3 taskset -c 40 ./pause Performance counter stats for 'timeout 3 taskset -c 0 ./nop': 3004.09 msec task-clock # 0.997 CPUs utilized 9 context-switches # 2.996 /sec 1 cpu-migrations # 0.333 /sec 741 page-faults # 246.663 /sec 8734459274 cycles # 2.908 GHz 27382699175 instructions # 3.14 insn per cycle 215882285 branches # 71.863 M/sec 170322 branch-misses # 0.08% of all branches 3.013204769 seconds time elapsed 2.999945000 seconds user 0.006140000 seconds sys Performance counter stats for 'timeout 3 taskset -c 40 ./pause': 2997.37 msec task-clock # 0.997 CPUs utilized 5 context-switches # 1.668 /sec 1 cpu-migrations # 0.334 /sec 738 page-faults # 246.216 /sec 8791793294 cycles # 2.933 GHz 219616224 instructions # 0.02 insn per cycle 5261702 branches # 1.755 M/sec 144141 branch-misses # 2.74% of all branches 3.005700376 seconds time elapsed 2.992557000 seconds user 0.006074000 seconds sys 现象： 计算当前 IPC 是跑在两个物理核心上的 (3.14+0.02)/(3.90+0.03) = 0.80，比同时跑 nop 的 0.42 提高明显。 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:3:5","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"待解答问题 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:4:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"为什么死循环里要写这么多 pause/nop，少了，是什么因素导致的 IPC 降低呢？ ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:4:1","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"有什么方式可以查看当前 CPU 的流水线长度吗？ TODO: ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:4:2","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"总结 学习了 perf 命令监控程序性能 学习了 taskset 命令控制绑核 学习了超线程相关信息 上面学习到的都是需要后续深入研究。 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:5:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"ref IPC(Instructions per cycle) NOP (code) X86_instruction_listings 说说如何判断 Linux 是否开启了超线程 Perf IPC 以及 CPU 性能 ","date":"2024-11-03","objectID":"/experiment-to-understand-how-hyperthreading-works/:6:0","tags":["debug","cpu","intel",""],"title":"超线程是如何工作的 - 实验","uri":"/experiment-to-understand-how-hyperthreading-works/"},{"categories":null,"content":"文章简介：探索为什么 sysbench 没有重连 Reconnect ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:0:0","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"背景 plantegg 提供了一个案例，大体上是阿里云上 sysbench 压测 MySQL 的时候，使用 MySQL 客户端 kill sysbench 发起的压测连接，发现 sysbench 没有重连，QPS 始终为 0。plantegg 给出了复现方法。 ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:1:0","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"问题复现 在我的 M1 mac 上使用 vagrant + vmware_fusion 启动了一个虚拟机，Vagrantfile 如下，虚拟机启动的环境内核版本更可控； # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| config.vm.box = \"bento/ubuntu-22.04\" config.vm.box_architecture = \"arm64\" config.vm.provider :vmware_fusion do |vm| # https://gist.github.com/jtopper/8588263 end config.vm.provision \"shell\", inline: \u003c\u003c-SHELL # https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ sed -E -i -e 's/(archive|ports).ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g' -e '/security.ubuntu.com/d' /etc/apt/sources.list apt update apt install -y tshark tcpdump net-tools gdb dstat apt autoremove apt install -y linux-tools-common linux-tools-$(uname -r) SHELL end # 先在 vm 中启动一个 mysql，后文以 plantegg# 开头 docker run -it -d --net=host -e MYSQL_ROOT_PASSWORD=123 --name=plantegg docker.m.daocloud.io/library/mysql:8 plantegg# mysql --ssl-mode=DISABLED -uroot -p123 # 在本地启动一个容器，用于执行 sysbench，后文以 sysbench# 开头 docker run --privileged -it --name sysbench registry.dockermirror.com/747301585/alinux3:20220901 bash #sysbench# uname -a Linux e590c969f57f 6.6.16-linuxkit #1 SMP Fri Feb 16 11:54:02 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux sysbench# yum install -y sysstat sysbench iproute net-tools strace ltrace perf gdb sysbench# export MYSQL_HOST='192.168.245.145' sysbench# sysbench --mysql-user='root' --mysql-password='123' --mysql-db='test' --mysql-host=$MYSQL_HOST --mysql-port='3306' --tables='16' --table-size='10000' --range-size='5' --db-ps-mode='disable' --skip-trx='on' --mysql-ignore-errors='all' --time='1180' --report-interval='1' --histogram='on' --threads=1 oltp_read_only prepare # 命令有些报错，不影响问题复现 sysbench# sysbench --mysql-user='root' --mysql-password='123' --mysql-db='test' --mysql-host=$MYSQL_HOST --mysql-port='3306' --tables='16' --table-size='10000' --range-size='5' --db-ps-mode='disable' --skip-trx='on' --mysql-ignore-errors='all' --time='1180' --report-interval='1' --histogram='on' --threads=1 oltp_read_only run plantegg# mysql --ssl-mode=DISABLED -uroot -p123 plantegg#mysql# select version(); +-----------+ | version() | +-----------+ | 8.4.2 | +-----------+ plantegg#mysql# show processlist; plantegg#mysql# kill some-pid; -- 复现关键点在这里 plantegg#mysql# show processlist; 复现后的现象： sysbench 吞吐跌零： sysbench# [ 15s ] thds: 1 tps: 68.07 qps: 2033.94 (r/w/o: 2033.94/0.00/0.00) lat (ms,95%): 36.89 err/s: 146.14 reconn/s: 0.00 [ 16s ] thds: 1 tps: 55.94 qps: 1998.85 (r/w/o: 1998.85/0.00/0.00) lat (ms,95%): 42.61 err/s: 168.82 reconn/s: 0.00 [ 17s ] thds: 1 tps: 24.81 qps: 1079.68 (r/w/o: 1079.68/0.00/0.00) lat (ms,95%): 74.46 err/s: 97.25 reconn/s: 0.00 [ 18s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 19s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 21s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 22s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 23s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 24s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 25s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 26s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 27s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 [ 28s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s: 0.00 reconn/s: 0.00 网络连接大量 CLOSE_WAIT： sysbench# ss -tnp | head -n 30 State Recv-Q Send-Q Local Address:Port Peer Address:PortProcess CLOSE-WAIT 28 0 172.17.0.2:55265 192.168.245.145:3306 users:((\"sysbench\",pid=207,fd=22757)) CLOSE-WAIT 28 0 172.17.0.2:","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:2:0","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"分析一下原因 先看看 cpu 到被消耗在了内核还是进程 pidstat 1 Linux 6.6.16-linuxkit (e590c969f57f) 10/17/24 _aarch64_ (4 CPU) 09:22:11 UID PID %usr %system %guest %wait %CPU CPU Command 09:22:12 0 115 0.00 64.08 0.00 0.00 64.08 0 sysbench 09:22:12 0 259 0.00 3.88 0.00 0.00 3.88 1 pidstat 09:22:12 UID PID %usr %system %guest %wait %CPU CPU Command 09:22:13 0 115 0.00 67.00 0.00 0.00 67.00 0 sysbench 09:22:13 UID PID %usr %system %guest %wait %CPU CPU Command 09:22:14 0 115 0.00 70.00 0.00 0.00 70.00 0 sysbench 09:22:14 UID PID %usr %system %guest %wait %CPU CPU Command 09:22:15 0 115 0.00 69.00 0.00 0.00 69.00 0 sysbench 可以看到所有的 cpu 都在内核态。实际跑下来，刚复现跌零时候 cpu 比较少，随着 TCP 连接的变多，内核 cpu 越来越高。 看看内核在干什么： perf top -g -p `pidof sysbench` + 98.36% 0.01% [kernel] [k] __sys_connect + 97.73% 0.05% libpthread-2.32.so [.] __libc_connect + 97.35% 5.06% [kernel] [k] __inet_hash_connect + 92.17% 0.01% [kernel] [k] __inet_stream_connect + 86.99% 59.51% [kernel] [k] __inet_check_established + 86.84% 0.00% [kernel] [k] tcp_v4_connect + 67.97% 0.01% [kernel] [k] do_el0_svc + 28.19% 28.19% [kernel] [k] inet_ehashfn + 19.36% 0.00% libc-2.32.so [.] thread_start + 19.36% 0.00% libpthread-2.32.so [.] start_thread + 19.35% 0.00% sysbench [.] 0x0000aaaade71bce0 + 19.35% 0.00% libluajit-5.1.so.2.1.0 [.] lua_pcall + 19.35% 0.00% [JIT] tid 207 [.] 0x0000ffffac384eb8 + 19.35% 0.00% sysbench [.] db_execute + 19.35% 0.00% sysbench [.] 0x0000aaaade72b744 + 19.35% 0.00% [kernel] [k] el0t_64_sync + 19.35% 0.00% [kernel] [k] el0t_64_sync_handler + 19.35% 0.01% [kernel] [k] el0_svc + 19.33% 0.00% sysbench [.] 0x0000aaaade72affc + 19.33% 0.00% sysbench [.] 0x0000aaaade72a078 + 19.33% 0.00% libmariadb.so.3 [.] 0x0000ffffae08ce54 + 19.33% 0.00% [kernel] [k] invoke_syscall.constprop.0 + 19.27% 0.00% libmariadb.so.3 [.] 0x0000ffffae084f10 + 19.27% 0.00% libmariadb.so.3 [.] 0x0000ffffae084600 + 19.26% 0.00% [kernel] [k] __arm64_sys_connect + 19.24% 0.00% [kernel] [k] inet_stream_connect + 19.24% 0.00% [kernel] [k] inet_hash_connect + 2.81% 0.98% [kernel] [k] __cond_resched + 2.81% 2.26% [kernel] [k] __local_bh_enable_ip __inet_check_established, inet_ehashfn 内核函数执行占用过多 cpu 可以看到 __inet_check_established 和 inet_ehashfn 两个函数占用比较高； TODO: 需要研究一下 kernel 函数 __inet_check_established 和 inet_ehashfn 性能消耗在了哪里。 现在看看进程在做什么事情： strace -f -p `pidof sysbench` 437 sendto(3, \"5\\0\\0\\0\\3SELECT c FROM sbtest7 WHERE\"..., 57, MSG_DONTWAIT|MSG_NOSIGNAL, NULL, 0) = 57 437 recvfrom(3, 0xffff80027600, 16384, MSG_DONTWAIT, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) 437 ppoll([{fd=3, events=POLLIN}], 1, NULL, NULL, 0) = 1 ([{fd=3, revents=POLLIN}]) 437 recvfrom(3, \"\\1\\0\\0\\1\\1*\\0\\0\\2\\3def\\4test\\7sbtest7\\7sbtes\"..., 16384, MSG_DONTWAIT, NULL, NULL) = 689 437 sendto(3, \";\\0\\0\\0\\3SELECT SUM(k) FROM sbtest15\"..., 63, MSG_DONTWAIT|MSG_NOSIGNAL, NULL, 0) = 63 437 recvfrom(3, 0xffff80027600, 16384, MSG_DONTWAIT, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) 437 ppoll([{fd=3, events=POLLIN}], 1, NULL, NULL, 0) = 1 ([{fd=3, revents=POLLIN}]) 437 recvfrom(3, \",\\0\\0\\1\\377z\\4#42S02Table 'test.sbtest1\"..., 16384, MSG_DONTWAIT, NULL, NULL) = 48 437 sendto(3, \"%\\0\\0\\0\\3SELECT c FROM sbtest10 WHER\"..., 41, MSG_DONTWAIT|MSG_NOSIGNAL, NULL, 0) = 41 437 recvfrom(3, 0xffff80027600, 16384, MSG_DONTWAIT, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) 437 ppoll([{fd=3, events=POLLIN}], 1, NULL, NULL, 0) = 1 ([{fd=3, revents=POLLIN}]) 437 recvfrom(3, \"\", 16384, MSG_DONTWAIT, NULL, NULL) = 0 437 close(3) = 0 437 getpid() = 435 437 socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3 437 fcntl(3, F_SETFL, O_RDONLY|O_NONBLOCK) = 0 437 connect(3, {sa_family=AF_INET, sin_port=htons(3306), sin_addr=inet_addr(\"192.168.245.145\")}, 16) = -1 EINPROGRESS (Operation now in progress) 437 ppoll([{fd=3, events=POLLOUT}], 1, NULL, NULL, 0) = 1 ([{fd=3, revents=POLLOUT}]) 437 getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0 437 fcntl(3, F_SETFL, O_RDONLY) = 0 437 clock_nanosleep(CLOCK_REALTIME, 0, {tv_sec=0, tv_nsec=1000000}, NULL) = ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:3:0","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"问题 ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:4:0","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"为什么 MySQL 8.4.2 有的 Recv-Q 是 28，有的是 132 State Recv-Q Send-Q Local Address:Port Peer Address:PortProcess CLOSE-WAIT 28 0 172.17.0.2:37863 192.168.245.145:3306 users:((\"sysbench\",pid=207,fd=20236)) CLOSE-WAIT 28 0 172.17.0.2:55403 192.168.245.145:3306 users:((\"sysbench\",pid=207,fd=16731)) CLOSE-WAIT 132 0 172.17.0.2:53963 192.168.245.145:3306 users:((\"sysbench\",pid=207,fd=25906)) TODO: 详细学习一下 Recv-Q Send-Q 分别意味着发生了什么事情 尝试抓 mysqld 的 syscall 发现，正常的登录过程，sendto 发送了 77(正常登录回显)+54(超时回显)+1(close 后的 fin 占用一个 byte)=132 strace -ff -p `pidof mysqld` -o mysqld_syscal.txt 14595 \u003c... accept resumed\u003e{sa_family=AF_INET6, sin6_port=htons(65517), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"::ffff:192.168.245.1\", \u0026sin6_addr), sin6_scope_id=0}, [128 =\u003e 28]) = 49 19382 sendto(49, \"2\\0\\0\\1\\377\\207\\4#08S01Got timeout reading\"..., 54, MSG_DONTWAIT, NULL, 0 \u003cunfinished ...\u003e 19382 setsockopt(49, SOL_TCP, TCP_NODELAY, [1], 4) = 0 19382 getpeername(49, {sa_family=AF_INET6, sin6_port=htons(64695), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"::ffff:192.168.245.1\", \u0026sin6_addr), sin6_scope_id=0}, [128 =\u003e 28]) = 0 19382 setsockopt(49, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0 19382 sendto(49, \"I\\0\\0\\0\\n8.4.2\\0\\2448\\0\\0W%59p.\\21Y\\0\\377\\377\\377\\2\\0\\377\\337\\25\"..., 77, MSG_DONTWAIT, NULL, 0) = 77 19382 recvfrom(49, 0xffff5c1d0db0, 4, MSG_DONTWAIT, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) 19382 sendto(49, \"2\\0\\0\\1\\377\\207\\4#08S01Got timeout reading\"..., 54, MSG_DONTWAIT, NULL, 0) = 54 19382 shutdown(49, SHUT_RDWR) = 0 19382 close(49) = 0 19373 setsockopt(49, SOL_TCP, TCP_NODELAY, [1], 4 \u003cunfinished ...\u003e 19373 getpeername(49, {sa_family=AF_INET6, sin6_port=htons(65517), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"::ffff:192.168.245.1\", \u0026sin6_addr), sin6_scope_id=0}, [128 =\u003e 28]) = 0 19373 setsockopt(49, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0 19373 sendto(49, \"I\\0\\0\\0\\n8.4.2\\0;9\\0\\0\\n(V\\0201D_\\26\\0\\377\\377\\377\\2\\0\\377\\337\\25\"..., 77, MSG_DONTWAIT, NULL, 0) = 77 19373 recvfrom(49, 0xffff78034a00, 4, MSG_DONTWAIT, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) echo $((54+77+1)) 132 尝试抓 mysqld 的 syscall 发现，连接过多时，sendto 发送了 27(too many conn)+1(close 后的 fin 占用一个 byte)=28 14595 accept(20, {sa_family=AF_INET6, sin6_port=htons(64852), sin6_flowinfo=htonl(0), inet_pton(AF_INET6, \"::ffff:192.168.245.1\", \u0026sin6_addr), sin6_scope_id=0}, [128 =\u003e 28]) = 197 14595 setsockopt(197, SOL_TCP, TCP_NODELAY, [1], 4) = 0 14595 sendto(197, \"\\27\\0\\0\\0\\377\\20\\4Too many connections\", 27, MSG_DONTWAIT, NULL, 0) = 27 14595 shutdown(197, SHUT_RDWR) = 0 14595 close(197) = 0 echo $((27+1)) 28 ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:4:1","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"为什么 CPU 这么高，CPU 都在忙什么 perf top 可以看到，kernel 在执行 __inet_check_established； strace -f -p `pidof sysbench` 437 mprotect(0xffff80037000, 8192, PROT_READ|PROT_WRITE) = 0 437 clock_nanosleep(CLOCK_REALTIME, 0, {tv_sec=0, tv_nsec=1000000}, NULL) = 0 437 getpid() = 435 437 mprotect(0xffff80039000, 8192, PROT_READ|PROT_WRITE) = 0 437 socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 6 437 fcntl(6, F_SETFL, O_RDONLY|O_NONBLOCK) = 0 437 connect(6, {sa_family=AF_INET, sin_port=htons(3306), sin_addr=inet_addr(\"192.168.245.145\")}, 16) = -1 EINPROGRESS (Operation now in progress) 437 ppoll([{fd=6, events=POLLOUT}], 1, NULL, NULL, 0) = 1 ([{fd=6, revents=POLLOUT}]) 437 getsockopt(6, SOL_SOCKET, SO_ERROR, [0], [4]) = 0 437 fcntl(6, F_SETFL, O_RDONLY) = 0 437 mprotect(0xffff8003b000, 8192, PROT_READ|PROT_WRITE) = 0 437 clock_nanosleep(CLOCK_REALTIME, 0, {tv_sec=0, tv_nsec=1000000}, NULL) = 0 437 getpid() = 435 437 mprotect(0xffff8003d000, 8192, PROT_READ|PROT_WRITE) = 0 437 socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 7 437 fcntl(7, F_SETFL, O_RDONLY|O_NONBLOCK) = 0 437 connect(7, {sa_family=AF_INET, sin_port=htons(3306), sin_addr=inet_addr(\"192.168.245.145\")}, 16) = -1 EINPROGRESS (Operation now in progress) 437 ppoll([{fd=7, events=POLLOUT}], 1, NULL, NULL, 0) = 1 ([{fd=7, revents=POLLOUT}]) 437 getsockopt(7, SOL_SOCKET, SO_ERROR, [0], [4]) = 0 437 fcntl(7, F_SETFL, O_RDONLY) = 0 437 mprotect(0xffff8003f000, 8192, PROT_READ|PROT_WRITE) = 0 437 clock_nanosleep(CLOCK_REALTIME, 0, {tv_sec=0, tv_nsec=1000000}, NULL) = 0 437 getpid() = 435 437 mprotect(0xffff80041000, 8192, PROT_READ|PROT_WRITE) = 0 437 socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 8 437 fcntl(8, F_SETFL, O_RDONLY|O_NONBLOCK) = 0 437 connect(8, {sa_family=AF_INET, sin_port=htons(3306), sin_addr=inet_addr(\"192.168.245.145\")}, 16) = -1 EINPROGRESS (Operation now in progress) 437 ppoll([{fd=8, events=POLLOUT}], 1, NULL, NULL, 0) = 1 ([{fd=8, revents=POLLOUT}]) 437 getsockopt(8, SOL_SOCKET, SO_ERROR, [0], [4]) = 0 437 fcntl(8, F_SETFL, O_RDONLY) = 0 437 mprotect(0xffff80043000, 8192, PROT_READ|PROT_WRITE) = 0 437 clock_nanosleep(CLOCK_REALTIME, 0, {tv_sec=0, tv_nsec=1000000}, NULL) = 0 strace -f -c -p `pidof sysbench` strace: Process 207 attached with 3 threads ^Cstrace: Process 207 detached strace: Process 208 detached strace: Process 209 detached % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 49.55 16.874015 3734 4518 clock_nanosleep 49.22 16.759397 3731 4491 4491 connect 0.56 0.192136 192136 1 restart_syscall 0.25 0.085898 19 4492 close 0.18 0.061314 13 4491 socket 0.11 0.037854 8 4491 fcntl 0.11 0.037465 8 4491 getpid 0.01 0.004097 146 28 write ------ ----------- ----------- --------- --------- ---------------- 100.00 34.052176 1261 27003 4491 total strace 可以看出 sysbench 在执行 connect，但是一直 errors，且可以看到几个 syscall calls 相差不大，echo $((4518-4491)) = 27 ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:4:2","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"什么原因会导致 CLOSE_WAIT 状态 从 wiki 中搬一张图： 被动关闭方接收到 fin 后，没有发送 fin，被动关闭方状态停留在 CLOSE_WAIT。此例子中 CLOSE_WAIT 出现在 sysbench 中，说明是由于 mysql server 关闭发起 close，而客户端被动关闭未 close，客户端（sysbench）连接状态为 CLOSE_WAIT； 在 sysbench 机器中执行 nc -vz ip 3306，报错 Cannot assign requested address. 也说明是 sysbench 的问题。 bash-4.4# nc -v 192.168.245.145 3306 Ncat: Version 7.92 ( https://nmap.org/ncat ) Ncat: Cannot assign requested address. ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:4:3","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"mysql 这里在做什么事情？mysql 多久会触发 close 行为呢？ mysql 接收到请求，后超时的包在这里 可以看到，mysql 在 0.00618s mysql 返回了登录验证的 prompt，此时 mysql 中的连接状态是： 10.008458 mysql 返回超时。说明 mysql 这里的登录超时时间是 10 s。 后续超时的连接被内核 reset 掉。 ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:4:4","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"为什么 Sysbench 要疯狂创建 4 万多个连接 TODO: 需要翻翻 sysbench 的代码，确认一下原因了，看到 syscall 中大量的 connect，大概是到了一个死循环处 TODO: perf top 中展开可以看到很多函数地址，并无执行的函数名字，猜测是 sysbench 的函数，需要编译 sysbench 后确认一下 TODO: EADDRNOTAVAIL; //Cannot assign requested address 这个错误的含意是什么，何时出现 ","date":"2024-10-16","objectID":"/why-sysbench-dont-reconnect-after-kill/:4:5","tags":["debug","mysql","sysbench"],"title":"sysbench benchmark MySQL 时候，为什么 kill 连接后，sysbench 没有重连","uri":"/why-sysbench-dont-reconnect-after-kill/"},{"categories":null,"content":"文章简介：k3s 环境在高 CPU\u0026\u0026Memory\u0026\u0026IO 负载下，pod 中的 container 随机健康检查失败 问题排查及解决 在大数据量下压测业务时，发现比较随机的 pod 中的 container failed（重启次数不断累积）, 健康检查失败。 kubectl describe pod postgres-xxxx TODO tail -f /var/lib/rancher/k3s/agent/containerd/containerd.log 看到非常多如下日志： time=\"2024-08-21T08:42:37.199951601+08:00\" level=error msg=\"ExecSync for \\\"69994ab2f7a951735b5d613d80b9ac33e1c1366a3e0fae5fd9ec24fb545cb1d6\\\" failed\" error=\"rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 5s exceeded: context deadline exceeded\" ... time=\"2024-07-30T10:39:39.772561484+08:00\" level=error msg=\"get state for 39756d9fdea7fcf15eb76d0c792614681b1d801fb8a70bc72c3042503e619473\" error=\"context deadline exceeded: unknown\" ... 当前部署的 k3s 已经为了解决 etcd 稳定性问题，将 etcd 的存储挂载到了 ssd 下。而 /var/lib/rancher/k3s/agent/containerd 下目录放到机械硬盘上了。这次的问题就在于 k3s 的健康检查设置的时间为 5s，健康检查是 containerd 执行命令，5s 内 containerd 未实际执行完对应的命令，超时失败多次后，pod 健康检查失败，pod 进入 unhealthy 状态。对外的表现就是访问对应的 service 的请求 timeout 导致业务不可用。社区 issue 可以参考 Containerd fails to stop containers 及 Pod stuck terminating / KillContainerError 解决办法：将 /var/lib/rancher/k3s/agent/containerd/ 挂载到 ssd 下即可。 k3s 的文档中已经在 Resource Profiling 中描述，为了保证 k3s 的稳定性，需要将 /var/lib/rancher/k3s/{agent,server,storage} 三个目录分开互不影响。我们将 k3s agent 及 server 放到 ssd 下，与 storage 放到不同的目录下，解决高负载下稳定性问题。 ","date":"2024-08-20","objectID":"/k8s-pod-random-killed/:0:0","tags":["k8s","k3s","containerd","bugs"],"title":"k3s 环境在高 CPU\u0026\u0026Memory\u0026\u0026IO 负载下，pod 中的 container 随机健康检查失败","uri":"/k8s-pod-random-killed/"},{"categories":null,"content":"文章简介：golang 的 timer 在调整系统时间下的行为 在测试 k3s 重启自动续签证书时，发现手动调整机器时间后，calico Unauthorized，pod 因无法创建 network sandbox 无法启动，且可以看到 calico-node -monitor-token 没有执行。 calico-node -monitor-token 是 calico 后台刷新证书的程序，，每次刷新都会 time.After max(nextExpiration/4, 5s*4)。但是实测下来，直接手工调整时间后，calico 并没有主动重新申请 token，这里涉及到 time.After 的工作机制。 linux 时间有 CLOCK_REALTIME_COARSE（墙上时钟） 和 CLOCK_MONOTONIC_COARSE （单调时钟），这两种时钟详细解释见 这里。 从 golang 中找到的代码看到 time.After 调用栈见： [checkTimers](https://github.com/golang/go/blob/release-branch.go1.22/src/runtime/proc.go#L3945) [runtime·nanotime1](https://github.com/golang/go/blob/release-branch.go1.22/src/runtime/sys_linux_amd64.s#L258) 可以看到 golang 中的 timer.Timer 使用的是 CLOCK_MONOTONIC_COARSE 也即单调时钟，date 命令调整时间并不会体现在这里获得到的时间，所以调整时间并不会让 calico 按照墙上时钟快速触发。 问题确认即可快速确认方案，如果为了规避时间问题，可以只重启 k3s 即可，如果期望可以快速使用 date 调整时间 calico 快速恢复，可以考虑重启 calico-node 进程重新生成 kubeconf。 golang issue 12914 中讨论了 nanotime 第三方无法调用，以及当前的进展。当前如果希望获得 单调时钟，可以参考 这个 ","date":"2024-08-10","objectID":"/go-timer-behavior-in-adjucting-sysmte-time/:0:0","tags":[],"title":"golang 的 timer 在调整系统时间下的行为","uri":"/go-timer-behavior-in-adjucting-sysmte-time/"},{"categories":null,"content":"文章简介：我是如何将 kafka 消费吞吐提高 100 倍的 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:0:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"背景 告警入库/归档效率差，200 左右 告警消费占用资源多，500w 下大量告警会直接压垮平台，平台主要业务 504 timeout 告警频繁入库/归档情况下，postgres 数据库告警相关表空洞率达到 90% 以上，大量数据空洞拖慢数据库查询几个数据量 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:1:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"分析原因 单表存储过多数据，单表超过 500w 行 数据库插入更新删除频繁，postgres mvvc 与 autovacuum 机制没有及时触发，导致空洞率提高 告警业务流程长，很多实时性要求不高的逻辑（告警自动化处置/统计）串在告警消费过程中，影响告警吞吐 告警消费有大量小而频繁的查询，没有添加缓存，数据库占用大量 qps 数据库使用默认配置，内存/cpu 没有充分使用 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:2:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"都做了哪些事 数据库配置调优 告警基于时间分区，每天一个分区，归档每次处理一个分区，分区索引与并行查询，提高查询吞吐及稳定性 告警消费流程重新梳理，拆成三个阶段，告警入库/告警统计/自动化处置; 告警消费过程不会因为告警自动化处置慢而影响到整体的吞吐 告警涉及到多张表批量入库，提高吞吐，降低资源消耗 告警归档同步到 hbase 中后，truncate 子表 加 cache 慢查询优化，优化 sql 及添加索引; 删除从未被使用过的索引 消费程序单独一个进程，资源/故障隔离 消费添加限流，提高稳定性 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:3:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"达到的效果 32 核 64GB 500w 告警下 告警入库吞吐可以到 40000，比优化前高 2 个数量级 告警列表取前 20 条数据 sql 提高 2500 倍；count 速度提高 27 倍；告警列表整体加载时间由 15s 缩短为 3s 以内，告警统计页面加载时间 504 timeout 优化到 15s 内 数据库调整参数后吞吐提高三倍 告警量 3000w 情况下服务可用，预期会存半年甚至更多的数据后服务依然可用 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:4:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"告警列表优化 优化前 explain (ANALYZE, VERBOSE, TIMING,COSTS, BUFFERS, SUMMARY) SELECT * FROM \"alarm\" WHERE (judged_state = 1 OR judged_state = 0) ORDER BY alarm.updated_at desc nulls last LIMIT 20; 优化后 create index if not exists alarm_multiline_index on alarm (created_at desc, updated_at desc nulls last, judged_state, alarm_level, disposed_state); ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:4:1","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"大屏大量使用的 count 优化 17134.851/623.675 = 27 倍 优化前： explain (ANALYZE, VERBOSE, TIMING,COSTS, BUFFERS, SUMMARY) SELECT COUNT(\"id\") FROM \"alarm\" WHERE created_at \u003e= '2023-05-30 07:34:46.002537+00' AND created_at \u003c '2023-05-31 07:34:46.002537+00'; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------- Finalize Aggregate (cost=840896.65..840896.66 rows=1 width=8) (actual time=17098.462..17134.785 rows=1 loops=1) Output: count(alarm.id) Buffers: shared hit=5615853 read=105541 written=1 -\u003e Gather (cost=840896.23..840896.64 rows=4 width=8) (actual time=17098.338..17134.774 rows=5 loops=1) Output: (PARTIAL count(alarm.id)) Workers Planned: 4 Workers Launched: 4 Buffers: shared hit=5615853 read=105541 written=1 -\u003e Partial Aggregate (cost=839896.23..839896.24 rows=1 width=8) (actual time=17094.127..17094.129 rows=1 loops=5) Output: PARTIAL count(alarm.id) Buffers: shared hit=5615853 read=105541 written=1 Worker 0: actual time=17093.145..17093.148 rows=1 loops=1 Buffers: shared hit=1478094 read=28657 Worker 1: actual time=17093.203..17093.205 rows=1 loops=1 Buffers: shared hit=1470692 read=28510 Worker 2: actual time=17093.147..17093.150 rows=1 loops=1 Buffers: shared hit=886771 read=16085 Worker 3: actual time=17093.167..17093.170 rows=1 loops=1 Buffers: shared hit=882627 read=16017 written=1 -\u003e Parallel Append (cost=0.55..836170.65 rows=1490232 width=37) (actual time=0.946..17023.812 rows=1188508 loops=5) Buffers: shared hit=5615853 read=105541 written=1 Worker 0: actual time=1.212..17006.371 rows=1491562 loops=1 Buffers: shared hit=1478094 read=28657 Worker 1: actual time=0.939..17005.878 rows=1484942 loops=1 Buffers: shared hit=1470692 read=28510 Worker 2: actual time=0.814..17034.576 rows=986228 loops=1 Buffers: shared hit=886771 read=16085 Worker 3: actual time=0.471..17035.496 rows=982083 loops=1 Buffers: shared hit=882627 read=16017 written=1 -\u003e Parallel Index Only Scan using alarm_20230530_pkey on public.alarm_20230530 alarm_1 (cost=0.56..713942.31 rows=1157253 width=37) (actual time=0.796..14931.447 rows=919 379 loops=5) Output: alarm_1.id Index Cond: ((alarm_1.created_at \u003e= '2023-05-30 07:34:46.002537+00'::timestamp with time zone) AND (alarm_1.created_at \u003c '2023-05-31 07:34:46.002537+00'::timestamp wi th time zone)) Heap Fetches: 296346 Buffers: shared hit=4552140 read=88971 written=1 Worker 0: actual time=1.211..16900.689 rows=1491562 loops=1 Buffers: shared hit=1478094 read=28657 Worker 1: actual time=0.938..16900.152 rows=1484942 loops=1 Buffers: shared hit=1470692 read=28510 Worker 2: actual time=0.501..13618.651 rows=537258 loops=1 Buffers: shared hit=532080 read=10556 Worker 3: actual time=1.098..13619.421 rows=536621 loops=1 Buffers: shared hit=530559 read=10531 written=1 -\u003e Parallel Index Only Scan using alarm_20230531_pkey on public.alarm_20230531 alarm_2 (cost=0.55..114777.18 rows=332979 width=37) (actual time=0.860..3347.784 rows=44854 8 loops=3) Output: alarm_2.id Index Cond: ((alarm_2.created_at \u003e= '2023-05-30 07:34:46.002537+00'::timestamp with time zone) AND (alarm_2.created_at \u003c '2023-05-31 07:34:46.002537+00'::timestamp wi th time zone)) Heap Fetches: 68366 Buffers: shared hit=1063713 read=16570 Worker 2: actual time=0.813..3346.199 rows=448970 loops=1 Buffers: shared hit=354691 read=5529 Worker 3: actual time=0.470..3347.495 rows=445462 loops=1 Buffers: shared hit=352068 read=5486 Query Identifier: 4802739785007802293 Planning: Buffers: shared hit=14 Planning Time: 0.299 ms Execution Time: 17134.851 ms (56 rows) 优化后： pedestal=# explain (ANALYZE, VERBOSE, TIMING,COSTS, BUFFERS, SUMMARY) SELECT COUNT(*) FROM \"alarm\" WHERE created_at \u003e= '2023-05-30 07:34:46.002537+00' AND created_at \u003c '2023-05-31 07:34:46.002537+00'; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:4:2","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"告警消费吞吐如何优化 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:5:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"该如何优化 优化数据库配置，提高数据库本身的处理能力 deadlock_timeout = 2s max_connections = 500 # 16 核 8 GB 内存 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 1GB effective_io_concurrency = 2 bgwriter_lru_maxpages = 500 wal_buffers = 32MB max_wal_size = 8GB min_wal_size = 4GB work_mem = 8MB max_worker_processes = 16 max_parallel_workers_per_gather = 4 max_parallel_workers = 16 max_parallel_maintenance_workers = 4 提高 pg 内存使用量 提高 pg cpu 使用量，sql 并行查询 批量入库提高吞吐 const ( AlarmSize = 50000 AlarmBatch = 100 ) // go test -timeout 30s -run ^TestSingleInsert$ github.com/exfly/alarmopt func TestSingleInsert(t *testing.T) { err := DB.AutoMigrate(\u0026Alarm{}) require.NoError(t, err) err = DB.Exec(\"truncate alarm;\").Error require.NoError(t, err) alarms := testGenAlarm(t, AlarmSize) err = DB.Debug().CreateInBatches(alarms, 1).Error require.NoError(t, err) } // go test -timeout 30s -run ^TestBatchInsert$ github.com/exfly/alarmopt func TestBatchInsert(t *testing.T) { err := DB.AutoMigrate(\u0026Alarm{}) require.NoError(t, err) err = DB.Exec(\"truncate alarm;\").Error require.NoError(t, err) alarms := testGenAlarm(t, AlarmSize) err = DB.Debug().CreateInBatches(alarms, AlarmBatch).Error require.NoError(t, err) } go test -timeout 30s -run ^TestSingleInsert$ github.com/exfly/alarmopt ok github.com/exfly/alarmopt 20.156s go test -timeout 30s -run ^TestBatchInsert$ github.com/exfly/alarmopt ok github.com/exfly/alarmopt 0.551s 20.156/0.551 = 36.58 倍 ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:5:1","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"如何做的告警消费批量入库 告警消费是从 kafka 中消费数据，见如下代码。Kafka 消息传递采用的方式是定时从 Server 中 pull Message，一次 pull 会拉多条数据，但是 sarama 的 ConsumerGroup 暴漏出的 interface sarama.ConsumerGroupClaim 中没有办法一次性获得整批数据，需要实现这部分聚合逻辑。 type handler struct { actPool *ants.PoolWithFunc } func (consumer *handler) Setup(session sarama.ConsumerGroupSession) error { return nil } func (consumer *handler) Cleanup(session sarama.ConsumerGroupSession) error { return nil } func (consumer *handler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error { for message := range claim.Messages() { if err := consumer.actPool.Invoke(message); err != nil { if err == ants.ErrPoolClosed { return err } } session.MarkMessage(message, \"\") } return nil } 需要的聚合逻辑如下所述： 从 chan 中一次拿出一个 Messsage 多条消息按照如下规则聚合到一起 当数量达到 N 个时 上一次发送数据到当前时间超过 T 时 批量入库手动实现 import ( \"context\" \"log/slog\" \"os\" \"sync\" \"testing\" \"time\" ) var ( logger = slog.New(slog.NewTextHandler(os.Stderr, \u0026slog.HandlerOptions{AddSource: true, Level: slog.LevelDebug})) ) type Alarm struct { ID int } type batchInsert struct { do func(context.Context, []Alarm) error interval time.Duration maxBatchSize int64 lock sync.RWMutex buf []Alarm once sync.Once } func (b *batchInsert) Do(ctx context.Context, in Alarm) error { b.once.Do(func() { go func() { ticker := time.NewTicker(b.interval) for range ticker.C { if b.lock.TryLock() { if len(b.buf) \u003e 0 { logger.Info(\"timer do\") if err := b.do(ctx, b.buf); err != nil { logger.Error(\"in ticker do failed\", \"err\", err) } else if err == nil { b.buf = nil } } b.lock.Unlock() } } }() }) b.lock.Lock() defer b.lock.Unlock() if b.buf == nil { b.buf = make([]Alarm, 0, b.maxBatchSize) } b.buf = append(b.buf, in) if len(b.buf) \u003e= int(b.maxBatchSize) { logger.Info(\"realtime do\") if err := b.do(ctx, b.buf); err != nil { logger.Error(\"do failed\", \"err\", err) } else if err == nil { b.buf = nil } } return nil } 社区 dataloader 实现 dataloader 来资源 Facebook 的 graphql，为了解决其 N+1 问题出现的 go 重的 Dataloader 实现：https://github.com/graph-gophers/dataloader/tree/master package alarmopt import ( \"context\" \"testing\" \"time\" dataloader \"github.com/graph-gophers/dataloader/v7\" ) func TestDataloaderNoCache(t *testing.T) { batchFunc := func(_ context.Context, keys []int) []*dataloader.Result[*Alarm] { var results []*dataloader.Result[*Alarm] // do some pretend work to resolve keys for _, k := range keys { results = append(results, \u0026dataloader.Result[*Alarm]{Data: \u0026Alarm{ID: k}}) } logger.Debug(\"real do\", \"ids\", keys) return results } cache := \u0026dataloader.NoCache[int, *Alarm]{} loader := dataloader.NewBatchedLoader( batchFunc, dataloader.WithCache[int, *Alarm](cache), dataloader.WithWait[int, *Alarm](time.Second*3), dataloader.WithBatchCapacity[int, *Alarm](5), dataloader.WithInputCapacity[int, *Alarm](1), ) for i := 0; i \u003c 50; i++ { logger.Debug(\"submit\", \"id\", i) loader.Load(context.Background(), i) } } func TestAlarmDataloaderNoCacheRetResult(t *testing.T) { batchFunc := func(_ context.Context, keys []Alarm) []*dataloader.Result[*Alarm] { var results []*dataloader.Result[*Alarm] // do some pretend work to resolve keys for _, k := range keys { results = append(results, \u0026dataloader.Result[*Alarm]{Data: \u0026Alarm{ID: k.ID}}) } logger.Debug(\"real do\", \"ids\", keys) return results } cache := \u0026dataloader.NoCache[Alarm, *Alarm]{} loader := dataloader.NewBatchedLoader( batchFunc, dataloader.WithCache[Alarm, *Alarm](cache), dataloader.WithWait[Alarm, *Alarm](time.Second*3), dataloader.WithBatchCapacity[Alarm, *Alarm](5), dataloader.WithInputCapacity[Alarm, *Alarm](1), ) for i := 0; i \u003c 50; i++ { logger.Debug(\"submit\", \"id\", i) result, err := loader.Load(context.Background(), Alarm{ID: i})() if err != nil { t.Error(err) } _ = result } } ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:5:2","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"REF 为什么默认使用 PostgreSQL 数据库？ ","date":"2024-05-13","objectID":"/how-to-increase-alarm-throughput-100-times/:6:0","tags":[],"title":"我是如何将 kafka 消费吞吐提高 100 倍","uri":"/how-to-increase-alarm-throughput-100-times/"},{"categories":null,"content":"文章简介：离线安装各种容器引擎，docker/containerd+nerdctl ","date":"2024-05-07","objectID":"/install-docker-engine-from-bin/:0:0","tags":["docker","linux","nerdctl","containerd","ops"],"title":"离线安装开箱即用的容器运行环境运行环境","uri":"/install-docker-engine-from-bin/"},{"categories":null,"content":"背景 通常每个 linux 发行版都会自带的包管理安装 docker，但这种安装方式在 to b 完全离线环境下适配不同种 linux 发行版安装成本比较高。docker 由 golang 开发，依赖都是捆绑到一起的，直接复制二进制文件即可完成部署，简单便捷。Docker 官方介绍了一种 Install Docker Engine from binaries 方法，但是这种方式不完整：This page contains information on how to install Docker using binaries. These instructions are mostly suitable for testing purposes. We do not recommend installing Docker using binaries in production environments as they don't have automatic security updates.。本文将介绍一种生产可用的离线安装 docker、且跨 linux 发行版的方法。 ","date":"2024-05-07","objectID":"/install-docker-engine-from-bin/:1:0","tags":["docker","linux","nerdctl","containerd","ops"],"title":"离线安装开箱即用的容器运行环境运行环境","uri":"/install-docker-engine-from-bin/"},{"categories":null,"content":"方法 1: 安装官方的 docker 离线包 Docker 官方文档中提供的方法 使用的方法是 sudo dockerd \u0026 ，没有服务保活，不适用与生产使用。在这个基础上，完善一下保活基本就可以了。 如下为 package.sh，打包一个离线安装包 #!/usr/bin/env bash set -o errtrace set -o errexit set -o nounset set -o pipefail set -o xtrace cd \"$(dirname \"$0\")\" function debug() { echo \"[$(date +'%Y-%m-%dT%H:%M:%S%z')]:DEBU: $*\" \u003e\u00262 } function info() { echo \"[$(date +'%Y-%m-%dT%H:%M:%S%z')]:INFO: $*\" \u003e\u00262 } function err() { echo \"[$(date +'%Y-%m-%dT%H:%M:%S%z')]:ERRR: $*\" \u003e\u00262 } DOCKER_VERSION=\"${DOCKER_VERSION:-\"26.1.1\"}\" DOCKRE_MIRROR=${DOCKRE_MIRROR:-\"https://mirrors.tuna.tsinghua.edu.cn\"} function machine_arch() { arch=$(uname -m) case $arch in x86_64) echo amd64 ;; aarch64) echo arm64 ;; *) echo $arch ;; esac } function docker_arch() { arch=$(uname -m) case $arch in arm64) echo aarch64 ;; *) echo $arch ;; esac } # https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/static/stable/aarch64/docker-26.1.1.tgz DOCKER_DOWNLOAD_URL=\"${DOCKRE_MIRROR}/docker-ce/linux/static/stable/$(docker_arch)/docker-${DOCKER_VERSION}.tgz\" BASEPATH=./installer mkdir -p $BASEPATH pushd $BASEPATH wget $DOCKER_DOWNLOAD_URL popd mkdir -p $BASEPATH/docker cat \u003e $BASEPATH/docker/daemon.json \u003c\u003c'EOF' { \"live-restore\": true, \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"200m\" } } EOF mkdir -p $BASEPATH/systemd cat \u003e $BASEPATH/systemd/containerd.service \u003c\u003c'EOF' [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Restart=always RestartSec=2 TimeoutSec=30 StartLimitInterval=0 KillMode=process Delegate=yes LimitNOFILE=1048576 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity # TasksMax=infinity [Install] WantedBy=multi-user.target EOF cat \u003e $BASEPATH/systemd/docker.service \u003c\u003c'EOF' [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/local/bin/dockerd -H fd:// ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=30 RestartSec=2 Restart=always # Note that StartLimit* options were moved from \"Service\" to \"Unit\" in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. # retry start always StartLimitInterval=0 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this option. # TasksMax=infinity # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process [Install] WantedBy=multi-user.target EOF cat \u003e $BASEPATH/systemd/docker.socket \u003c\u003c'EOF' [Unit] Description=Docker Socket for the API PartOf=docker.service [Socket] ListenStream=/var/run/docker.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target EOF tar -zvcf dockerbin-installer.tar.gz installer install.sh 其中 install.sh 内容为： #!/usr/bin/env bash set -o errtrace set -o errexit set -o nounset set -o pipefail set ","date":"2024-05-07","objectID":"/install-docker-engine-from-bin/:2:0","tags":["docker","linux","nerdctl","containerd","ops"],"title":"离线安装开箱即用的容器运行环境运行环境","uri":"/install-docker-engine-from-bin/"},{"categories":null,"content":"方法 2: nerdctl docker 的内存占用相对来说还是过于复杂，进程会包含 containerd + dockerd，并且 dockerd 本身就包含了自己的网络实现和 overlay 存储实现。而 containerd + ctr 即可完成对日常容器镜像的管理等工作，再配合 nerdctl 兼容部分 docker 的命令行参数，如果只是在类似云主机中跑服务的场景，完全可以使用此方法替换。 nerdctl 的安装非常简单，再 nerdctl release 中下载nerdctl-full-*-linux-*.tar.gz, `` wget -O nerdctl-full-1.7.6-linux-amd64.tar.gz https://github.com/containerd/nerdctl/releases/download/v1.7.6/nerdctl-full-1.7.6-linux-amd64.tar.gz tar -zvxf nerdctl-full-*-linux-amd64.tar.gz -C /usr/local sudo systemctl enable --now containerd sudo systemctl enable --now buildkit # 如果需要 docker build，则需要启动他 nerdctl run --debug-full -d --name nginx -p 80:80 nginx:alpine nerdctl 包含 compose 命令，可以直接使用。 ","date":"2024-05-07","objectID":"/install-docker-engine-from-bin/:3:0","tags":["docker","linux","nerdctl","containerd","ops"],"title":"离线安装开箱即用的容器运行环境运行环境","uri":"/install-docker-engine-from-bin/"},{"categories":null,"content":"方法 3: truenas scale 中使用类 docker 的体验 玩 truenas scale 的小伙伴会了解到，truenas scale 中实际启动了一个单节点的 k3s 作为容器引擎。很多小伙伴反馈 k3s 的学习门槛过高了。truenas scale 虽然是基于 debian 的 linux 发行版，实际使用的包管理不是 apt，且不能使用 apt 安装其他的包，具体信息如下： apt Package management tools are disabled on TrueNAS appliances. Attempting to update SCALE with apt or methods other than the SCALE web interface can result in a nonfunctional system. 经过调研，可以使用 这个项目 ，使用 nerdctl 作为 客户端访问隔离环境中的 containerd，做到与 方法 2 中描述的相同使用体验。 再我的 truenas 中，我的工作流是本地使用 docker-compose 将需要配置的服务写好，然后 rsync 到 trunenas 上，nerdctl compose up -d 完事。如果添加的 alias docker=nerdctl, 几乎可以忽略掉 nerdctl 的存在，当成 docker 使用即可。 ","date":"2024-05-07","objectID":"/install-docker-engine-from-bin/:4:0","tags":["docker","linux","nerdctl","containerd","ops"],"title":"离线安装开箱即用的容器运行环境运行环境","uri":"/install-docker-engine-from-bin/"},{"categories":null,"content":"总结 如上，请欢快的使用 docker / containerd 吧。 ","date":"2024-05-07","objectID":"/install-docker-engine-from-bin/:5:0","tags":["docker","linux","nerdctl","containerd","ops"],"title":"离线安装开箱即用的容器运行环境运行环境","uri":"/install-docker-engine-from-bin/"},{"categories":null,"content":"文章简介：一次 sarama 访问 kafka 报错 client/metadata got error from broker 1002 while fetching metadata: write tcp x.x.x.x:49749-\u003ex.x.x.x:9092: write: broken pipe 问题排查 ","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:0:0","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"问题现象 生产环境部署了 3 节点 kafka 集群，用 golang 使用 IBM/sarama 的 consumergroup 消费 kafka 数据，规律性的报错： 2024-04-01T19:20:08.341+0800 ERROR [Input] [Kafka-Sarama] client/metadata got error from broker 1002 while fetching metadata: write tcp myclient:57096-\u003emyserver:9092: write: broken pipe ... 2024-04-01T21:29:07.462+0800 ERROR [Input] [Kafka-Sarama] client/metadata got error from broker 1002 while fetching metadata: EOF sarama 的配置见： config := sarama.NewConfig() // Kafka broker 版本号 version, err := sarama.ParseKafkaVersion(\"2.0.0\") if err != nil { return err } config.Version = version config.Net.KeepAlive = time.Second * 5 config.ClientID = i.Name + \"test-abc\" config.Metadata.RefreshFrequency = 1 * time.Minute config.Metadata.Retry.Max = 9999999 config.Metadata.Retry.BackoffFunc = func(retries, maxRetries int) time.Duration { if retries \u003e= maxRetries { return 0 } log.Info(\"[Input] [Kafka] Metadata Has Anomalies, Please Handle It\") return 20 * time.Second } config.Consumer.Offsets.Initial = -1 config.Consumer.Group.Heartbeat.Interval = time.Second * 3 详细日志见 2024-04-01T19:05:11.132+0800 DEBUG [Input] [Kafka-Sarama] consumer/broker/1001 added subscription to test4/0 2024-04-01T19:06:08.015+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:07:07.995+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:08:07.857+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:09:07.548+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:10:07.793+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:11:07.849+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg42:9092 2024-04-01T19:12:09.010+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:13:08.559+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:14:07.847+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:15:07.692+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:16:08.470+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:17:08.396+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:18:08.160+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:19:07.845+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg43:9092 2024-04-01T19:20:08.341+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg41:9092 2024-04-01T19:20:08.341+0800 ERROR [Input] [Kafka-Sarama] client/metadata got error from broker 1002 while fetching metadata: write tcp myclient:57096-\u003emyserver:9092: write: broken pipe 2024-04-01T19:20:08.341+0800 DEBUG [Input] [Debug] [Kafka-Sarama] Closed connection to broker bg41:9092 2024-04-01T19:20:08.341+0800 DEBUG [Input] [Kafka-Sarama] client/brokers deregistered broker #1002 at bg41:9092 2024-04-01T19:20:09.476+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg42:9092 2024-04-01T19:20:09.506+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/brokers registered new broker #1002 at bg41:9092 2024-04-01T19:21:08.197+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from br","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:1:0","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"解决办法 sarama 的 keepalive 及 heatbeat 机制问题，并非对所有节点都有可靠的 heatbeat，请求闲置时间超过阈值连接被 kafka server 主动 close，sarama 使用了这个坏的连接导致 write: broken pipe 或 EOF + // manual heatbeat + go func() { + for range time.NewTicker(time.Minute).C { + fmt.Printf(\"manual heatbeat\\n\") + for _, one := range client.Brokers() { + _, err := one.Heartbeat(\u0026sarama.HeartbeatRequest{}) + if err != nil { + fmt.Printf(\"%v %v\\n\", one.Addr(), err) + } + } + } + }() + ","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:2:0","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"TL;DR ","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:3:0","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"why kafka close conn? 看到 write: broken pipe 首先从底层分析，可能来源为服务端主动 close。kafka 主动 close 可能来源为两处 tcp keepalive 应用层超过了 connections.max.idle.ms TCP keepalive 先看第一种情况，TCP keepalive 图中高亮的一个包是第一次开始 TCP keepalive 的地方，每个 tcp keepalive 间隔 5s 左右，恰好与代码中的配置 config.Net.KeepAlive = time.Second * 5 匹配。说明 tcp keepalive 参数已经按照预期工作了。 kafka connections.max.idle.ms 查看配置，发现默认的 connections.max.idle.ms 10 分钟，session.timeout.ms 为 45s，而每次到 client/metadata fetching metadata for all topics from broker bg41:9092 到 write: broken pipe 的最小间隔时间都超过了 10min，日志中一个例子如下，间隔为 18min： 2024-04-01T21:46:07.437+0800 DEBUG [Input] [Debug] [Kafka-Sarama] client/metadata fetching metadata for all topics from broker bg41:9092 2024-04-01T22:04:07.444+0800 ERROR [Input] [Kafka-Sarama] client/metadata got error from broker 1002 while fetching metadata: write tcp myclient:62648-\u003emyserver:9092: write: broken pipe 在 tcpdump 中看到一次 fetch medadate 到 write broken pipe 经历了 (2447-1756)/60 = 11.51min。 既然是超过了最大空闲时间，想到使用跟 kafka 交互最轻量的操作 heartbeat 试试看是否可以修复。按照上文的 解决办法 修复后，便不再出现 write: broken pipe 错误，至此，可以完全确认是 sarama 触发了 kafka 的最大空闲时间 (connections.max.idle.ms) 导致的。 为什么会这样呢？ ","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:3:1","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"sarama 的 heatbeat 是怎么工作的 TODO: 详细的分析一下 kafka 机制及 sasrama 的工作过程，分析一下出现此问题出现过程 TODO: 翻代码，查看 sarama 的 heatbeat 的实现找原因 ","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:3:2","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"是否可以修复一下 TODO: 尝试看是否提 pr 修复掉这个问题 ","date":"2024-04-08","objectID":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/:3:3","tags":[],"title":"Kafka Sarama Go Fetch Metadta Write Broken Pipe","uri":"/kafka-sarama-go-fetch-metadta-write-broken-pipe/"},{"categories":null,"content":"文章简介：总结最近几年使用 k8s 的经验 ","date":"2024-03-16","objectID":"/k8s-in-action/:0:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"背景 ToB 项目 离线安装运行环境 容器化 多组件编排 基于这个背景下，公司选择使用 k8s。 ","date":"2024-03-16","objectID":"/k8s-in-action/:1:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"rke 选择使用 rancher/rke 这个 k8s 发行版，docker/ansible 等依赖离线化处理，支持 ubuntu 20.04/ centos 7.9。 ","date":"2024-03-16","objectID":"/k8s-in-action/:2:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"基于 kustomize 的部署 kustomize 提供了 template free 的 k8s manifest yaml 生成工具，可以简单的为 base/prod/staging 等不同的生产环境打 patch 定制不同的环境。当前会使用 helm chart 生成 yaml 文件供 helm charts 使用。此时所有的部署相关文件是在统一的一个仓库 deploy 中维护的。这种模式下每个项目都会有自己的 deploy.sh 文件，用于持续部署 patch 项目的 image。如果某个项目的部署模式变化了，要同时修改 deploy 这个仓库。两项目耦合严重。 ","date":"2024-03-16","objectID":"/k8s-in-action/:3:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"基于 helm charts 以 app 为单位的部署 为了解决 deploy 项目与 app 项目耦合严重的问题，将deploy 中 app 的 configmap/service/deployment 等资源移动到 app 仓库中，此时 deploy 只需要引用 app 仓库的版本号/分支即可。 ","date":"2024-03-16","objectID":"/k8s-in-action/:4:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"longhorn + nacos 系统极不稳定来源 在第一次生产验证过程中，k8s 的分布式文件存储使用了 longhorn。使用过程中会频繁的 IO error, longhorn 官方甚至列出了非常多的已知问题。在生产使用过程中会比较频繁的出现文件系统 readonly/读写延迟过高，nacos 服务规律性不可用、重启 nacos 5-10分钟，导致依赖此服务的其他所有服务长时间不可用。由于 poc 为单机版本，考虑使用本地的文件系统替换分布式文件系统。 ","date":"2024-03-16","objectID":"/k8s-in-action/:5:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"第一次生产 POC 阶段不可复现升级过程 由于产品是 ToB 项目，产品产出物为离线安装包。在客户离线网络下安装后，会频繁的更新某个服务的镜像。急需一个稳定可复现的升级过程，方便现场离线升级，而不是每次我们产出文字版本、有歧义的升级过程文档。 ","date":"2024-03-16","objectID":"/k8s-in-action/:6:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"古怪的 helm upgrade 第一版本升级包有一个比较神奇 helm 升级过程 helm show values appname1 \u003e values.yaml helm upgrade --debug --install appname1 appname1-0.0.1.tgz -f values.yaml app 的 values 经过重构后，很容易出现 values结构变化，变化的化，需要如此人工写新的升级逻辑 helm show values appname1 \u003e values.yaml migrate_appname1_patch_values.py helm upgrade --debug --install appname1 appname1-0.0.1.tgz -f values.yaml 导致升级维护困难 ","date":"2024-03-16","objectID":"/k8s-in-action/:6:1","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"改进后的 helm upgrade 使用 IaC 的思路，将所有的基础设施都在仓库中维护，将所有 values.yaml 也要放到升级包中，而不是 charts 放到仓库，values 使用生产中的 helm upgrade --debug --install appname1 appname1-0.0.1.tgz -f values.yaml 为了保证 values.yaml 部分值需要依赖生产环境，values.yaml 是一种 template 比如 golang 的 template，使用新的工具基于当前生产环境中的 deploy-env configmap 生成, 其他所有部分都放到 gitlab 中维护。 kubectl get cm deploy-env -o go-template --template='{{range $key, $value := .data}}export {{$key}}='\"'\"'{{$value}}'\"'\"';{{end}}' gotemplate values.yaml.tmpl \u003e values.yaml helm upgrade --debug --install appname1 appname1-0.0.1.tgz -f values.yaml 最终所有的升级打包过程会变成： 从 gitlab 中下载所有的 charts charts + values 渲染出 k8s manifest yaml, 从其中提取出 docker img，并下载下来 最终所有的升级过程变成： skopeo copy file docker://host/xxx 遍历所有 apps，渲染 values.yaml.tmpl, helm upgrade ","date":"2024-03-16","objectID":"/k8s-in-action/:6:2","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"选择更简单的架构，k3s 改造 rke 的 k8s 发行版依赖 docker，docker 需要自己安装。当前的实现中需要使用发行版中的 docker 依赖树。客户场景需要支持很多不同中 linux 发行版，适配不同的发行版成本搞。搭上 k3s 的顺风车，单文件启动兼容 k8s 的服务，其余组件已经容器化，安装过程简化了很多适配 linux 发行版的过程。 ","date":"2024-03-16","objectID":"/k8s-in-action/:7:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"k8s 集群回归 集群部署 文件系统采用本地存储，要求应用层面的副本机制保证服务数据不丢: rancher/local-path-provisioner ","date":"2024-03-16","objectID":"/k8s-in-action/:8:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"总结 小集群（\u003c=50节点）k3s 真的很香，80% 企业的服务都是少于 50节点的（未实际统计，凭当前简单的企业集群数量估算） 分布式文件系统真的太理想化了，当前测试发现 longhorn 尽量不要在生产使用，除非他真的适合你的场景 IaC 在运维场景为我们省去了太多后期维护的麻烦事，能用就用，不能用也要想方设法使用 ","date":"2024-03-16","objectID":"/k8s-in-action/:9:0","tags":["k8s","k3s"],"title":"K8s 实践经验 (k8s in action)","uri":"/k8s-in-action/"},{"categories":null,"content":"文章简介：介绍迁移鲲鹏 arm64 架构的方案及遇到的坑 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:0:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"背景介绍 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:1:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"项目背景 客户要求 arm64 机器的需求。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:1:1","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"ARM64 指令集特性 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:2:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"AMD64 到 ARM64 的迁移策略 由于容器化，需要支持 k8s 在 arm64 下运行，所有使用的容器支持 amd64+arm64；有源码的项目采用重新构建的方式，无源码且只支持 amd64 的项目采用 binfmt_misc 方式运行；最终完成全部迁移过程。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:3:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"准备工作 看看我们的硬件信息 root@cosmos:~# cat /etc/os-release NAME=\"openEuler\" VERSION=\"22.03 (LTS-SP2)\" ID=\"openEuler\" VERSION_ID=\"22.03\" PRETTY_NAME=\"openEuler 22.03 (LTS-SP2)\" ANSI_COLOR=\"0;31\" root@cosmos:~# hostnamectl Static hostname: cosmos Icon name: computer-server Chassis: server 🖳 Machine ID: 682d69e664ef4e5695cb3eaf805d7158 Boot ID: 034c84cb591145dab0cad91871dd5dd6 Operating System: Debian GNU/Linux 12 (bookworm) Kernel: Linux 6.1.62-generic Architecture: arm64 Hardware Vendor: PowerLeader Hardware Model: CT0101202306013E7A Firmware Version: 1.93 root@cosmos:~# uname -a Linux cosmos 6.1.62-generic #1 SMP Sun Nov 19 05:53:26 UTC 2023 aarch64 GNU/Linux root@cosmos:~# lscpu Architecture: aarch64 CPU op-mode(s): 64-bit Byte Order: Little Endian CPU(s): 48 On-line CPU(s) list: 0-47 Vendor ID: HiSilicon BIOS Vendor ID: HiSilicon Model name: Kunpeng-920 BIOS Model name: HUAWEI Kunpeng 920 3210 To be filled by O.E.M. CPU @ 2.6GHz BIOS CPU family: 280 Model: 0 Thread(s) per core: 1 Core(s) per socket: 24 Socket(s): 2 Stepping: 0x1 Frequency boost: disabled CPU(s) scaling MHz: 100% CPU max MHz: 2600.0000 CPU min MHz: 200.0000 BogoMIPS: 200.00 Flags: fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm Caches (sum of all): L1d: 3 MiB (48 instances) L1i: 3 MiB (48 instances) L2: 24 MiB (48 instances) L3: 48 MiB (2 instances) NUMA: NUMA node(s): 2 NUMA node0 CPU(s): 0-23 NUMA node1 CPU(s): 24-47 Vulnerabilities: Gather data sampling: Not affected Itlb multihit: Not affected L1tf: Not affected Mds: Not affected Meltdown: Not affected Mmio stale data: Not affected Retbleed: Not affected Spec rstack overflow: Not affected Spec store bypass: Not affected Spectre v1: Mitigation; __user pointer sanitization Spectre v2: Not affected Srbds: Not affected Tsx async abort: Not affected ","date":"2024-01-29","objectID":"/kunpeng-arm64/:4:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"安装和配置 ARM64 版本的 Kubernetes 当前社区的 k8s 发行版已经支持 arm64，根据社区的文档安装即可 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:4:1","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"迁移过程中的语言挑战 华为有一本鲲鹏软件迁移一本通，可以后续看一下 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:5:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"Go 语言应用的特殊注意事项 golang 从 1.5 开始支持 cross compile, arm 的支持进展见这里。 对于纯 golang 编写的项目，可通过简单的修改 CICD 适配构建 arm64 架构的可执行文件即可。GOOS=linux GOARCH=arm64 go build ... 对于使用了 cgo 的项目，则需要付出更多努力。使用 gcc 在 go build 过程中编译的项目，需要修改编译方式，做 crosscompile，一个例子比如使用了 sqlite 的项目交叉编译。一些项目在项目中已经将多种架构的动态链接库/静态链接库文件 commit 到仓库中，运行时直接使用即可，比如 confluent-kafka-go。部分使用了 cgo 项目可能不支持 arm64，比如 github.com/bytedance/sonic 是不支持 arm64 的，需要寻找替代方案，比如替换成标准库或者其他支持 arm64 的库（如 github.com/json-iterator/go） ","date":"2024-01-29","objectID":"/kunpeng-arm64/:5:1","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"Python 项目的适配和修改 修改 python 版本为支持 arm64 的 python 版本，python2.7 以及 python 3 都有比较好的 arm64 支持 含 C 模块或全 C 模块的迁移，下载模块源码，并使用 -fsigned-char 选项重新编译。另外 python 官方提供了一种能够方便构建不同架构的 wheel 的基础设施（基于 docker）pypa/manylinux ","date":"2024-01-29","objectID":"/kunpeng-arm64/:5:2","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"Java 应用的迁移考虑 Java 不同于一般的编译语言或解释型语言。它首先将源代码编译成字节码，再依赖各种不同平台上的虚拟机来解释执行字节码，从而具有“一次编写，到处运行”的跨平台特性。但是 java 支持的 JNI 可以在 java 中调用 c/c++ 等其他语言编写的动态链接库，这种本地程序是不跨平台的，导致不能直接采用更换 arm64 jre 不修改程序的方式做迁移。 更换 arm64 架构的 JRE 如下图所示，对于纯 Java 的程序，可以考虑直接更换 ARM64 的 JRE 来运行服务。如果第三方库是用到了 jni 调用本地方法，就需要对这个库适配 arm64 jni 如果是开源软件，直接基于代码做 arm64 适配，如果为非开源项目，则需要寻找替代方案、或寻求商业支持 设置 JVM 参数解决程序运行是改动点 三大经验：首先，完成一次 Full GC 后，应该释放出 70% 的堆空间（30% 的空间仍然占用）。其次，假设老年代存活对象 (即 Full GC 后老年代内存占用) 大小为 X，建议堆的总大小是 X 的三到四倍，年轻代的大小是 X 的 1 到 1.5 倍，老年代的大小是 X 的两到三倍，永久代的大小是 X 的 1.2 到 1.5 倍。第三，JDK 官方的建议是年轻代大小占整个堆空间大小的 3/8 左右。 在具体的项目过程中，会有如下差异： 线程大小的 Xss 参数在 ARM 上默认值是 2m，在 x86 为 1m。因此如果线程开的太多，就需要调整 Xss 参数大小，防止出现耗时。 由于 ARM 和 x86 指令集的差异性，导致 JDK 的 JIT 编译存在差异。可以通过设置参数 ReservedCodeCacheSize，调整 CodeCache 大小。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:5:3","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"C/C++ 等编译型语言如何迁移 不过，C/C++ 代码在迁移中也会有诸多问题存在，最具代表性的五类迁移问题如下： 1、编译脚本和编译选项的移植。不同的架构平台会有独特的编译选项支持硬件特性，与当前编译平台属性强相关这种带有架构属性的编译选项需要进行移植，这些编译选项一般以–m 开头； 2、编译宏的移植。编译宏的作用是确定平台下需要执行哪个分支代码，一般分为 x86 自定义宏和用户自定义的宏。两类宏的编译移植方式各不相同； 3、builtin 函数问题。builtin 函数是编译器自定义的函数，有较好的性能，可以实现一些简单快捷的功能，根据相应需求进行使用优化，助力程序编写； 4、内联汇编移植，常用迁移方法有汇编指令方式替换以及 builtin 函数替换两种； 5、SSE intrinsic 函数移植。一般在多媒体技术开发以及数学矩阵库中应用较多的 SSE intrinsic 函数移植较为复杂，为重难点。比如 clickhouse 中使用到了 SIMD 指令，需要替换成 arm64 下对应的使用方式，并且因为鲲鹏已经是比较旧的指令集了，对于一些新的指令不支持导致 clickhouse 报错非法指令集，需要删除新的指令集特性重新编译，或者降级 clickhouse 版本到未使用新指令级特性的版本。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:5:4","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"非开源项目的迁移挑战 对于非开源商业项目，可以直接找商业公司寻求支持。但是对于部分非开源商业项目，已经过了维护期，就需要我们自己想办法迁移了。 比较显而易见的方式是使用 qemu 在 arm64 上直接执行 amd64 的二进制可执行程序。支持的原理可以简单理解为，linux 提供了一种机制，使内核能够将不支持的 bin 格式转交给虚拟机/转换程序执行，详细见 binfmt_misc, 注意此中方法在部分应用下是可用的，比如 golang 的二进制程序、部分 c 程序，对于像 redis、java 等程序，会有 segment fault 或者卡住不可用的情况，需要亲自测试确认一下应用是否可用。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:6:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"使用 Docker 构建支持多架构的镜像 期望要给镜像支持多个架构，这样由客户端决定拉取哪一种架构的镜像。 根据 docker 官方的文档描述，使用 docker 构建多架构 docker image 一般有如下几种方式： 使用内核支持的 QEMU emulation 创建一个 builder，后端为多个不同架构的物理机 使用 cross-compile 在本地架构构建不同架构的可执行程序 不同的方式的使用条件不同，我们逐个分析。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:7:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"使用内核支持的 QEMU emulation 使用内核支持的 qemu emulation，可以 在本地使用 qemu 启动不同的架构的类似 vm 的进程，在对应架构上执行代码。由于是基于 qemu emulation，性能会比较低，对于一些 cpu 密集型的任务比如编译、压缩、解压速度会有很大的影响。 # 注册多架构支持 docker run --privileged --rm tonistiigi/binfmt --install all # 查看是否注册成功, enabled 为注册的架构 cat /proc/sys/fs/binfmt_misc/qemu-* enabled interpreter /usr/bin/qemu-x86_64 flags: POCF offset 0 magic 7f454c4602010100000000000000000002003e00 mask fffffffffffefe00fffffffffffffffffeffffff 默认的 docker driver 是不支持多架构构建的，需要使用如下方式创建一个新的 builder。 docker buildx create --name mybuilder --bootstrap --use docker buildx build --platform=linux/amd64,linux/arm64 -f Dockerfile -t dev:tmp . 在编译 arm java 程序时候，可能会遇到 hang 的情况，这种方法需要实际试验一下看是否可行。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:7:1","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"创建一个 builder，后端为多个不同架构的物理机 此种方式首先需要多台不同种架构的硬件机器。构建过程会在对应架构机器中执行，比如 arm64 会在 arm64 的机器中执行，amd64 会在 amd64 的机器中执行，所以有更好的性能、更原生的体验，支持的场景更多一些。 使用如下方式在本地构建一个支持 amd64 和 arm64 的 buildkit: EXTRA_BUILDKIT_ADDRESS=1.1.1.1 docker buildx create --use --name buildkit --driver=docker-container --driver-opt=image=buildkit:buildx-stable-1 docker buildx create --use --name buildkit --append ${EXTRA_BUILDKIT_ADDRESS} --platform linux/arm64 --driver=docker-container --driver-opt=image=buildkit:buildx-stable-1 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:7:2","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"使用 cross-compile 在本地架构构建不同架构的可执行程序 如果编程语言支持 cross-compilation，在不同的架构构建过程中，可以指定某个 stage 在当前机器架构中构建跨架构的二进制程序，后复制到运行时 stage 中。 官方文档见这里 如下为一个例子： # syntax=docker/dockerfile:1 FROM --platform=$BUILDPLATFORM golang:1.17-alpine AS build WORKDIR /src COPY . . ARG TARGETOS TARGETARCH RUN GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o /out/myapp . FROM alpine COPY --from=build /out/myapp /bin docker buildx build --platform=linux/amd64,linux/arm64 . ","date":"2024-01-29","objectID":"/kunpeng-arm64/:7:3","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"性能优化和调整 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:8:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"故障排除 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"clickhouse Illegal instruction (core dumped) 原因可以追溯上游 issue 上游的镜像构建可能在较新的机器中编译，新机器可能支持更多的特性，而 kunpeng ARMv8.2 的指令集无此特性导致不能运行。 可以切换到版本 clickhouse/clickhouse-server:23.3.9.55 或者 bitnami/clickhouse:23.3.9-debian-11-r0，2024.03.05 测试可用。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:1","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"k3s-root binaries cause segmentation fault on aarch64 nodes with 64k page size issue getconf PAGESIZE /var/lib/rancher/k3s/data/current/bin/mv Segmentation fault cento8 pagesize 64k, centos9 revert, donot use centos8 like os OS 页表选择 VA/PA BITS NR_CPUS NODES_SHIFT（NODES) RHEL/CentOS 7 Series 64K 48 4096 2 (4) RHEL/CentOS 8 Series 64K 48/52 4096 3 (8) RHEL/CentOS 9 Series 4K 48 4096 6 (64) SLE12-SP1-ARM 64K 42 128 2 (4) SLE12-SP2 4K 48 128 2 (4) SLE12-SP3/SP4 4K 48 256 2 (4) SLE15-SP1 4K 48 480 2 (4) SLE15-SP2/SP3/SP4 4K 48 768 6 (64) SLE15-SP4 (64K) 64K 52 768 6 (64) ubuntu 20.04 4K uos 1040d 64K kylinv10sp3 64K openEuler 20.03 64K openEuler 22.03 4K dockerhub 中镜像几乎都为在 4k pagesize 中编译的；当前 CI arm64 CI pagesize 为 4k；长期看新的操作系统几乎都为 4k，决定全栈 4k，不支持的操作系统通过重新编译 4k 的 linux 内核支持。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:2","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"redis Unsupported system page size [root@cosmos ~]# kubectl logs -f shared-redis-node-0 -c redis \u003cjemalloc\u003e: Unsupported system page size Could not connect to Redis at shared-redis.default.svc.cluster.local:26379: Connection refused 08:13:17.49 INFO ==\u003e Configuring the node as master \u003cjemalloc\u003e: Unsupported system page size \u003cjemalloc\u003e: Unsupported system page size 出这个错误的原因是因为 jemalloc 是在编译时确定 pagesize 的大小；在 pagesize 4k 的环境中编译放到 64k 环境中运行，即会报错。详细信息可以看 这里。基于当前遇到的问题，决定重新编译内核，改成 4k 的环境。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:3","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"WARNING Your kernel has a bug that could lead to data corruption during background save. Please upgrade to the latest stable kernel 编译内核 到4.19.90 出现新的问题，需要在 redis common 配置文件中添加 ignore-warnings ARM64-COW-BUG 1:M 28 Feb 2024 08:47:04.347 * Running mode=standalone, port=6379. 1:M 28 Feb 2024 08:47:04.347 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 28 Feb 2024 08:47:04.347 # Server initialized 1:M 28 Feb 2024 08:47:04.348 # WARNING Your kernel has a bug that could lead to data corruption during background save. Please upgrade to the latest stable kernel. 1:M 28 Feb 2024 08:47:04.348 # Redis will now exit to prevent data corruption. Note that it is possible to suppress this warning by setting the following config: ignore-warnings ARM64-COW-BUG 这是一个内核 bug： ref: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=977615 # Fixed in versions linux/5.9.15-1, linux/5.10~rc6-1~exp1, linux/5.10~rc7-1~exp1, linux/4.19.171-1 换了新的内核 4.19.x 修复。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:4","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"chrome/chromium Trace/breakpoint trap (core dumped) 错误原因同样是因为 pagesize，chromium 社区对 pagesize 64k 的支持不感兴趣，并且 chromium 当前由于SlotSpanMetadata 还不支持 64k，导致 chrome/chromium 是不支持 64k pagesize 的，如果想在 arm64 下运行，pagesize 要求 4k。同样的，还有一些类似的 issue： [https://github.com/electron/electron/issues/25387](Electron segmentation fault on CentOS7 aarch64(arm64)) browserless When the kernel PAGESIZE is 64k ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:5","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"postgres 在 pagesize 64k 下的 sql 查询会比 4k 下慢几个数量级 具体可以看这里，此文中说同一个 sql/同数据量下, pg 在 64k 的执行时间是 4k 下的 80+ 倍。 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:6","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"hadoop 版本 3.1.1 不支持 arm64，需要重新源码编译 TODO: ","date":"2024-01-29","objectID":"/kunpeng-arm64/:9:7","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"学到的经验和教训 linux kernel pagesize 尽量选择 4k，问题会少很多 善用 docker multiarch build ","date":"2024-01-29","objectID":"/kunpeng-arm64/:10:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"Q\u0026A 和讨论 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:11:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"参考 鲲鹏软件迁移 ppt 鲲鹏性能优化十板斧 5.0 ","date":"2024-01-29","objectID":"/kunpeng-arm64/:12:0","tags":["ops","linux","arm64","docker",""],"title":"鲲鹏 920 arm64 机器适配经验","uri":"/kunpeng-arm64/"},{"categories":null,"content":"文章简介：bitnami/postgres-ha 使用 pg_upgrade 全自动大版本升级 从 14 到 15 ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:0:0","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"Infrastructural notes ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:1:0","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"Reasons to upgrade 开始我所在产品计划做基于 k8s 的集群版，postgres 需要一个可靠的可以自动主从切换的 HA 方案。本着不重新造轮子的原则，选择了 bitnami/postgres-ha 的方案。调研时发现开了 witness/pg_rewind 后，pg 反复重启。为此把 witness/pg_rewind 关了后上了生产。后续由于我们百 G 存储的 pg 测试集群偶发了一次丢所有的数据，调查后决定开启 pg_rewind 防止丢数据。调研中发现最新的 bitnami/postgres-ha 基于 postgres 15, pg 14 开启后不工作，postgres 15 已经经过一段时间的生产环境验证，决定尝试 pg14 升级 15。 当前官方提供的升级方案包括: Dump \u0026 restore: 最简单也是最耗时的方法。将面临更长的停机时间，我们一个测试环境采用不落盘方案，30G 数据大约花费了 30 分钟，在更大的数据库或不同的硬件配置可能会有不同的执行时间; Logical replication: 停机时间最短的方式(所有数据库实例数据同步完成后切换旧集群到新集群这段时间会有短暂秒级服务不可用)。从 pg10 开始就内置了逻辑复制。 PostgreSQL official upgrade tool: pg_upgrade 官方维护了一个命令行工具, 用于此场景. 我所面临的场景要求原地升级，不接受使用一个新的 bitnami/postgres-ha 集群替换新的集群，允许比较短的维护时间窗口（可停服），故采用方案 3： PostgreSQL official upgrade tool pg_upgrade。如果对停机时间有严格要求的，还是建议采用方案 2 Logical replication。 因为我们的场景属于容器化场景，在一个 k8s 集群中部署了我们所有的数据库服务及业务服务，我们先在 docker 环境下熟悉一下 pg_upgrade 的操作。 ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:1:1","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"基于 docker 的 postgres 升级过程探索 调研过程中看到容器场景比较理想的迁移方案 blog Terabyte-scale PostgreSQL upgrade from 9.6 to 14 ，其中为他们的场景自制了一个用于升级的 docker image，此项目中对自己的介绍 This is a PoC for using pg_upgrade inside Docker -- learn from it, adapt it for your needs; don't expect it to work as-is!。接下来我们使用此方法复现一下大版本升级过程。 ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:2:0","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"升级过程 在此过程中我们会先启动一个 postgres14, 然后使用 tianon/postgres-upgrade:14-to-15 从 14 升级到 15. 然后启动 pg15 验证是否升级成功。 先看一下 docker-compose.yaml 文件 version: \"3\" services: db14: image: postgres:14-bookworm container_name: db14 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: test POSTGRES_DB: test PGDATA: /var/lib/postgresql/all/db14 volumes: - ./data/:/var/lib/postgresql/all ports: - 5432:5432 db15: image: postgres:15-bookworm container_name: db15 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: test POSTGRES_DB: test PGDATA: /var/lib/postgresql/all/db15 volumes: - ./data/:/var/lib/postgresql/all ports: - 5433:5432 upgrade: image: tianon/postgres-upgrade:14-to-15 container_name: upgrade environment: PGDATAOLD: /var/lib/postgresql/all/db14 PGDATANEW: /var/lib/postgresql/all/db15 POSTGRES_USER: test POSTGRES_PASSWORD: password command: [\"tail\", \"-f\", \"/dev/null\"] volumes: - ./data/:/var/lib/postgresql/all 首先我们把 pg 14 启动起来，并创建一些数据。 docker-compose up -d db14 [+] Building 0.0s (0/0) docker-container:multiarch [+] Running 2/2 ✔ Network docker_default Created 0.1s ✔ Container db14 Started export database='postgres://postgres:test@localhost:5432/test?sslmode=disable' docker-compose exec db14 psql \"$database\" -c 'select version();' ------------------------------------------------------------------------------------------------------------ ----------------- PostgreSQL 14.10 (Debian 14.10-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14 ) 12.2.0, 64-bit (1 row) docker-compose exec db14 psql \"$database\" -c 'CREATE TABLE IF NOT EXISTS t_test( ID INT NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL );' CREATE TABLE docker-compose exec db14 psql \"$database\" -c 'select count(*) from t_test;' count ------- 0 (1 row) docker-compose exec db14 psql \"$database\" -c 'insert into t_test SELECT generate_series(1,1) as key,repeat( chr(int4(random()*26)+65),4), (random()*(6^2))::integer,null,(random()*(10^4))::integer;' INSERT 0 1 docker-compose exec db14 psql \"$database\" -c 'select count(*) from t_test;' count ------- 1 (1 row) 开始升级过程 docker-compose stop db14 [+] Stopping 1/1 ✔ Container db14 Stopped docker-compose up -d upgrade [+] Running 1/1 ✔ Container upgrade Started docker-compose exec upgrade docker-upgrade pg_upgrade The files belonging to this database system will be owned by user \"postgres\". This user must also own the server process. The database cluster will be initialized with locale \"en_US.utf8\". The default database encoding has accordingly been set to \"UTF8\". The default text search configuration will be set to \"english\". Data page checksums are disabled. fixing permissions on existing directory /var/lib/postgresql/all/db15 ... ok creating subdirectories ... ok selecting dynamic shared memory implementation ... posix selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting default time zone ... Etc/UTC creating configuration files ... ok running bootstrap script ... ok performing post-bootstrap initialization ... ok syncing data to disk ... ok initdb: warning: enabling \"trust\" authentication for local connections initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /var/lib/postgresql/all/db15 -l logfile start Performing Consistency Checks ----------------------------- Checking cluster versions ok Checking database user is the install user ok Checking database connection settings ok Checking for prepared transactions ok Checking for system-defined composite types in user tables ok Checking for reg* data types in user tables ok Checking for contrib/isn with bigint-passing mismatch ok Creating dump of global objects ok Creating dump of database schemas ok Checking for presence of required libraries ok Checking database user is the install user ok Checking for prepared transactions ok Checking for ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:2:1","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"升级注意事项 如果使用了 PostgreSQL extension， 需要手动安装 在升级容器新版本 PG 手动安装 apt update \u0026\u0026 apt install postgresql-14-pgaudit apt update \u0026\u0026 apt install postgresql-15-pgaudit 旧 postgres:14-alpine 使用 postgres:15-bookworm-pg_upgrade 新运行环境 postgres:15-alpine 使用 postgres:15-bookworm 做 pg_upgrade 遇到 warning: database \"postgres\" has no actual collation version, but a version was recorded REINDEX REINDEX DATABASE splat;ALTER DATABASE splat REFRESH COLLATION VERSION; ``` ```bash ERROR: invalid collation version change glibc 版本/系统版本变化导致 pg_upgrade 升级后报错 官方建议 新旧及升级容器的 linux 运行时都使用相同的. 比如此例子中应都使用 alpine:3.14. 最终，写了完整验证自动化脚本: # docker-compose.yaml version: \"3\" services: db14: image: postgres:14-bookworm container_name: db14 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: test POSTGRES_DB: test PGDATA: /var/lib/postgresql/all/db14 volumes: - ./data/:/var/lib/postgresql/all ports: - 5432:5432 db15: image: postgres:15-bookworm container_name: db15 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: test POSTGRES_DB: test PGDATA: /var/lib/postgresql/all/db15 volumes: - ./data/:/var/lib/postgresql/all ports: - 5433:5432 upgrade: image: tianon/postgres-upgrade:14-to-15 container_name: upgrade environment: PGDATAOLD: /var/lib/postgresql/all/db14 PGDATANEW: /var/lib/postgresql/all/db15 POSTGRES_USER: test POSTGRES_PASSWORD: password command: [\"tail\", \"-f\", \"/dev/null\"] volumes: - ./data/:/var/lib/postgresql/all # test.sh #!/usr/bin/env bash set -o errtrace set -o errexit set -o nounset set -o pipefail set -o xtrace cd \"$(dirname \"$0\")\" export database='postgres://postgres:test@localhost:5432/test?sslmode=disable' docker-compose down rm -rf data sleep 3 docker-compose up -d db14 sleep 5 docker-compose exec db14 pg_isready -d \"$database\" -t 20 docker-compose exec db14 psql \"$database\" -c 'select version();' docker-compose exec db14 psql \"$database\" -c 'CREATE TABLE IF NOT EXISTS t_test( ID INT NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL );' docker-compose exec db14 psql \"$database\" -c 'select count(*) from t_test;' docker-compose exec db14 psql \"$database\" -c 'insert into t_test SELECT generate_series(1,1) as key,repeat( chr(int4(random()*26)+65),4), (random()*(6^2))::integer,null,(random()*(10^4))::integer;' docker-compose exec db14 psql \"$database\" -c 'select count(*) from t_test;' docker-compose stop db14 docker-compose up -d upgrade docker-compose exec upgrade docker-upgrade pg_upgrade docker-compose up -d db15 sleep 3 docker-compose exec db15 psql \"$database\" -c 'ALTER DATABASE test REFRESH COLLATION VERSION;' docker-compose exec db15 vacuumdb --username=postgres --all --analyze-in-stages docker-compose logs -f db15 ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:2:2","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"基于 k8s 的 bitnami/postgres-ha 升级的尝试 Terabyte-scale PostgreSQL upgrade from 9.6 to 14 中有描述原地升级方法，在此复现，并将过程自动化。 重复升级方法。 首先创建一个 postgres 14 的 pg 集群 helm pull oci://registry-1.docker.io/bitnamicharts/postgresql-ha --version 12.3.1 helm upgrade --install shared postgresql-ha-12.3.1.tgz --set postgresql.image.tag='14.10.0-debian-11-r6' --set postgresql.replicaCount=1 --set postgresql.sharedPreloadLibraries='\"pgaudit, repmgr, pg_stat_statements\"' 先准备升级 pod # 停止pg kubectl scale --replicas=0 sts shared-postgresql-ha-postgresql cat \u003c\u003c'EOF' | kubectl apply -f - apiVersion: batch/v1 kind: Job metadata: name: pgupgrade spec: completions: 1 backoffLimit: 1 template: spec: restartPolicy: Never volumes: - name: data-sharedx-pgha-0 persistentVolumeClaim: claimName: data-shared-postgresql-ha-postgresql-0 containers: - name: pg-upgrade image: tianon/postgres-upgrade:14-to-15 imagePullPolicy: IfNotPresent command: - \"sh\" - -c - \"tail -f /dev/null\" env: - name: PGDATABASE value: /bitnami/postgresql/ - name: PGDATAOLD value: /bitnami/postgresql/data - name: PGDATANEW value: /bitnami/postgresql/datanew volumeMounts: - mountPath: \"/bitnami/postgresql\" name: data-sharedx-pgha-0 EOF # 获得升级 shell kubectl exec -it $(kubectl get pod -l batch.kubernetes.io/job-name=pgupgrade -o name) -- bash ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:3:0","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"尝试 docker-upgrade pg_upgrade Performing Consistency Checks ----------------------------- Checking cluster versions ok *failure* Consult the last few lines of \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T060414.361/log/pg_upgrade_server.log\" for the probable cause of the failure. connection to server on socket \"/var/lib/postgresql/.s.PGSQL.50432\" failed: No such file or directory Is the server running locally and accepting connections on that socket? could not connect to source postmaster started with the command: \"/usr/lib/postgresql/14/bin/pg_ctl\" -w -l \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T060414.361/log/pg_upgrade_server.log\" -D \"/bitnami/postgresql/data\" -o \"-p 50432 -b -c listen_addresses='' -c unix_socket_permissions=0700 -c unix_socket_directories='/var/lib/postgresql'\" start Failure, exiting cat /bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T060414.361/log/pg_upgrade_server.log ----------------------------------------------------------------- pg_upgrade run on Tue Nov 28 06:04:14 2023 ----------------------------------------------------------------- command: \"/usr/lib/postgresql/14/bin/pg_ctl\" -w -l \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T060414.361/log/pg_upgrade_server.log\" -D \"/bitnami/postgresql/data\" -o \"-p 50432 -b -c listen_addresses='' -c unix_socket_permissions=0700 -c unix_socket_directories='/var/lib/postgresql'\" start \u003e\u003e \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T060414.361/log/pg_upgrade_server.log\" 2\u003e\u00261 waiting for server to start....postgres: could not access the server configuration file \"/bitnami/postgresql/data/postgresql.conf\": No such file or directory stopped waiting pg_ctl: could not start server Examine the log output. 看起来旧数据需要 postgresql.conf 才可以启动。现将所有旧配置都复制到数据目录 kubectl exec -it shared-postgresql-ha-postgresql-0 -- cp /opt/bitnami/postgresql/conf/postgresql.conf /bitnami/postgresql/data root@task-pg-upgrade-545cbc8cdf-6z8mf:/var/lib/postgresql# cat /bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T064300.020/log/pg_upgrade_server.log ----------------------------------------------------------------- pg_upgrade run on Tue Nov 28 06:43:00 2023 ----------------------------------------------------------------- command: \"/usr/lib/postgresql/14/bin/pg_ctl\" -w -l \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T064300.020/log/pg_upgrade_server.log\" -D \"/bitnami/postgresql/data\" -o \"-p 50432 -b -c listen_addresses='' -c unix_socket_permissions=0700 -c unix_socket_directories='/var/lib/postgresql'\" start \u003e\u003e \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T064300.020/log/pg_upgrade_server.log\" 2\u003e\u00261 waiting for server to start....2023-11-28 06:43:00.206 GMT [177] LOG: could not open configuration directory \"/bitnami/postgresql/data/conf.d\": No such file or directory 2023-11-28 06:43:00.207 GMT [177] FATAL: configuration file \"/bitnami/postgresql/data/postgresql.conf\" contains errors stopped waiting pg_ctl: could not start server Examine the log output. 执行 mkdir -p /bitnami/postgresql/data/conf.d 修复 cat/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T064744.773/log/pg_upgrade_server.log ----------------------------------------------------------------- pg_upgrade run on Tue Nov 28 06:47:44 2023 ----------------------------------------------------------------- command: \"/usr/lib/postgresql/14/bin/pg_ctl\" -w -l \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T064744.773/log/pg_upgrade_server.log\" -D \"/bitnami/postgresql/data\" -o \"-p 50432 -b -c listen_addresses='' -c unix_socket_permissions=0700 -c unix_socket_directories='/var/lib/postgresql'\" start \u003e\u003e \"/bitnami/postgresql/datanew/pg_upgrade_output.d/20231128T064744.773/log/pg_upgrade_server.log\" 2\u003e\u00261 waiting for server to start....2023-11-28 06:47:44.970 GMT [255] FATAL: 58P01: could not access file \"repmgr\": No such file or directory 2023-11-28 06:47:44.970 GMT [255] LOCATION: internal_load_library, dfmgr.c:208 2023-11-28 06:47:44.970 GMT [","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:3:1","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"自动化 k8s bitnami/postgres-ha 升级步骤比较多，在此汇总出自动化, 代码放到 exfly/bitnami-pg-upgrade. 可以根据情况适量修改。 ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:3:2","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"参考 官方文档 Terabyte-scale PostgreSQL upgrade from 9.6 to 14 bitnami 提供的升级 postgres 的方法 Better documentation needed on major version upgrades ","date":"2023-12-04","objectID":"/postgres-upgrade-bitnami-ha/:4:0","tags":["db","postgres","bitnami","bitnami/postgres-ha","pg_upgrade"],"title":"在 k8s 实际生产中 bitnami/postgres-ha 的一次 14 升级 15 经验及过程","uri":"/postgres-upgrade-bitnami-ha/"},{"categories":null,"content":"文章简介：本文介绍了一种跨平台（支持 Mac，Linux，Windows）的命令行渲染 draw.io 图表文件为图片的方法. draw.io 是一款比较简单的画流程图、架构图的开源免费软件。具体介绍可以看下 wikipedia 介绍。一般我们使用他的网页版画图，下载原文件及渲染的 PNG 文件用于文档中. 但如果希望修改一下图内容，需要浏览器打开 draw.io，上传源文件，修改，下载，修改文档中使用到这个图的链接。是否有一种方法在本地 CLI 渲染成固定文件名字的文件，本地修改后一键修改呢？ 首先 vscode 有 draw.io 的插件，可以直接 GUI 方式修改图，保存到 draw.io 源文件。使用如下命令行工具可以将当前目录下的 *.drawio 渲染成 *.draio.png. 直接将生成的 png 文件在 markdown 中引用。修改图的工作流变成 vscode draw.io 插件修改图后保存，后直接执行脚本即可。 使用脚本 render.sh 如下. 首先下载安装 drawio-desktop, 并根据平台修改脚本 draw_io 部分。 #!/usr/bin/env bash set -o errtrace set -o errexit set -o nounset set -o pipefail set -o xtrace cd \"$(dirname \"$0\")\" draw_io='/Applications/draw.io.app/Contents/MacOS/draw.io' for drawio_file in $(ls *.drawio); do echo \"$drawio_file\" ${draw_io} -x --format png -o \"$drawio_file.png\" \"$drawio_file\" done ⚠️ 注意：本人只测试过 Mac，其他平台可能会少量修改脚本内容 ","date":"2023-10-29","objectID":"/draw.io-cli-render/:0:0","tags":["tools","draw.io","cli"],"title":"跨平台命令行渲染 draw.io 方法","uri":"/draw.io-cli-render/"},{"categories":null,"content":"文章简介：本文简单介绍了 Hadoop Resource Manager 页面 HA 方式，并基于此实现了一种 Openrestry + Lua 编写动态脚本，将 Hadoop Resource Manager 使用 Nginx 反向代理出来的方法。其中提供的代码，可直接运行起来。 ","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:0:0","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"问题场景 Hadoop Resource Manager(RM) 的管理页面采用了类似客户端负载均衡的 HA 方案。Resource Manager 分为两部分，Master/Standby。当 http 请求到 Standby，Standby 将重定向到 Master，这样浏览器即可访问到实际工作的 RM 了。而这种场景下，使用 Nginx 作为 Load Balance 代理 RM 就不合适，因为对 nginx 来说，nginx 期望所有的实例都可以对外提供服务。Nginx 是否可以提供一种机制，找到实际的 RM master 节点，将所有请求发送到这个节点呢? ","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:1:0","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"解决办法 HadoopResourceManagerHA 文档 对于 Load Balancer 有如下描述 If you are running a set of ResourceManagers behind a Load Balancer (e.g. Azure or AWS ) and would like the Load Balancer to point to the active RM, you can use the /isActive HTTP endpoint as a health probe. http://RM_HOSTNAME/isActive will return a 200 status code response if the RM is in Active HA State, 405 otherwise. 在使用的 HadoopResourceManager 版本 没有 /isActive 这个路由，所以不能使用这个办法。 查看资料的时候，发现 http://ip:8088/jmx?qry=Hadoop:service=ResourceManager,name=ClusterMetrics 中会有主节点的 hostname, 并且这个接口只有 Master 才会有响应。基于这个特征，我们可以通过检查是否可以返回主节点信息判断是否为主。 另外，Standby 307 的 response body 中也明确标识了当前是 standby，此行为也可以作为判断是否为主的标识。 当前我们各组建版本: ResourceManager 版本3.1.1.3.1.0.0-78 from e4f82af51faec922b4804d0232a637422ec29e64 by jenkins source checksum 47cebb2682958b68f58d47415f5d2555 on 2018-12-06T12:28Z Hadoop version: 3.1.1.3.1.0.0-78 from e4f82af51faec922b4804d0232a637422ec29e64 by jenkins source checksum eab9fa2a6aa38c6362c66d8df75774 on 2018-12-06T12:26Z 具体的响应的样子如下图 尝试请求 Standby 看下效果: curl xx.xx.xx.xx:8088/ -k -v * Trying xx.xx.xx.xx:8088... * Connected to xx.xx.xx.xx (xx.xx.xx.xx) port 8088 (#0) \u003e GET / HTTP/1.1 \u003e Host: xx.xx.xx.xx:8088 \u003e User-Agent: curl/7.88.1 \u003e Accept: */* \u003e \u003c HTTP/1.1 307 Temporary Redirect \u003c Date: Wed, 25 Oct 2023 02:05:48 GMT \u003c Cache-Control: no-cache \u003c Expires: Wed, 25 Oct 2023 02:05:48 GMT \u003c Date: Wed, 25 Oct 2023 02:05:48 GMT \u003c Pragma: no-cache \u003c Content-Type: text/plain;charset=utf-8 \u003c X-Frame-Options: SAMEORIGIN \u003c Location: http://bigdata3:8088/ \u003c Content-Length: 43 \u003c This is standby RM. The redirect url is: / curl 'http://xx.xx.xx.xx:8088/jmx?qry=Hadoop:service=ResourceManager,name=ClusterMetrics' -k -v * Trying xx.xx.xx.xx:8088... * Connected to xx.xx.xx.xx (xx.xx.xx.xx) port 8088 (#0) \u003e GET /jmx?qry=Hadoop:service=ResourceManager,name=ClusterMetrics HTTP/1.1 \u003e Host: xx.xx.xx.xx:8088 \u003e User-Agent: curl/7.88.1 \u003e Accept: */* \u003e \u003c HTTP/1.1 307 Temporary Redirect \u003c Date: Wed, 25 Oct 2023 02:10:32 GMT \u003c Cache-Control: no-cache \u003c Expires: Wed, 25 Oct 2023 02:10:32 GMT \u003c Date: Wed, 25 Oct 2023 02:10:32 GMT \u003c Pragma: no-cache \u003c Content-Type: text/plain;charset=utf-8 \u003c X-Frame-Options: SAMEORIGIN \u003c Location: http://bigdata3:8088/jmx?qry=Hadoop%3Aservice%3DResourceManager%2Cname%3DClusterMetrics \u003c Content-Length: 109 \u003c This is standby RM. The redirect url is: /jmx?qry=Hadoop%3Aservice%3DResourceManager%2Cname%3DClusterMetrics * Connection #0 to host xx.xx.xx.xx left intact 请求 Master 节点，看下请求是什么样子的: curl xx.xx.xx.xx:8088/ -k -v * Trying xx.xx.xx.xx:8088... * Connected to xx.xx.xx.xx (xx.xx.xx.xx) port 8088 (#0) \u003e GET / HTTP/1.1 \u003e Host: xx.xx.xx.xx:8088 \u003e User-Agent: curl/7.88.1 \u003e Accept: */* \u003e \u003c HTTP/1.1 302 Found \u003c Date: Wed, 25 Oct 2023 02:06:01 GMT \u003c Cache-Control: no-cache \u003c Expires: Wed, 25 Oct 2023 02:06:01 GMT \u003c Date: Wed, 25 Oct 2023 02:06:01 GMT \u003c Pragma: no-cache \u003c Content-Type: text/plain;charset=utf-8 \u003c X-Frame-Options: SAMEORIGIN \u003c Vary: Accept-Encoding \u003c Location: http://xx.xx.xx.xx:8088/cluster \u003c Content-Length: 0 curl 'http://xx.xx.xx.xx:8088/jmx?qry=Hadoop:service=ResourceManager,name=ClusterMetrics' -k -v * Trying xx.xx.xx.xx:8088... * Connected to xx.xx.xx.xx (xx.xx.xx.xx) port 8088 (#0) \u003e GET /jmx?qry=Hadoop:service=ResourceManager,name=ClusterMetrics HTTP/1.1 \u003e Host: xx.xx.xx.xx:8088 \u003e User-Agent: curl/7.88.1 \u003e Accept: */* \u003e \u003c HTTP/1.1 200 OK \u003c Date: Wed, 25 Oct 2023 02:09:49 GMT \u003c Cache-Control: no-cache \u003c Expires: Wed, 25 Oct 2023 02:09:49 GMT \u003c Date: Wed, 25 Oct 2023 02:09:49 GMT \u003c Pragma: no-cache \u003c Content-Type: application/json; charset=utf8 \u003c X-Frame-Options: SAMEORIGIN \u003c Vary: Accept-Encoding \u003c Access-Control-Allow-Methods: GET \u003c Access-Control-Allow-Origin: * \u003c Transfer-Encoding: chunked \u003c { \"beans\" : [ { \"name\" : \"Hadoop:service=ResourceManager,name=ClusterMetrics\", \"modelerType\" : \"ClusterMetrics\", \"tag.ClusterMetrics\" : \"ResourceManager\", \"tag.Context\" : \"yarn\", \"tag.Hostname\" : \"bigdata3\", \"NumActiveNMs\" : 3, \"NumDecommissioningNMs\" : 0, \"NumDecomm","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:2:0","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"nginx 使用 nginx 原生支持的能力判断主 upstream 的方法可以考虑 1，2，3 三种方法。 翻了下 nginx upstream 的文档, Dynamically configurable group with periodic health checks is available as part of our commercial subscription:. 所以准备尝试一下 ngx_http_upstream_hc_module。我们直接使用 nginx:latest 镜像，配置如下: upstream backend { server bing.com:80; # server www.baidu.com:80; } server { listen 8080; server_name localhost; location / { proxy_set_header Host 'bing.com'; proxy_pass http://backend; health_check; } } 启动 nginx 会报如下错误: 2023/10/23 01:56:53 [emerg] 1#1: unknown directive \"health_check\" in /etc/nginx/conf.d/default.conf:13 错误表示内置的 nginx 没编译 ngx_http_upstream_hc_module。如果希望使用这种方式，需要自编译 nginx 添加此模块。暂不想自编译 nginx，此方法弃用。 ","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:2:1","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"openresty+lua 换一个思路，如果我们期望的行为是在请求的时候，能够区分开，使用基于 nginx 的 更动态的 openresty+lua 组合写脚本方案找到当前的主，问题即可解决了。 https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html 首先准备一下 openresty 的 docker 镜像 docker build --platform linux/amd64 -t openresty:dev -f Dockerfile . FROM openresty/openresty:latest RUN sed -E -i 's/(deb|security).debian.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apt/sources.list RUN DEBIAN_FRONTEND=noninteractive apt-get update \\ \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\ wget \\ curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* nginx.conf 文件如下 nginx.conf_root daemon off; worker_processes 3; user root root; events { worker_connections 1024; } http { default_type application/octet-stream; include /etc/nginx/conf.d/*.conf; } 默认路由核心逻辑如下，请求进来后，通过执行 shell 请求所有节点，找出当前的主的 hostname, 然后将请求路由到 hostname 对应的 upstream default.conf server { listen 80; server_name localhost; location / { access_by_lua_block { os.execute(\"/bin/bash /etc/nginx/conf.d/hadoop-status.sh \u003e /tmp/hadoop-status.tmp\") handle = io.open(\"/tmp/hadoop-status.tmp\", \"r\") result = handle:read(\"*a\") handle:close() ngx.log(ngx.ERR, result) ngx.exec(\"@\"..result) } } location @bigdata1 { proxy_pass http://192.168.100.41:8088; } location @bigdata2 { proxy_pass http://192.168.100.42:8088; } location @bigdata3 { proxy_pass http://192.168.100.43:8088; } } 找到主的节点的方法比较简单，遍历所有的节点，有 jmx 返回的即为 Master。Master 中包含当前的 hostname，从脚本返回，传递给 nginx 做路由选择即可. 代码如下: hadoop-status.sh #!/usr/bin/env bash set -o errtrace set -o errexit set -o nounset # set -o pipefail # set -o xtrace cd \"$(dirname \"$0\")\" # cat /etc/nginx/conf.d/host_ip.txt | while read line; cat /etc/nginx/conf.d/host_ip.txt | while read line; do # 忽略备注行 if [[ $line == \\#* ]] then continue fi linearray=( $line ) RAW_IP=${linearray[0]} RAW_HOSTNAME=${linearray[1]} MASTER_NODE=$(wget -qO- \"http://${RAW_IP}:8088/jmx?qry=Hadoop:service=ResourceManager,name=ClusterMetrics\" | sed 's/,/\\n/g' | grep 'tag.Hostname' | sed 's/tag.Hostname\" : \"//g' | sed 's/[\" ]//g') if [ \"${RAW_HOSTNAME}\" == \"${MASTER_NODE}\" ]; then echo -n $RAW_HOSTNAME fi done host_ip.txt 中包含所有节点 ip 和 hostname 192.168.100.41 bigdata1 192.168.100.42 bigdata2 192.168.100.43 bigdata3 执行下边命令后，访问 127.0.0.1:12346 看下效果 docker run --rm -it -p 12346:80 -v $(pwd)/:/etc/nginx/conf.d/ openresty:dev nginx -c /etc/nginx/conf.d/nginx.conf_root ","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:2:2","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"总结 如上，使用 openresty+lua 编写检查下游 upstream 中所有节点主为何，然后 nginx 请求路由到主。 ","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:3:0","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"ref 使用 Nginx+Lua 代理 Hadoop HA, 思路来自于此，此文中的方法只接受两个节点，生产基本不可用 ","date":"2023-10-22","objectID":"/hadoop-resource-manager-behind-nginx/:4:0","tags":["nginx","openresty","lua","bigdata"],"title":"Hadoop ResourceManager Behind Nginx,  使用 Nginx 反代 Hadoop ResourceManager 管理界面","uri":"/hadoop-resource-manager-behind-nginx/"},{"categories":null,"content":"文章简介：如何配置一个跨 linux 发行版解压即用的 python 运行环境 ","date":"2023-09-24","objectID":"/portable-python/:0:0","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"场景分析 我们所处的工作环境偏 ToB 离线安装升级。python 在运维工具上有成熟的 ansible 等运维工具。而在实际生产使用过程中，需要适配不同的 linux 发行版。在这过程中，我们需要一个比较 稳定开箱即用的 python 运行环境 安装简单 (最好解压即可运行) 第三方包安装尽可能简单 我们所面向的问题： linux 发行版众多，一些为 python2，一些为 python3；代码可以兼容 python2 和 python3，但部分依赖不兼容 python2 和 python3；并且 python2 已经 EOL，不期望为它花费力气做适配 linux 发行版 glibc 版本各不相同，高版本 glibc 下编译 python 放到低版本 glibc 运行环境下会 glibc 版本检查失败 ","date":"2023-09-24","objectID":"/portable-python/:1:0","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"解决办法 ","date":"2023-09-24","objectID":"/portable-python/:2:0","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"docker 第一个想到的是 docker. 基于 docker 的容器 python 运行环境，docker image 中包含所有的 python 运行时依赖，包括 python 代码及动态库（包括 glibc） FROM python:3.9-slim as base FROM base as builder RUN mkdir /install WORKDIR /install COPY requirements.txt /requirements.txt RUN sed -E -i -e 's/deb.debian.org/mirrors.aliyun.com/g' -e 's/security.debian.org/mirrors.aliyun.com/g' /etc/apt/sources.list RUN apt update --allow-insecure-repositories \\ \u0026\u0026 apt install -y gcc python3.9-dev libffi-dev -y RUN pip install --no-cache-dir --trusted-host mirrors.aliyun.com --prefix=/install -r /requirements.txt # FROM gcr.io/distroless/python3 FROM base COPY --from=builder /install /usr/local WORKDIR /app COPY hello.py /app/ CMD [\"hello.py\", \"/etc\"] 但这种方式也有缺陷，docker 采用 namespace 的方式将 / 与 宿主机的 / 隔离开，容器如果期望操作宿主机的 systemd/crond 会比较复杂，可能会需要 docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh 。另外一种方式是使用 ssh 免密操作宿主机 （ssh root@hostip sh）, 同样操作复杂。 ","date":"2023-09-24","objectID":"/portable-python/:2:1","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"portable python 想法来自于 golang, 所有的依赖都以静态编译的方式编译到一个文件中，单文件复制到其他机器即可运行，之前在静态编译 chrony, 基于这个想法，调研下 python 有没有类似的能力. 将如下命令放到 Dockerfile 中 # Dockerfile FROM ubuntu:22.04 RUN sed -E -i -e 's/(archive|ports).ubuntu.com/mirrors.aliyun.com/g' -e '/security.ubuntu.com/d' /etc/apt/sources.list RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends ca-certificates zstd wget \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* \u0026\u0026 \\ update-ca-certificates WORKDIR /opt/ ARG filename=cpython-3.10.13+20230826-x86_64-unknown-linux-gnu-debug-full RUN wget https://github.com/indygreg/python-build-standalone/releases/download/20230826/${filename}.tar.zst \\ \u0026\u0026 unzstd ${filename}.tar.zst \\ \u0026\u0026 mkdir -p portable \u0026\u0026 tar -xvf ${filename}.tar -C portable \\ \u0026\u0026 rm -rf ${filename}.tar.zst ${filename}.tar ENV PYTHON_HOME=/opt/portable/python/ RUN ${PYTHON_HOME}/install/bin/pip3 install --debug --verbose requests psycopg2-binary ansible -i https://pypi.tuna.tsinghua.edu.cn/simple 生成的 python 包的命令 docker build -t portable-python:dev --build-arg HTTPS_PROXY=\"http://host.docker.internal:7890\" . docker run --rm -it -v \"$(pwd):/host\" --workdir=/opt/portable portable-python:dev tar -cvf /host/portable-python-3.10.13.tar python 当前已经测试过的操作系统: centos:7 ubuntu:20.04 ubuntu:22.04 rockylinux:8-minimal rockylinux:9-minimal openeuler:20.03-lts openeuler:22.03-lts 测试过的 pip 包: requests psycopg2-binary ansible ","date":"2023-09-24","objectID":"/portable-python/:2:2","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"总结 TODO: 这里缺一个总结 处理问题的思路决定了做事的效率. ","date":"2023-09-24","objectID":"/portable-python/:3:0","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"参考 Keep These Portable Python Builds for Linux Always With You indygreg/python-build-standalone ","date":"2023-09-24","objectID":"/portable-python/:4:0","tags":["python","ops","portable","linux"],"title":"如何配置一个跨 linux 发行版解压即用的 python 运行环境(Portable Python)","uri":"/portable-python/"},{"categories":null,"content":"文章简介：记一次 grpc 短连接未关闭导致的资源泄漏问题的排查过程 ","date":"2023-08-24","objectID":"/go-grpc-short-link-is-not-closed-causing-leak/:0:0","tags":["go","grpc","bug","内存泄漏"],"title":"记一次 grpc 短连接未关闭导致的资源泄漏问题的排查过程","uri":"/go-grpc-short-link-is-not-closed-causing-leak/"},{"categories":null,"content":"问题现象 正在欢快的写 bug 的时候，测试师傅突然呼叫我说一个测试环境的 cpu 90%+, 内存 10GB(超出预期太多)。赶紧登录到机器上看下现场。 ","date":"2023-08-24","objectID":"/go-grpc-short-link-is-not-closed-causing-leak/:1:0","tags":["go","grpc","bug","内存泄漏"],"title":"记一次 grpc 短连接未关闭导致的资源泄漏问题的排查过程","uri":"/go-grpc-short-link-is-not-closed-causing-leak/"},{"categories":null,"content":"问题排查过程 首先使用 fgprof 看下当前比较耗时的代码逻辑在哪里： export ENDPOINT=http://localhost:9699 go tool pprof -http=:8081 ${ENDPOINT}/debug/fgprof 图中可以看到 grpc.Server 相关的时间时间比较多。此图可以反映出慢处理路径。但看到 grpc server 慢，不能反映出是 grpc server 处理慢。 其次看 profile： go tool pprof -http=:8081 \"${ENDPOINT}/debug/pprof/profile?seconds=30\" 此图反映出业务占用 On-CPU 时间 37%，在努力消费 kafka 中的数据； 30% cpu 在执行 runtime.gcBgMarkWorker，其他时间占比比较少，可忽略。37%时间消耗在消费 kafka 消息中，这个符合预期，这个业务主要做消费 kafka 中的数据。30% 的 runtime.gcBgMarkWorker 执行逻辑从名字可以看出，在执行 gc 的逻辑；runtime.scanobject 占用了 30%的时间，大量时间消耗在 gc 上，猜测是有内存泄漏. 查看 heapdump 中的 inused_objects： go tool pprof -http=:8081 ${ENDPOINT}/debug/pprof/heap 从图中可以看到 grpc.DialContext/grpc.(*addrConn).resetTransport 函数执行路径分配了过多内存。grpc.DialContext 的作用是创建 grpc 连接，grpc.(*addrConn).resetTransport 是 http2 连接保活. 查看当前 goroutine stacktrace，多少函数调用栈包含 resetTransport 即表示当前进程有多少 grpc 连接. 恐怖的 20348，并且在持续的增大. 问题可以大胆猜测，由于 kafka 消费过程中每个消息执行一次 service.SendNoticeCallBack, 每次掉用 grpc 创建连接 rpcclient.NewMsgDogServiceGrpcClient， 而没有关闭连接，导致大量的连接持续后台保活，grpc 连接过多对象过多导致 gc 时间过多，内存占用多。 基于这个假设，查看这个grpc客户端对应的服务端的连接状态: 服务端连接也基本与上边的客户端连接数对的上。 ","date":"2023-08-24","objectID":"/go-grpc-short-link-is-not-closed-causing-leak/:2:0","tags":["go","grpc","bug","内存泄漏"],"title":"记一次 grpc 短连接未关闭导致的资源泄漏问题的排查过程","uri":"/go-grpc-short-link-is-not-closed-causing-leak/"},{"categories":null,"content":"解决办法 先简单打个 patch，找到 service.SendNoticeCallBack 函数，把 rpcclient.NewMsgDogServiceGrpcClient 返回的 grpc 连接在使用完后及时关闭。 长期解决办法: 即grpc复用连接，在初始化应用的时候初始化一个 grpc 连接，作为依赖传递给此函数，使用的时候直接使用创建好的连接掉用服务. ","date":"2023-08-24","objectID":"/go-grpc-short-link-is-not-closed-causing-leak/:3:0","tags":["go","grpc","bug","内存泄漏"],"title":"记一次 grpc 短连接未关闭导致的资源泄漏问题的排查过程","uri":"/go-grpc-short-link-is-not-closed-causing-leak/"},{"categories":null,"content":"文章简介：tcpdump 可以收到包，udp server 无法收包，简单的排查过程 ","date":"2023-03-09","objectID":"/udp-tcpdump-receive-pkg-sock-not/:0:0","tags":[],"title":"tcpdump 可以收到包，udp server 无法收包，最终发现是 uRPF 的锅 DRAFT","uri":"/udp-tcpdump-receive-pkg-sock-not/"},{"categories":null,"content":"现象 我们的某机器从 0.0.0.0:514/udp 接受数据，ip 为 10.1.1.11/24 和 10.1.2.10/24.另一台机器 10.1.1.10/24 向 10.1.1.11/24 发送 udp 包。 machine1: 1. 10.1.1.10/24 machine2: 1. 10.1.2.10/24 2. 10.1.1.11/24 我们使用 tcpdump 查看收到了数据，命令如下： tcpdump -i any host 10.1.1.10 and udp port 514 or icmp -X nstat -s -az | grep TcpExtIPReversePathFilter TcpExtIPReversePathFilter xxx 0.0 而 machine1 514/udp 服务没有收到数据。 ","date":"2023-03-09","objectID":"/udp-tcpdump-receive-pkg-sock-not/:1:0","tags":[],"title":"tcpdump 可以收到包，udp server 无法收包，最终发现是 uRPF 的锅 DRAFT","uri":"/udp-tcpdump-receive-pkg-sock-not/"},{"categories":null,"content":"如何快速恢复的 给 machine2 服务添加路由后， ip route add 10.1.1.10/32 via 10.1.1.1 ","date":"2023-03-09","objectID":"/udp-tcpdump-receive-pkg-sock-not/:2:0","tags":[],"title":"tcpdump 可以收到包，udp server 无法收包，最终发现是 uRPF 的锅 DRAFT","uri":"/udp-tcpdump-receive-pkg-sock-not/"},{"categories":null,"content":"原因 TODO: 找到了大致方向，尝试复现验证下 重新梳理 linux 的实现机制，在 这里 找到了类似的现象，无路由 udp 无法收包。 解决办法： sysctl net.ipv4.conf.all.rp_filter=0 sysctl net.ipv4.conf.eth0.rp_filter=0 ","date":"2023-03-09","objectID":"/udp-tcpdump-receive-pkg-sock-not/:3:0","tags":[],"title":"tcpdump 可以收到包，udp server 无法收包，最终发现是 uRPF 的锅 DRAFT","uri":"/udp-tcpdump-receive-pkg-sock-not/"},{"categories":null,"content":"文章简介：介绍在 gitlab 中使用 chatgpt 做 code review 的方法和代码 最近 chatgpt 非常火暴，实际体验感受了一下，其在作为辅助工具上还是可以使用的. 当前 chatgpt apitoken 接入在国内是可以直接访问的. ","date":"2023-02-28","objectID":"/gitlab-chatgpt-mr-code-review/:0:0","tags":["gitlab","devops","chatgpt","code_review"],"title":"在 gitlab 中使用 chatgpt 做 code review","uri":"/gitlab-chatgpt-mr-code-review/"},{"categories":null,"content":"准备 首先，我们需要一些魔法手段获得一个账号以及生成一个 chatgpt 的 apitoken. ","date":"2023-02-28","objectID":"/gitlab-chatgpt-mr-code-review/:1:0","tags":["gitlab","devops","chatgpt","code_review"],"title":"在 gitlab 中使用 chatgpt 做 code review","uri":"/gitlab-chatgpt-mr-code-review/"},{"categories":null,"content":"当前使用的 prompt Bellow is the code patch, please help me do a brief code review, if any bug risk and improvement suggestion are welcome %s 除此之外，还有很多好玩有用的 prompt 见 这里 ","date":"2023-02-28","objectID":"/gitlab-chatgpt-mr-code-review/:2:0","tags":["gitlab","devops","chatgpt","code_review"],"title":"在 gitlab 中使用 chatgpt 做 code review","uri":"/gitlab-chatgpt-mr-code-review/"},{"categories":null,"content":"开始制作 code review 脚本 CI 中使用 chatgpt 做 code review 的主要实现思路是: 使用 git diff 生成当前 mr 的 patch 向我的后端转发服务器发送 patch， 返回 code review 的结果 向当前 mr 创建评论，评论内容为 code review 的结果 如下是我们自动 code review 脚本,其中 http://1.1.1.1:5151/ 是我开发的一个后端服务，将的请求代理到 chatgpt中. git fetch git diff origin/${CI_MERGE_REQUEST_TARGET_BRANCH_NAME:-\"main\"}...origin/${CI_COMMIT_REF_NAME} | tee mr.patch echo \"patch\" CHATBODY=$(jq --null-input --arg category \"code_review\" --arg prompt \"$(cat mr.patch)\" '{\"category\": $category, \"prompt\": $prompt}') echo \"code reviews\" curl -X 'POST' 'http://1.1.1.1:5151/api/v1/chat/conversation' -H 'accept: text/plain' -H 'Content-Type: application/json' -d \"${CHATBODY}\" | tee code_review.txt # echo \"comments\" NOTES=$(jq --null-input --arg body \"$(cat code_review.txt)\" '{\"body\": $body}') curl --location --request POST --header \"PRIVATE-TOKEN: $ADMIN_ACCESS_TOKEN\" --header \"Content-Type: application/json\" \"https://git.example.com/api/v4/projects/$CI_MERGE_REQUEST_PROJECT_ID/merge_requests/$CI_MERGE_REQUEST_IID/notes\" -d \"$NOTES\" -k 实际发送的 prompt 见如下: Bellow is the code patch, please help me do a brief code review, if any bug risk and improvement suggestion are welcome diff --git a/content/posts/ai/gitlab-chatgpt-mr-code-review.md b/content/posts/ai/gitlab-chatgpt-mr-code-review.md index 0f3a990..dab4d0b 100644 --- a/content/posts/ai/gitlab-chatgpt-mr-code-review.md +++ b/content/posts/ai/gitlab-chatgpt-mr-code-review.md @@ -18,6 +18,15 @@ date: 2023-02-28T08:50:33+08:00 首先，我们需要一些魔法手段获得一个账号以及生成一个 chatgpt 的 apitoken. +## 当前使用的 prompt + +```txt +Bellow is the code patch, please help me do a brief code review, if any bug risk and improvement suggestion are welcome +%s +``` + +除此之外，还有很多好玩有用的 prompt 见 [这里](https://github.com/f/awesome-chatgpt-prompts) + ## 开始制作 code review 脚本 CI 中使用 chatgpt 做 code review 的主要实现思路是: 我的 chatgpt code review 计划在提交 mr 以及 mr 的每次 commit 的时候执行。 .gitlab.yaml 文件部分内容如下: chatgpt: stage: check image: alpine:edge script: - apk add git curl - ci/chatgpt.sh allow_failure: true rules: - if: $CI_PIPELINE_SOURCE == \"merge_request_event\" when: manual - when: never ","date":"2023-02-28","objectID":"/gitlab-chatgpt-mr-code-review/:3:0","tags":["gitlab","devops","chatgpt","code_review"],"title":"在 gitlab 中使用 chatgpt 做 code review","uri":"/gitlab-chatgpt-mr-code-review/"},{"categories":null,"content":"文章简介：ubuntu 22.04 vagrant up Authentication failure. Retrying, vagrant ssh 可以登录进 vm 问题处理 ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:0:0","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"问题现象 vagrant up 会报错 default: Warning: Authentication failure. Retrying.... $ vagrant init ubuntu/jammy64 # ubuntu 22.04 $ vagrant up Bringing machine 'default' up with 'virtualbox' provider... ==\u003e default: Importing base box 'rockylinux/9'... ==\u003e default: Matching MAC address for NAT networking... ==\u003e default: Checking if box 'rockylinux/9' version '1.0.0' is up to date... ==\u003e default: Setting the name of the VM: network_default_1670762466033_92955 ==\u003e default: Clearing any previously set network interfaces... ==\u003e default: Preparing network interfaces based on configuration... default: Adapter 1: nat ==\u003e default: Forwarding ports... default: 22 (guest) =\u003e 2222 (host) (adapter 1) ==\u003e default: Booting VM... ==\u003e default: Waiting for machine to boot. This may take a few minutes... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant default: SSH auth method: private key default: Warning: Authentication failure. Retrying... $ vagrant --debug up ... D, [2022-08-18T08:41:57.807561 #278356] DEBUG -- net.ssh.authentication.session[20120]: allowed methods: publickey D, [2022-08-18T08:41:57.807635 #278356] DEBUG -- net.ssh.authentication.methods.none[20134]: none failed D, [2022-08-18T08:41:57.807714 #278356] DEBUG -- net.ssh.authentication.session[20120]: trying publickey D, [2022-08-18T08:41:57.809423 #278356] DEBUG -- net.ssh.authentication.agent[20148]: connecting to ssh-agent D, [2022-08-18T08:41:57.809776 #278356] DEBUG -- net.ssh.authentication.agent[20148]: sending agent request 1 len 48 D, [2022-08-18T08:41:57.810239 #278356] DEBUG -- net.ssh.authentication.agent[20148]: received agent packet 5 len 1 D, [2022-08-18T08:41:57.810303 #278356] DEBUG -- net.ssh.authentication.agent[20148]: sending agent request 11 len 0 D, [2022-08-18T08:41:57.810656 #278356] DEBUG -- net.ssh.authentication.agent[20148]: received agent packet 12 len 841 D, [2022-08-18T08:41:57.811330 #278356] DEBUG -- net.ssh.authentication.methods.publickey[20170]: trying publickey (dd:3b:b8:2e:85:04:06:e9:ab:ff:a8:0a:c0:04:6e:d6) D, [2022-08-18T08:41:57.811474 #278356] DEBUG -- socket[2010c]: using encrypt-then-mac D, [2022-08-18T08:41:57.811624 #278356] DEBUG -- socket[2010c]: queueing packet nr 5 type 50 len 352 D, [2022-08-18T08:41:57.811724 #278356] DEBUG -- socket[2010c]: sent 420 bytes D, [2022-08-18T08:41:57.819401 #278356] DEBUG -- socket[2010c]: read 100 bytes D, [2022-08-18T08:41:57.819740 #278356] DEBUG -- socket[2010c]: received packet nr 5 type 51 len 32 D, [2022-08-18T08:41:57.819872 #278356] DEBUG -- net.ssh.authentication.session[20120]: allowed methods: publickey E, [2022-08-18T08:41:57.819952 #278356] ERROR -- net.ssh.authentication.session[20120]: all authorization methods failed (tried none, publickey) DEBUG ssh: == Net-SSH connection debug-level log END == INFO ssh: SSH not ready: #\u003cVagrant::Errors::SSHAuthenticationFailed: SSH authentication failed! This is typically caused by the public/private keypair for the SSH user not being properly set on the guest VM. Please verify that the guest VM is setup with the proper public key, and that the private key path for Vagrant is setup properly as well.\u003e ... vagrant ssh 可以登录 $ journalctl -u sshd -f -n 1000 Dec 11 12:42:49 10.0.2.15 sshd[1466]: userauth_pubkey: key type ssh-rsa not in PubkeyAcceptedAlgorithms [preauth] Dec 11 12:42:49 10.0.2.15 sshd[1466]: Connection closed by authenticating user vagrant 10.0.2.2 port 58914 [preauth] Dec 11 12:42:49 10.0.2.15 sshd[1458]: pam_unix(sshd:session): session opened for user vagrant(uid=1000) by (uid=0) 可以看到错误原因是 userauth_pubkey: key type ssh-rsa not in PubkeyAcceptedAlgorithms. ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:1:0","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"处理方法 ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:2:0","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"ubuntu 22.04 需要预先准备两个 terminal: term1 和 term2. # term1: vagrant up # 执行完后切换到 term2 # term2: vagrant ssh echo 'PubkeyAcceptedKeyTypes=+ssh-rsa' \u003e\u003e /etc/ssh/sshd_config systemctl restart sshd # term1 # 可以看到日志, 可以看到问题已经解决了 ==\u003e default: Booting VM... ==\u003e default: Waiting for machine to boot. This may take a few minutes... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant default: SSH auth method: private key default: Warning: Authentication failure. Retrying... default: Warning: Authentication failure. Retrying... default: Warning: Authentication failure. Retrying... default: default: Vagrant insecure key detected. Vagrant will automatically replace default: this with a newly generated keypair for better security. default: default: Inserting generated public key within guest... default: Removing insecure key from the guest if it's present... default: Key inserted! Disconnecting and reconnecting using new SSH key... ==\u003e default: Machine booted and ready! ==\u003e default: Checking for guest additions in VM... ==\u003e default: Mounting shared folders... default: /vagrant =\u003e /Volumes/code/ ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:2:1","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"rocylinux 9 update-crypto-policies --set LEGACY ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:2:2","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"原因 TODO https://github.com/hashicorp/vagrant/issues/12840 https://bugzilla.redhat.com/show_bug.cgi?id=2061607 https://bbs.archlinux.org/viewtopic.php?id=270005 ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:3:0","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"总结 TODO ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:4:0","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"ref network_configuration ","date":"2022-12-11","objectID":"/vagrant-authentication-failure-retrying/:5:0","tags":["Linux","vagrant","bugs"],"title":"vagrant up Authentication failure. Retrying","uri":"/vagrant-authentication-failure-retrying/"},{"categories":null,"content":"文章简介：chrony 静态编译及跨操作系统安装 systemd service ","date":"2022-11-26","objectID":"/static-build-chrony/:0:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"更新 基于 ubuntu 的镜像编译出的 chronyc 在 centos7.9 下执行 chronyc sources -v 会 Segmentation fault, 初步断定是由于 glibc 静态编译时有部分动态依赖的 wake link 被使用到导致的。替换成基于 alpine 的 musl 即可解决问题. FROM alpine:edge as build ARG branch=4.2 ENV DEBIAN_FRONTEND noninteractive RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories WORKDIR /opt # COPY chrony . # 二选一 RUN apk add build-base libcap-dev texinfo nettle-dev gnutls-dev bash wget tar gcc make \\ \u0026\u0026 wget https://download.tuxfamily.org/chrony/chrony-${branch}.tar.gz \\ \u0026\u0026 tar zxf chrony-${branch}.tar.gz \\ \u0026\u0026 mv chrony-${branch} chrony \\ \u0026\u0026 rm -f chrony-${branch}.tar.gz RUN cd chrony; \\ CFLAGS='-static -g' LDFLAGS='-static -lm' \\ ./configure \\ --enable-scfilter \\ --enable-ntp-signd \\ \u0026\u0026 make; echo $?; \\ mkdir -p /install_root; \\ make DESTDIR=/install_root install \\ \u0026\u0026 find /install_root RUN rm -rf /install_root/usr/local/share \\ \u0026\u0026 rmdir /install_root/var/lib/chrony/ /install_root/etc FROM scratch AS bin COPY --from=build /install_root / ","date":"2022-11-26","objectID":"/static-build-chrony/:1:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"正文 最近一段时间做的需求主要是适配 kylinsec 国产化操作系统. 因为产品是 tob 私有化部署 k8s 集群，并且多机器需要同步时间。经过调研后发现 chrony 已经作为 redhat 8 等多种发行版的默认选项，决定统一使用 chrony 作为我们的时间同步 daemon. 应用的自动化部署方案使用了 ansible，不同 Linux 发行版软件包不相同，所以适配一种 Linux 发行版，就需要为这种 Linux 发行版下载对应软件包。比如 chrony 在 centos 7/8/9, ubuntu 20.04/18.04, kylinsec … 等都需要单独下载一遍 chrony 安装包，整体总共适配了 6 遍，十分麻烦。当需要适配的软件包更多后，成本就凸显出来了. 可以选择静态编译，抹平各发行版的差异，一次搞定。 还好，静态编译的工作已经有大佬做过了静态编译 chrony, dockerfile 如下 # docker buildx build -f Dockerfile.chrony --platform linux/amd64 --target bin --output installer FROM ubuntu:20.04 as build ARG branch=4.2 ENV DEBIAN_FRONTEND noninteractive RUN sed -ri 's/(ports|deb|security|archive).(debian.org|ubuntu.com)/mirrors.tuna.tsinghua.edu.cn/g' /etc/apt/sources.list \\ \u0026\u0026 apt-get update WORKDIR /opt # COPY chrony . # 二选一 #RUN apt-get install -y git \u0026\u0026 git clone --branch ${branch} https://git.tuxfamily.org/chrony/chrony.git RUN apt-get install -y wget \u0026\u0026 wget https://download.tuxfamily.org/chrony/chrony-${branch}.tar.gz \\ \u0026\u0026 tar zxf chrony-${branch}.tar.gz \\ \u0026\u0026 mv chrony-${branch} chrony \\ \u0026\u0026 rm -f chrony-${branch}.tar.gz RUN apt-get install -y \\ bison asciidoctor \\ gcc \\ make \\ pkg-config \\ libcap-dev \\ pps-tools \\ libedit-dev \\ nettle-dev \\ libnss3-dev \\ libtomcrypt-dev \\ libgnutls28-dev \\ libseccomp-dev RUN cd chrony; \\ CFLAGS='-static -s' LDFLAGS='-static -lm' \\ ./configure \\ --enable-scfilter \\ --enable-ntp-signd \\ \u0026\u0026 make; echo $?; \\ mkdir -p /install_root; \\ make DESTDIR=/install_root install \\ \u0026\u0026 find /install_root RUN rm -rf /install_root/usr/local/share \\ \u0026\u0026 rmdir /install_root/var/lib/chrony/ /install_root/etc FROM scratch AS bin COPY --from=build /install_root / buildx 一步编译出软件包 docker buildx build -f Dockerfile.chrony --platform linux/amd64 --target bin --output installer file installer/{usr/local/bin/chronyc,usr/local/sbin/chronyd} installer/usr/local/bin/chronyc: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, BuildID[sha1]=a8851c783b1074f3414d8b38bb337cb10a8b016d, for GNU/Linux 3.2.0, stripped installer/usr/local/sbin/chronyd: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, BuildID[sha1]=a7ed52ca561252969d5e0f29b8894380b70bd8c5, for GNU/Linux 3.2.0, stripped # 多平台编译 docker buildx build . --platform linux/amd64,linux/arm64 --target bin --output installer ","date":"2022-11-26","objectID":"/static-build-chrony/:2:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"有了二进制文件该如何部署服务呢？ 如下是写好的 systemd service 文件 cat \u003c\u003cEOF \u003e/etc/chrony.conf # Generated by TODO # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server pool.ntp.org iburst driftfile /var/lib/chrony/drift makestep 1.0 3 # Enable kernel synchronization of the real-time clock (RTC). rtcsync allow 0.0.0.0/0 local stratum 10 logdir /var/log/chrony log measurements statistics tracking EOF cat \u003c\u003cEOF \u003e/etc/chrony.keys # This file is solely used for NTP authentication with symmetric keys # as defined by RFC 1305 and RFC 5905. # # It can contain ID/key pairs which can be generated using the “keygen” option # from “chronyc”; for example: # chronyc keygen 1 SHA256 256 \u003e\u003e /etc/chrony/chrony.keys # would generate a 256-bit SHA-256 key using ID 1. # # A list of supported hash functions and output encoding is available by # consulting the \"keyfile\" directive in the chrony.conf(5) man page. EOF cat \u003c\u003cEOF \u003e/etc/systemd/system/chrony.service [Unit] Description=chrony, an NTP client/server Documentation=man:chronyd(8) man:chronyc(1) man:chrony.conf(5) Conflicts=openntpd.service ntp.service ntpsec.service systemd-timesyncd.service Wants=time-sync.target Before=time-sync.target After=network.target # ConditionCapability=CAP_SYS_TIME [Service] # Type=forking # PIDFile=/run/chronyd.pid # EnvironmentFile=-/etc/default/chrony # Starter takes care of special cases mostly for containers ExecStart=/usr/sbin/chronyd -F -0 -d -f /etc/chrony.conf PrivateTmp=yes ProtectHome=yes ProtectSystem=full [Install] Alias=chronyd.service WantedBy=multi-user.target EOF systemctl enable --now chrony ","date":"2022-11-26","objectID":"/static-build-chrony/:3:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"到这里就可以了吗？ 当我们执行 timedatectl set-ntp off 后, chronyd 服务并没有按照预期停止时间同步。根据文档 timedated , 命令 timedatectl set-ntp off 会启动或停止 systemd-timedated.service; 文档 systemd-timedated.service 描述, systemd-timedated.service 会从 /usr/lib/systemd/ntp-units.d/50-chronyd.list 文件中启停系统时间同步服务。当前使用的时间服务是 chrony，需要覆盖文件/usr/lib/systemd/ntp-units.d/50-chronyd.list中的值，使得 timedatectl set-ntp off 能够按照预期启停时间同步. echo 'chrony.service' \u003e/usr/lib/systemd/ntp-units.d/50-chronyd.list 到这里，chrony 正式工作了. ","date":"2022-11-26","objectID":"/static-build-chrony/:4:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"安装 chrony service 的完整脚本 #!/usr/bin/env bash set -o errtrace set -o errexit set -o nounset set -o pipefail set -o xtrace cd $(dirname \"$0\") if [[ $EUID -ne 0 ]]; then echo \"This script must be run as root\" exit 1 fi DIR=$(dirname $(realpath $0)) install_chrony() { systemctl stop chrony || true systemctl disable chrony || true # 向前兼容 ubuntu 20.04 中可能会安装有 chrony systemctl stop chronyd || true systemctl disable chronyd || true # ubuntu 20.04 中会有此服务 systemctl stop systemd-timesyncd || true systemctl disable systemd-timesyncd || true chown root:root /var/run/chrony || true # for chrony logdir on ubuntu rm -rf /var/log/chrony mkdir -p /var/log/chrony mkdir -p /var/lib/chrony/ mkdir -p /etc/chrony/ # ubuntu 默认的配置文件位置是 /etc/chrony/chrony.conf # centos7 默认的配置文件位置是 /etc/chrony.conf test -f \"/etc/chrony/chrony.conf\" \u0026\u0026 mv /etc/chrony/chrony.conf /etc/chrony/chrony.conf.$(date \"+%Y-%m-%d_%H%M%S\").bak test -f /etc/chrony.conf \u0026\u0026 mv /etc/chrony.conf /etc/chrony.conf.$(date \"+%Y-%m-%d_%H%M%S\").bak ln -s -f /etc/chrony.conf /etc/chrony/chrony.conf cat \u003c\u003cEOF \u003e/etc/chrony.conf # Generated by TODO # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server pool.ntp.org iburst driftfile /var/lib/chrony/drift makestep 1.0 3 # Enable kernel synchronization of the real-time clock (RTC). rtcsync allow 0.0.0.0/0 local stratum 10 logdir /var/log/chrony log measurements statistics tracking EOF cat \u003c\u003cEOF \u003e/etc/chrony.keys # This file is solely used for NTP authentication with symmetric keys # as defined by RFC 1305 and RFC 5905. # # It can contain ID/key pairs which can be generated using the “keygen” option # from “chronyc”; for example: # chronyc keygen 1 SHA256 256 \u003e\u003e /etc/chrony/chrony.keys # would generate a 256-bit SHA-256 key using ID 1. # # A list of supported hash functions and output encoding is available by # consulting the \"keyfile\" directive in the chrony.conf(5) man page. EOF cat \u003c\u003cEOF \u003e/etc/systemd/system/chrony.service [Unit] Description=chrony, an NTP client/server Documentation=man:chronyd(8) man:chronyc(1) man:chrony.conf(5) Conflicts=openntpd.service ntp.service ntpsec.service systemd-timesyncd.service Wants=time-sync.target Before=time-sync.target After=network.target # ConditionCapability=CAP_SYS_TIME [Service] # Type=forking # PIDFile=/run/chronyd.pid # EnvironmentFile=-/etc/default/chrony # Starter takes care of special cases mostly for containers ExecStart=/usr/sbin/chronyd -F -1 -d -f /etc/chrony.conf PrivateTmp=yes ProtectHome=yes ProtectSystem=full [Install] Alias=chronyd.service WantedBy=multi-user.target EOF cp usr/local/sbin/chronyd /usr/sbin/chronyd cp usr/local/bin/chronyc /usr/bin/chronyc # timedatectl set-ntp off 会触发 # http://www.freedesktop.org/wiki/Software/systemd/timedated # systemctl status systemd-timedated # https://www.freedesktop.org/software/systemd/man/systemd-timedated.service.html# # STEP: centos7.9 ls /usr/lib/systemd/ntp-units.d/ echo 'chrony.service' \u003e/usr/lib/systemd/ntp-units.d/50-chronyd.list || true # /usr/sbin/chronyd -f /etc/chrony/chrony.conf -d -R systemctl enable --now chrony || true systemctl restart chrony } install_chrony ","date":"2022-11-26","objectID":"/static-build-chrony/:5:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"参考 chrony 静态编译 chrony 文档 timedated 文档 systemd-timedated.service ","date":"2022-11-26","objectID":"/static-build-chrony/:6:0","tags":["Linux","ops","静态编译"],"title":"chrony 静态编译及跨 Linux 发行版安装 systemd service","uri":"/static-build-chrony/"},{"categories":null,"content":"gorm no valid transactions ","date":"2022-09-15","objectID":"/go-jinzhu-gorm-no-valid-transactions/:0:0","tags":["db","gorm","go","bugs"],"title":"golang jinzhu/gorm no valid transactions","uri":"/go-jinzhu-gorm-no-valid-transactions/"},{"categories":null,"content":"现象 poc: package main import ( \"fmt\" \"github.com/jinzhu/gorm\" _ \"github.com/jinzhu/gorm/dialects/postgres\" ) var db *gorm.DB // A is just a demo table. type A struct{ Val string } func init() { var err error db, err = gorm.Open(\"postgres\", \"postgres://postgres:postgres@localhost?sslmode=disable\") if err != nil { panic(err) } db.LogMode(true) } func main() { panicOnDBErr(db.DropTableIfExists(\u0026A{})) panicOnDBErr(db.AutoMigrate(\u0026A{})) panicOnDBErr(db.Begin()) panicOnDBErr(db.Create(\u0026A{\"test\"})) panicOnDBErr(db.Rollback()) // panic here var list []A panicOnDBErr(db.Find(\u0026list)) fmt.Println(\u0026list) } func panicOnDBErr(db *gorm.DB) { panicOnErr(db.Error) } func panicOnErr(err error) { if err != nil { panic(err) } } (main.go:26) [2017-12-02 14:30:56] [49.42ms] DROP TABLE \"as\" [0 rows affected or returned ] (main.go:27) [2017-12-02 14:30:56] [40.77ms] CREATE TABLE \"as\" (\"val\" text ) [0 rows affected or returned ] (main.go:31) [2017-12-02 14:30:56] [0.51ms] INSERT INTO \"as\" (\"val\") VALUES ('test') RETURNING \"as\".* [1 rows affected or returned ] (main.go:33) [2017-12-02 14:30:56] no valid transaction panic: no valid transaction goroutine 1 [running]: main.panicOnErr(0x850c20, 0xc4201324e0) main.go:47 +0x4a main.panicOnDBErr(0xc420180000) main.go:42 +0x38 main.main() main.go:33 +0x1a8 exit status 2 ","date":"2022-09-15","objectID":"/go-jinzhu-gorm-no-valid-transactions/:1:0","tags":["db","gorm","go","bugs"],"title":"golang jinzhu/gorm no valid transactions","uri":"/go-jinzhu-gorm-no-valid-transactions/"},{"categories":null,"content":"问题出现原因及解决方法 问题原因：db.Begin() 会将 db 对象锁定在一个数据库连接中，当 rollback 后，当前数据库连接已经不可用了。 解决方法：db.Begin() 之前，clone 一个 db 示例，用 clone 出的 newDB 实例执行 newDB.Begin(), 保证 db 变量中的状态始终是状态健康的. ","date":"2022-09-15","objectID":"/go-jinzhu-gorm-no-valid-transactions/:2:0","tags":["db","gorm","go","bugs"],"title":"golang jinzhu/gorm no valid transactions","uri":"/go-jinzhu-gorm-no-valid-transactions/"},{"categories":null,"content":"references gorm issue/1699 ","date":"2022-09-15","objectID":"/go-jinzhu-gorm-no-valid-transactions/:3:0","tags":["db","gorm","go","bugs"],"title":"golang jinzhu/gorm no valid transactions","uri":"/go-jinzhu-gorm-no-valid-transactions/"},{"categories":null,"content":"Sarama go-metrics.newExpDecaySampleHeap 占用大量内存 ","date":"2022-09-15","objectID":"/go-sarama-metrics-spend-too-much-mem/:0:0","tags":["db"],"title":"Sarama go-metrics.newExpDecaySampleHeap 占用大量内存问题处理","uri":"/go-sarama-metrics-spend-too-much-mem/"},{"categories":null,"content":"问题现象 内存泄漏， ","date":"2022-09-15","objectID":"/go-sarama-metrics-spend-too-much-mem/:1:0","tags":["db"],"title":"Sarama go-metrics.newExpDecaySampleHeap 占用大量内存问题处理","uri":"/go-sarama-metrics-spend-too-much-mem/"},{"categories":null,"content":"references issues/1321 Memory leak when using metrics A way to stop meter rcrowley/go-metrics/meter.go#L37 代码 ","date":"2022-09-15","objectID":"/go-sarama-metrics-spend-too-much-mem/:2:0","tags":["db"],"title":"Sarama go-metrics.newExpDecaySampleHeap 占用大量内存问题处理","uri":"/go-sarama-metrics-spend-too-much-mem/"},{"categories":null,"content":"Postgres 内部工作机制 介绍 TODO: Postgres 介绍 Database 和 Tables 一个 Postgres server 中会有多个 database，每个 database 中会有多张 table，每个 table 中会有多条数据记录 (tuple), 每个数据记录会有多个字段。除此之外，每个 database 中还会有很多其他被管理的对象. 每个数据库对象都有唯一的 OID, 各数据库对象被保存在各自的 system catalogs 中. 例如 database 和 heap tuple 被保存在 pg_database 和 pg_class 中, 可以通过如下 sql 查询到 postgres=# select datname, oid from pg_database where datname = 'postgres'; postgres | 5 postgres=# select relname, oid from pg_class where relname='actor'; actor | 16475 ","date":"2022-09-14","objectID":"/postgres-internal/:0:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"物理结构 当前我们介绍文件目录存储结构。 Postgres 的一个数据库实例的数据存储在环境变量 PGDATA 中，通常 PGDATA 的值为 /var/lib/pgsql/data。同一台机器中可以部署多个 Postgres 服务实例，不同的服务实例使用不同的 PGDATA 以及不同的端口. PGDATA 子目录下包含数据库控制配置文件及数据文件。控制数据库服务实例运行的配置文件 postgresql.conf、pg_hba.conf 和 pg_ident.conf 通常情况下也存储在 PGDATA 中，也可以把它们放到其他的地方(具体可以看 postgres 命令的启动参数或者 pg_ctl 的启动参数) PGDATA 中的文件结构如下: # tree -L 2 $PGDATA . ├── PG_VERSION ├── base │ ├── 1 │ └── pgsql_tmp ├── global │ ├── 1213 │ ├── 1213_fsm │ ├── 1213_vm │ ├── pg_control │ ├── pg_filenode.map │ └── pg_internal.init ├── pg_commit_ts ├── pg_dynshmem ├── pg_hba.conf ├── pg_ident.conf ├── pg_logical │ ├── mappings │ ├── replorigin_checkpoint │ └── snapshots ├── pg_multixact │ ├── members │ └── offsets ├── pg_notify ├── pg_replslot ├── pg_serial ├── pg_snapshots ├── pg_stat ├── pg_stat_tmp ├── pg_subtrans │ └── 0000 ├── pg_tblspc ├── pg_twophase ├── pg_wal │ ├── 000000010000000000000004 │ ├── 000000010000000000000005 │ └── archive_status ├── pg_xact │ └── 0000 ├── postgresql.auto.conf ├── postgresql.conf ├── postmaster.opts └── postmaster.pid https://www.postgresql.org/docs/devel/storage-file-layout.html Item Description PG_VERSION PostgreSQL 主要版本号的文件 base 每个数据库的子目录 global 系统表, 比如 pg_database pg_commit_ts 事务提交时间戳数据, Version 9.5 or later. pg_dynshmem dynamic shared memory subsystem 使用的文件, Version 9.4 or later. pg_logical status data for logical decoding pg_multixact multitransaction status data (used for shared row locks) pg_notify LISTEN/NOTIFY status data pg_replslot replication slot data pg_serial information about committed serializable transactions pg_snapshots exported snapshots pg_stat permanent files for the statistics subsystem pg_stat_tmp temporary files for the statistics subsystem pg_subtrans subtransaction status data pg_tblspc symbolic links to tablespaces pg_twophase state files for prepared transactions pg_wal WAL (Write Ahead Log) files. It is renamed from pg_xlog in Version 10. pg_xact transaction commit status data, It is renamed from pg_clog in Version 10. postgresql.auto.conf A file used for storing configuration parameters that are set by ALTER SYSTEM postmaster.opts A file recording the command-line options the server was last started with postmaster.pid A lock file recording the current postmaster process ID (PID), cluster data directory path, postmaster start timestamp, port number, Unix-domain socket directory path (could be empty), first valid listen_address (IP address or *, or empty if not listening on TCP), and shared memory segment ID (this file is not present after server shutdown) ","date":"2022-09-14","objectID":"/postgres-internal/:1:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"数据库对象在文件存储中的布局 为了研究数据库数据存储，首先启动数据库实例，创建如下表结构: create database test; CREATE TABLE sal_emp ( id int primary key, name text, pay_by_quarter integer[], schedule text[][] ); INSERT INTO sal_emp VALUES ( 1, 'Bill', '{10000, 10000, 10000, 10000}', '{{\"meeting\", \"lunch\"}, {\"training\", \"presentation\"}}' ); create index sal_emp_btree ON sal_emp (pay_by_quarter); create index sal_emp_gin ON sal_emp USING gin(pay_by_quarter); ","date":"2022-09-14","objectID":"/postgres-internal/:2:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"database、tables、index 等文件布局 在 9.0 版本之后，可以通过如下命令查看当前登录数据库的数据目录 test=# show data_directory; data_directory --------------------------- /home/vagrant/pgdata/data (1 row) 数据库位于 base 子目录下，数据库目录名字是其 oid。例如 test 的 oid 为 16966，他的目录名字为 16966. test=# select oid,datname from pg_database; oid | datname -------+----------- 5 | postgres 16966 | test 1 | template1 4 | template0 $ ls -ld base/16966/ drwx------ 2 vagrant vagrant 12288 Sep 16 01:59 base/16966/ 每个不超过 1GB 的 table 或者 index 存储在其所属的 database 目录下的一个文件下. test=# SELECT relname, oid, relfilenode FROM pg_class WHERE relname = 'sal_emp'; relname | oid | relfilenode ---------+-------+------------- sal_emp | 16967 | 16967 test=# select * from pg_relation_filepath('sal_emp'); pg_relation_filepath ---------------------- base/16966/16967 $ ls -alh base/16966/16967 -rw------- 1 vagrant vagrant 8.0K Sep 16 02:03 base/16966/16967 当一个 table 或者 index 的文件大小超过 1GB，PostgreSQL 会创建名字类似于 relfilenode.1, 并使用它。如果新的文件被填满了，下一个新的文件 relfilenode.2 将被创建，以此类推。 ls -alh base/16966/16967* -rw------- 1 vagrant vagrant 1.0G Sep 16 02:03 base/16966/16967 -rw------- 1 vagrant vagrant 1M Sep 16 02:03 base/16966/16967.1 单个 table/index 索引文件大小是可以配置的, PostgreSQL 编译选项是 --with-segsize . 仔细观察数据库子目录，可以发现一些文件的后缀是 _fsm 和 _vm, 它们分别是对应文件的 free space map visibility map, 分别存储着每个文件中每个 page 的 空闲空间及是否可见。indexs 只有 fsm，没有 vm. 可以使用如下命令反向查看文件对应的数据库对象. test=# SELECT pg_filenode_relation(0, 16967); pg_filenode_relation ---------------------- sal_emp (1 row) ","date":"2022-09-14","objectID":"/postgres-internal/:2:1","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Tablespaces PostgreSQL 中的 Tablespaces 是 base 目录之外的附加数据区域。 该功能已在 8.0 版本中实现。 TODO: 补充更多细节 ","date":"2022-09-14","objectID":"/postgres-internal/:3:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Heap Table File 的内部文件格式 storage-page-layout 数据文件内部存储着大量的 pages, 每个 page 的固定大小为 8192 byte(8KB)。每个数据文件中的 page 从 0 开始编号，这些编号叫 block numbers. page 大小可以执行 sql 查看, SELECT current_setting('block_size');，其值可以在编译时添加参数修改 –with-blocksize=BLOCKSIZE eq: ./configure --with-blocksize=BLOCKSIZE --with-wal-blocksize=BLOCKSIZE 不同种类的文件的内部布局不相同。如下为 heap table file 的文件布局 heap table 中的 page 包含如下三部分： heap tuple(s): 一个 heap tuple 代表 tables 中的一行记录。heap tuple 以栈入顺序添加到 page 中，即新的 tuple 回被插入到旧的前边; line pointer(s): line pointer 持有 tuple 的指针。line pointer 是一个变长指针，每个 line pointer 的序号从 1 开始，当新的 tuple 被插入到 page 中时，指向新 tuple 的 line pointer 也会被从插入到 line pointer 数组的尾部; header data: 位于 page 的开始位置，记录了当前 page 的元数据。其详细构成定义在 PageHeaderData 中.其主要信息如下: pd_lsn: 保存了 WAL 的 LSN，于 WAL 机制相关； pd_checksum: 保存了当前 page 的 checksum； pd_lower, pd_upper: pd_lower 指向 linepointer 的末尾，pd_upper 指向最新的 tuple 的开始； pd_special, 这个变量是给 index 使用的，他指向 page 的末尾最后一个 byte lin pointer 的最后到最新 tuple 的开始是当前 page 的 空闲空间。 tuple identifier（TID）唯一标识一个 tuple. TID 包含两个指，block number 标识它所在的 page，offset number 标识它在 page 中的位置. 它的典型使用场景的 index. 另外，当 heap tuple 的大下超过 2k(1/4 page size 8k) 时候，存储会使用 TOAST(The Oversized-Attribute Storage Technique), 详细信息见 Postgres 文档 tuple 最多 MaxHeapAttributeNumber=1664 个 column. ","date":"2022-09-14","objectID":"/postgres-internal/:4:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"heap tuple 的读写 ","date":"2022-09-14","objectID":"/postgres-internal/:5:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"heap tuple 写 如下图所示, 假设当前 table 只有一页数据，当前页中有一条数据. pg_lower 指向 linp 最后一项的末尾, pg_upper 指向最后一个 tuple 的开始. tuple 的增长方向是从每页的尾部到头部。第二个 tuple 会插入到第一个 tuple 的前边(地址更小的位置), 新的 linp 会插入到第一个 linp 后边，pg_lower 会指向新的 linp 尾部, 原本指向第一个 tuple 头部的 pg_upper 会修改为指向第二个 tuple 头部, 其他 page header 中的数据(eq: pd_lsn, pg_checksum, pg_flag)也会被修改. 修改后的 page 布局如下图所示 TODO: 执行 sql 配合 pg_inspect 分析插入读取过程 ","date":"2022-09-14","objectID":"/postgres-internal/:5:1","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"heap tuple 读 heap tuple 的读有两种方式： sequential scan 和 index scan: sequential scan: 每个 tables 会有多个文件，每个文件中会有很多固定大小的 page 顺序存储，每个 page 中会有很多 tuple。sequential scan 会扫描所有文件中所有 page 中的 所有 tuple. index scan: index 是一棵搜索树，index 中包含很多 index tuple，这些 index tuple 中包含 index key 和 指向 heap tuple 的 TID。假设 index tuple 中包含如下内容 ‘(block=3, offset=7)’. 当 index scan 找到某个 index tuple 符合条件时，获得到 index tuple 中的 TID. 由于 page 是固定大小的，可以直接使用类似于 lseek 调用 定位到对应的 page 后将数据 load 到内存，再由 offset 定位到 内存中 linp 的位置，linp 中包含 tuple 的指针及状态，数据即可定位到并读出来。 TODO: 这里最好可以画个图 进程模型与内存模型 ","date":"2022-09-14","objectID":"/postgres-internal/:5:2","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"进程模型 Postgres server 是多进程模型，多个进程协同工作完成所有的数据库管理工作，它包含如下进程： postgres server 进程是所有 pg 进程的父进程，监听 tcp 端口并对外提供服务 backend 进程处理从客户端接收到的请求 各种 background processes （包括 VACUUM、CHECKPOINT 等） replication associated processes 处理流复制 background worker process 用户定义的后台进程，详细信息见 office docs 查看 Postgres 进程进程树的方法如下，其中假设 pg 的进程 id 为 503065 export PID=503065 ps f -p $(pstree -p ${PID} | sed 's/(/\\n(/g' | grep '(' | sed 's/(\\(.*\\)).*/\\1/' | tr \"\\n\" \" \") PID TTY STAT TIME COMMAND 503065 pts/1 S+ 0:00 ./src/backend/postgres 503068 ? Ss 0:00 \\_ postgres: checkpointer 503069 ? Ss 0:00 \\_ postgres: background writer 503071 ? Ss 0:00 \\_ postgres: walwriter 503072 ? Ss 0:00 \\_ postgres: autovacuum launcher 503073 ? Ss 0:00 \\_ postgres: logical replication launcher 503265 ? Ss 0:00 \\_ postgres: vagrant postgres [local] idle postgres server 进程是所有进程的父进程，负责启动并管理各后台进程；另外会监听 tcp 端口。当有新的连接请求过来时，主进程会初始化并 fork 子进程，由子进程处理请求。 backend 进程由 postgres server 启动，处理客户端请求，直到客户端断开 tcp 连接后， backend 进程也会一起退出。Postgres 支持有多个客户端同时连接并操作数据库。客户端数量的上限由配置项 max_connections 决定，默认值是 100。当客户端连接数超过 max_connections 后，服务会拒绝新的连接，直到当前连接数少于 max_connections 为止。另外，当连接数量超过 max_connections 后，服务器无法接受新的连接，此时如果想连接上 postgres 做一些维护工作将会被拒绝。配置了选项superuser_reserved_connections ，即使连接数超过了 max_connections ，超级管理员也可以登录到服务器。 如果很多客户端（如 WEB 应用）频繁重复与 PostgreSQL 服务器的连接和断开连接，会增加建立连接和创建后端进程的成本，因为 PostgreSQL 没有实现原生的连接池特性.这会在一些情况下严重的影响数据库的性能。一些数据库中间件可以解决此问题(pgbouncer 、pgpool-II) ","date":"2022-09-14","objectID":"/postgres-internal/:6:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"内存模型 宽泛的给 Postgres 中的内存可以被分成两类 本地内存区 - 每个 backend 分配来自己使用 共享内存区 - Postgres server 进程分配，由所有进程共享使用的内存区 ","date":"2022-09-14","objectID":"/postgres-internal/:7:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"本地内存区 每个本地内存区会分配一个本地内存区用于处理查询，每个本地内存区有多个子内存区，这些子内存区一些是固定大小的，一些是可变大小的。 子区域 描述 work_mem 此区域用于 ORDER BY 和 DISTINCT 排序 tuple, 以及 join 操作中的 merge-join 和 hash-join. maintenance_work_mem 一些维护工作 (VACUUM, REINDEX) 使用的内存. temp_buffers 用于存储临时表. ","date":"2022-09-14","objectID":"/postgres-internal/:7:1","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"共享内存区 Postgres 启动的时候会分配固定大小的多个内存区 子区域 description shared buffer pool PostgreSQL 从持久存储中加载表和索引中的 page 到此区域, 之后直接原地操作. WAL buffer 为了保证数据库失败不对数据，Postgres 支持 WAL 机制. WAL 是 Postgres 的事务日志; WAL buffer 是 WAL 数据写回持久存储前的缓存区域。 commit log Commit Log(CLOG) 保存事务提交状态，用于 MVCC 机制. 并发控制 tutorial-transactions ","date":"2022-09-14","objectID":"/postgres-internal/:7:2","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"事务 ID TODO: ","date":"2022-09-14","objectID":"/postgres-internal/:8:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Tuple 的结构 HeapTuple 分为 普通 tuple 和 TOAST tuple。 我们只阐述 普通 tuple。 一个 HeapTuple 包含三部分 HeapTupleHeaderData structure NULL bitmap 用户数据 HeapTupleHeaderData 包含很多字段，其中跟并发控制相关的字段如下： t_xmin 插入此 tuple 的事务的 txid t_xmax 更新或删除此 tuple 的事务的 txid。如果 tuple 没有被删除或更新过，此指为 0，标识 INVALID. t_cid 在当前事务下，执行此命令前执行过多少 sql；从 0 开始。例如我们在一个事务下执行多个 INSERT sql, “BEGIN;INSERT;INSERT;INSERT;COMMIT;”, 第一个 INSERT 命令执行，t_cid=0； 第二个值为 1，一次类推。 t_ctid 当前 tuple 的 tid 或者新的 tuple 的 tid. tid 标识一个 tuple。如果此 tuple 有被更新，则指向新的 tuple，否则指向自己. DROP TABLE tbl; CREATE TABLE tbl (data text); BEGIN;INSERT INTO tbl VALUES('A');INSERT INTO tbl VALUES('B');INSERT INTO tbl VALUES('C');COMMIT; SELECT lp as tuple, t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_infomask2, t_infomask, t_hoff, t_bits, t_data FROM heap_page_items(get_raw_page('tbl', 0)); tuple | t_xmin | t_xmax | t_cid | t_ctid | t_infomask2 | t_infomask | t_hoff | t_bits | t_data -------+--------+--------+-------+--------+-------------+------------+--------+--------+-------- 1 | 1130 | 0 | 0 | (0,1) | 1 | 2050 | 24 | | \\x0541 2 | 1130 | 0 | 1 | (0,2) | 1 | 2050 | 24 | | \\x0542 3 | 1130 | 0 | 2 | (0,3) | 1 | 2050 | 24 | | \\x0543 点我展开 HeapTupleHeaderData 声明 typedef struct HeapTupleFields { TransactionId t_xmin; /* inserting xact ID */ TransactionId t_xmax; /* deleting or locking xact ID */ union { CommandId t_cid; /* inserting or deleting command ID, or both */ TransactionId t_xvac; /* old-style VACUUM FULL xact ID */ } t_field3; } HeapTupleFields; typedef struct DatumTupleFields { int32 datum_len_; /* varlena header (do not touch directly!) */ int32 datum_typmod; /* -1, or identifier of a record type */ Oid datum_typeid; /* composite type OID, or RECORDOID */ /* * Note: field ordering is chosen with thought that Oid might someday * widen to 64 bits. */ } DatumTupleFields; typedef struct HeapTupleHeaderData { union { HeapTupleFields t_heap; DatumTupleFields t_datum; } t_choice; ItemPointerData t_ctid; /* current TID of this or newer tuple */ /* Fields below here must match MinimalTupleData! */ uint16 t_infomask2; /* number of attributes + various flags */ uint16 t_infomask; /* various flag bits, see below */ uint8 t_hoff; /* sizeof header incl. bitmap, padding */ /* ^ - 23 bytes - ^ */ bits8 t_bits[1]; /* bitmap of NULLs -- VARIABLE LENGTH */ /* MORE DATA FOLLOWS AT END OF STRUCT */ } HeapTupleHeaderData; typedef HeapTupleHeaderData *HeapTupleHeader; ","date":"2022-09-14","objectID":"/postgres-internal/:9:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Tuple Insert Update Delete 当前节主要描述 tuple，下面没有表示 page header 、linp。 DROP TABLE tbl; CREATE TABLE tbl (data text); INSERT INTO tbl VALUES('A');UPDATE tbl SET data='B'; SELECT t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_bits, t_data FROM heap_page_items(get_raw_page('tbl', 0)); t_xmin | t_xmax | t_cid | t_ctid | t_bits | t_data --------+--------+-------+--------+--------+-------- 99 | 100 | 0 | (0,2) | | \\x0541 100 | 0 | 0 | (0,2) | | \\x0542 ","date":"2022-09-14","objectID":"/postgres-internal/:10:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Insert insert 操作会直接将 tuple 插入到 page 中. DROP TABLE tbl; CREATE TABLE tbl (data text); INSERT INTO tbl VALUES('A'); SELECT t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_data FROM heap_page_items(get_raw_page('tbl', 0)); t_xmin | t_xmax | t_cid | t_ctid | t_data --------+--------+-------+--------+-------- 99 | 0 | 0 | (0,1) | \\x0541 t_xmin 为 99，这个 tuple 是在事务 txid=99 插入的 t_xmax 为 0，当前 tuple 没有被更新或删除过 t_cid 为 0，表示 tid=99 第一个 sql t_ctid 为 ‘(0,1)’, 指向他自己，因为他是第一个 tuple ","date":"2022-09-14","objectID":"/postgres-internal/:10:1","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Delete 删除操作是逻辑删除. 执行 DELETE 操作实际的操作是将 t_xmax 的值修改为 txid=100 的 txid DROP TABLE tbl; CREATE TABLE tbl (data text); INSERT INTO tbl VALUES('A'); -- txid=99 Delete from tbl where data='A'; -- txid=100 SELECT lp as tuple, t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_data FROM heap_page_items(get_raw_page('tbl', 0)); tuple | t_xmin | t_xmax | t_cid | t_ctid | t_data -------+--------+--------+-------+--------+-------- 1 | 99 | 100 | 0 | (0,1) | \\x0541 当事务被提交之后，tuple1 已经不需要了。通常这种不需要的 tuple 被称为 dead tuples. dead tuples 最终会被清理掉。清理 dead tuples 工作由 VACUUM 执行。 ","date":"2022-09-14","objectID":"/postgres-internal/:10:2","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Update 更新操作会删除(t_xmax 修改为当前 txid)旧的 tuple，插入一个新的 tuple. 如下，我们在一个事务中执行一次插入，后续在同一个事务下执行两个 UPDATE。 TODO: 画图把数据变化过程可视化一下 DROP TABLE tbl; CREATE TABLE tbl (data text); INSERT INTO tbl VALUES('A'); -- txid=99 SELECT lp as tuple, t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_data FROM heap_page_items(get_raw_page('tbl', 0)); BEGIN; UPDATE tbl set data='B'; SELECT lp as tuple, t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_data FROM heap_page_items(get_raw_page('tbl', 0)); UPDATE tbl set data='C'; COMMIT; SELECT lp as tuple, t_xmin, t_xmax, t_field3 as t_cid, t_ctid, t_data FROM heap_page_items(get_raw_page('tbl', 0)); -- 执行 insert tuple | t_xmin | t_xmax | t_cid | t_ctid | t_data -------+--------+--------+-------+--------+-------- 1 | 1168 | 0 | 0 | (0,1) | \\x0541 -- 执行第一次 update tuple | t_xmin | t_xmax | t_cid | t_ctid | t_data -------+--------+--------+-------+--------+-------- 1 | 1168 | 1169 | 0 | (0,2) | \\x0541 2 | 1169 | 0 | 0 | (0,2) | \\x0542 -- 执行第二次 update tuple | t_xmin | t_xmax | t_cid | t_ctid | t_data -------+--------+--------+-------+--------+-------- 1 | 1168 | 1169 | 0 | (0,2) | \\x0541 2 | 1169 | 1169 | 0 | (0,3) | \\x0542 3 | 1169 | 0 | 1 | (0,3) | \\x0543 假设第二个事务的 txid=1169 执行第一个 Update 的时候, 通过设置 t_xmax=txid 删除 Tuple1; t_ctid 指向新的 tuple Tuple2. tuple 的 head 变化如下： Tuple1: t_xmax=txid t_ctid '(0,1)' 修改为 '(0,2)' Tuple2: t_xmin=txid t_xmax=0 t_cid=0 t_ctid='(0,2)' 执行第二个 Update 的时候，通过设置 t_xmax=txid 删除 Tuple2; t_ctid 指向 Tuple3 Tuple2: t_xmax=txid t_ctid '(0,2)' 修改为 '(0,3)' Tuple3: t_xmin=txid t_xmax=0 t_cid=1 t_ctid='(0,3)' 如果 txid 事务 commited，Tuple1、Tuple2 会变成 dead tuple；如果 txid 事务 aborded，Tuple2、Tuple3 会变成 dead tuple ","date":"2022-09-14","objectID":"/postgres-internal/:10:3","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Free Space Map Free Space Map 用于跟踪数据库关系的可用空间. 当插入 heap/index tuple 的时候，PostgreSQL 使用 FSM 寻找合适的 page 插入 tuple. FSM 文件命名为其关系的 filenode 数字加后缀 _fsm. 例如，关系 public.user 保存的文件的 filenode 为 1000，则它的 FSM 文件为同目录下的 1000_fsm 文件。FSM 文件会在需要的时候加载到 shared memory 中。 pg_freespacemap 扩展可以查看关系的空闲空间大小。如下查询关系 tbl 的空闲空间百分率。 CREATE EXTENSION pg_freespacemap; SELECT *, round(100 * avail/8192 ,2) as \"freespace ratio\" FROM pg_freespace('tbl'); blkno | avail | freespace ratio -------+-------+----------------- 0 | 0 | 0.00 ","date":"2022-09-14","objectID":"/postgres-internal/:10:4","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Commit Log (clog) Commit Log 保存着事务状态。Commit Log 通常也被叫做 clog，被分配在 shared memory, 贯穿在整个事务的处理过程中。 ","date":"2022-09-14","objectID":"/postgres-internal/:11:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Transaction Status Postgres 定义了4中事务状态 N_PROGRESS, COMMITTED, ABORTED, and SUB_COMMITTED. ","date":"2022-09-14","objectID":"/postgres-internal/:11:1","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Clog 如何工作的 clog 由 shared memory 中一个到多个 8kb 大小的 page 构成. clog 是逻辑上的数组。数组的 index 表示事务的 id，数组的各项 如图 T1: txid=200 commit, 状态由 IN_PROGRESS 变为 COMMITTED T2: txid=201 abort, 状态由 IN_PROGRESS 变为 ABORTED 后续会讲述 clog 是如何使用的。 ","date":"2022-09-14","objectID":"/postgres-internal/:11:2","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"Transaction Snapshot transaction snapshot 保存了所有事务的的执行状态信息。Postgres 内部定义了文本格式的 transaction Snapshot 如 ‘100💯’. ‘100💯’ 意思是txids\u003c=99 没有在活跃， txids\u003e=100 在活跃。 内置的函数 txid_current_snapshot 展示当前事务的快照 SELECT txid_current_snapshot(); txid_current_snapshot ----------------------- 100:104:100,102 文本格式的 txid_current_snapshot 是 ‘xmin:xmax:xip_list’, 每部分的含义如下： xmin 最早的活跃事务。早于它的事务要么 committed 事务可见，要么 rollback 提交内容不生效/不可见 xmax 第一个尚未分配的 txid。 所有 \u003e=txid 的事务尚未启动，是不可见的 xip_list 活跃的事务。这个列表只包含 xmin 和 xmax 之间的事务 如上图，第一个例子 ‘100💯’ xmin=100，表示 txids\u003c100 的没活跃 xmax=100，表示 txids\u003e=100 的活跃 第二个例子 ‘100:104:100,102’ xmin=100，表示 txids\u003c100 的没有活跃 xmax=104，表示 txids\u003e=104 的正在活跃 xip_list=100,102，表示 100和102 正常执行 transaction manager 管理 transaction snapshots。transaction snapshots 用于在事务执行过程中判断 tuple 是否可见。相同的 tuple 及相同的 transaction snapshots 下，不同的事务隔离级别会有不同的可见结果。 transaction manager 中持有当前正在执行的事务的执行状态。如图，三个事务相继执行，TxA 和 TxB 的事务隔离级别是 READ COMMITTED，TxC 的是 REPREATABLE READ. T1: TxA 开始执行第一个 SELECT 命令。当执行第一个命令时候，TxA请求获得 txid 和 快照。transaction manager 分配 txid=200, transation snapshot '200:200:' T2: TxB 开始执行第一个 SELECT 命令，transaction manager 分配 tx=201，transaction snapshot '200:200:', 因此 TxB 中无法看到 TxA T3: TxC 开始执行第一个 SELECT 命令，分配 tx=202，tx snapshot '200:200:'. TxC 看不到 TxA 和 TxB T4: TxA 提交。transaction manager 移除他的事务信息 T5: TxB 和 TxC 再次执行 SELECT命令。 TxB 请求获得 tx snapshoot，因为他的事务隔离级别是 `READ COMMITTED`，TxB 获得的 tx snapshot 是 '201:201:'。 TxA 已经 commited 了，故 TxB 可以看到 TxA TxC 因为其事务隔离级别是 `REPEATABLE READ`，应该继续使用之前获得的 snapshot '200:200:'. TxC 看不到 TxA 和 TxB ","date":"2022-09-14","objectID":"/postgres-internal/:12:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"可见性规则 基于现在的堆文件的实现，一个事务如何判断当前获取到的 tuple 是可见的呢？interdb 做的检查规则如下: /* t_xmin status = ABORTED */ Rule 1: IF t_xmin status is 'ABORTED' THEN RETURN 'Invisible' END IF /* t_xmin status = IN_PROGRESS */ IF t_xmin status is 'IN_PROGRESS' THEN IF t_xmin = current_txid THEN Rule 2: IF t_xmax = INVALID THEN RETURN 'Visible' Rule 3: ELSE /* this tuple has been deleted or updated by the current transaction itself. */ RETURN 'Invisible' END IF Rule 4: ELSE /* t_xmin ≠ current_txid */ RETURN 'Invisible' END IF END IF /* t_xmin status = COMMITTED */ IF t_xmin status is 'COMMITTED' THEN Rule 5: IF t_xmin is active in the obtained transaction snapshot THEN RETURN 'Invisible' Rule 6: ELSE IF t_xmax = INVALID OR status of t_xmax is 'ABORTED' THEN RETURN 'Visible' ELSE IF t_xmax status is 'IN_PROGRESS' THEN Rule 7: IF t_xmax = current_txid THEN RETURN 'Invisible' Rule 8: ELSE /* t_xmax ≠ current_txid */ RETURN 'Visible' END IF ELSE IF t_xmax status is 'COMMITTED' THEN Rule 9: IF t_xmax is active in the obtained transaction snapshot THEN RETURN 'Visible' Rule 10: ELSE RETURN 'Invisible' END IF END IF END IF 被 ABORTED 的 tuple 不可见（Rule 1）。 正在执行的事务，当前事务自己可见(Rule 2, Rule 3, Rule 4). Rule 7, tuple 被当前事务删除了 Rule 8，其他的事务的删除没有提交，当前事务是可见的 Rule 1: If Status(t_xmin) = ABORTED =\u003e Invisible Rule 2: If Status(t_xmin) = IN_PROGRESS ∧ t_xmin = current_txid ∧ t_xmax = INVAILD =\u003e Visible Rule 3: If Status(t_xmin) = IN_PROGRESS ∧ t_xmin = current_txid ∧ t_xmax != INVAILD =\u003e Invisible Rule 4: If Status(t_xmin) = IN_PROGRESS ∧ t_xmin != current_txid =\u003e Invisible Rule 5: If Status(t_xmin) = COMMITTED ∧ Snapshot(t_xmin) = active =\u003e Invisible Rule 6: If Status(t_xmin) = COMMITTED ∧ (t_xmax = INVALID ∨ Status(t_xmax) = ABORTED) =\u003e Visible Rule 7: If Status(t_xmin) = COMMITTED ∧ Status(t_xmax) = IN_PROGRESS ∧ t_xmax = current_txid =\u003e Invisible Rule 8: If Status(t_xmin) = COMMITTED ∧ Status(t_xmax) = IN_PROGRESS ∧ t_xmax != current_txid =\u003e Visible Rule 9: If Status(t_xmin) = COMMITTED ∧ Status(t_xmax) = COMMITTED ∧ Snapshot(t_xmax) = active =\u003e Visible Rule 10: If Status(t_xmin) = COMMITTED ∧ Status(t_xmax) = COMMITTED ∧ Snapshot(t_xmax) != active =\u003e Invisible ","date":"2022-09-14","objectID":"/postgres-internal/:13:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"TOAST TODO VACUUM Processing TODO Buffer Manager TODO Write Ahead Logging (WAL) TODO references postgres-src The Internals of PostgreSQL for database administrators and system developers ","date":"2022-09-14","objectID":"/postgres-internal/:14:0","tags":["db","postgres"],"title":"Postgres 内部工作原理","uri":"/postgres-internal/"},{"categories":null,"content":"文章简介: docker 内置 bridge 的 ip 修改方法 Configure the default bridge network docker daemon 的配置文件 /etc/docker/daemon.json 提供了自定义 docker builtin bridge 配置项. # 修改 daemon.json 中 \"bip\" { .... \"bip: \"100.100.1.1/24\", # bip: 修改 builtin bridge 的 ip \"default-address-pools\":[{\"base\":\"100.100.0.0/16\",\"size\":24}} ... } docker stop $(docker ps -qa) \u0026\u0026 systemctl restart docker \u0026\u0026 docker start $(docker ps -qa) 注意: docker 如果已经有容器在运行，bip 的配置是不会生效。配置添加 {\"debug\": true} 后会有如下日志: Jun 11 18:31:45 hostname dockerd[323728]: time=\"2022-06-11T18:31:45.251000544+08:00\" level=debug msg=\"RequestPool(LocalDefault, 172.17.0.0/16, , map[], false)\" Jun 11 18:31:45 hostname dockerd[323728]: time=\"2022-06-22T18:31:45.251092001+08:00\" level=debug msg=\"RequestAddress(LocalDefault/172.17.0.0/16, 172.17.0.1, map[RequestAddressType:com.docker.network.gateway])\" Jun 11 18:31:45 hostname dockerd[323728]: time=\"2022-06-11T18:31:45.251140234+08:00\" level=debug msg=\"Request address PoolID:172.17.0.0/16 App: ipam/default/data, ID: LocalDefault/172.17.0.0/16, DBIndex: 0x0, Bits: 65536, Unselected: 65534, Sequence: (0x80000000, 1)-\u003e(0x0, 2046)-\u003e(0x1, 1)-\u003eend Curr:0 Serial:false PrefAddress:172.17.0.1 \" Jun 11 18:31:45 hostname dockerd[323728]: time=\"2022-06-11T18:31:45.272456556+08:00\" level=info msg=\"There are old running containers, the network config will not take affect\" 解决方法是先将所有启动的容器 stop，然后重启 docker 配置生效后，start 容器 ","date":"2022-06-23","objectID":"/docker-change-default-bridge-ip/:0:0","tags":["docker","fix"],"title":"Docker 修改内置 bridge ip 不生效, 原因及解决办法","uri":"/docker-change-default-bridge-ip/"},{"categories":null,"content":"文章简介：Linux Network 是如何使用及工作的 Linux Network 非常复杂，其本身的机制非常复杂，每一种 Linux 发行版的网络也有差别，管理工具也纷繁复杂。这里先简单的描述在 Centos7 和 Ubuntu20.04 下如何配置网络，后简单的描述一下 network 是怎样工作的. Centos6 使用 network.service Centos7 network.service 和 NetworkManager.service 并存 Centos8 之后只使用 NetworkManager.service Ubuntu 使用 netplan 前端，NetworkManager/networkd 后端 ","date":"2022-01-01","objectID":"/linux_config_network/:0:0","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"Linux 网络设备 TODO: 网络设备架构 TODO: 网络接口命名 ","date":"2022-01-01","objectID":"/linux_config_network/:1:0","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"network.service 使用方法 修改 /etc/sysconfig/network-scripts 下的 ifcfg-*文件，之后执行 service network restart，配置即可生效。 注意: 网络配置修改可能会停掉当前的 ssh 连接 原理： network.service 会执行 /etc/rc.d/init.d/network start 启动网络服务，启动的过程中，使用 ip 命令启动各 device，使用 route 配置系统路由，使用 sysctl 配置等。 ","date":"2022-01-01","objectID":"/linux_config_network/:1:1","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"NetworkNanager 使用方法 Managing IP Networking 包含优点及如何使用 D-Bus 的 unix socket: /var/run/dbus/system_bus_socket udev 配置: /etc/udev/udev.conf 配置文件目录 /etc/NetworkManager /etc/sysconfig/network-scripts 命令 systemctl restart NetworkManager.service # 重启服务 journalctl -u NetworkManager.service -f -n 100 # 查看日志 # 查询连接信息 nmcli conn # 添加连接 nmcli conn add type ethernet con-name eth0-static ifname eth0 ipv4.method manual ipv4.addresses \"10.0.2.16/24\" ipv4.gateway 10.0.2.2 ipv4.dns 114.114.114.114 ipv6.method auto # 修改连接 nmcli conn modify eth0-static ipv4.method manual ipv4.addresses 10.0.2.254/24 ipv4.gateway 10.0.2.2 # interactive edit connection nmcli conn edit \u003cconn-name\u003e # 系统 link(设备) nmcli dev # 重新应用配置 nmcli dev reapply eth0 # TUI nmtui 我们可以使用 nmcli 配置网络，当出现错误的时候，通过 journalctl 查看日志，分析配置错误原因，继续使用 nmcli 配置网络，直到网络服务配置好. ","date":"2022-01-01","objectID":"/linux_config_network/:1:2","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"NetworkNanager 工作原理 NetworkNanager 有两个组件: NetworkManager daemon, 管理连接和监听并报告网络变化 一些管理前端（front-ends）, 比如 nmcli,一些 GUI 程序等 NetworkManager daemon 依赖于 D-Bus, 实现了管理连接的接口，底层使用 netlink 跟 linux 内核交互，获得设备信息，配置连接（比如修改 ipv4 的地址等）。nmcli 跟 D-Bus 交互连接 NetworkManager daemon，获得信息以及配置网络 # ip 命令也是使用 netlink 跟 kernel 交互的 strace -fff ip a socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3 sendto(3, {{len=40, type=RTM_GETLINK, flags=NLM_F_REQUEST|NLM_F_DUMP, seq=1641054708, pid=0}, {ifi_family=AF_UNSPEC, ifi_type=ARPHRD_NETROM, ifi_index=0, ifi_flags=0, ifi_change=0}, {{nla_len=8, nla_type=IFLA_EXT_MASK}, 1}}, 40, 0, NULL, 0) = 40 recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000}, msg_namelen=12, msg_iov=[{iov_base=NULL, iov_len=0}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_TRUNC}, MSG_PEEK|MSG_TRUNC) recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000} sendto(3, {{len=40, type=RTM_GETADDR, flags=NLM_F_REQUEST|NLM_F_DUMP, seq=1641054709, pid=0}, {ifa_family=AF_UNSPEC, ifa_prefixlen=0, ifa_flags=0, ifa_scope=RT_SCOPE_UNIVERSE, ifa_index=0}, {nla_len=0, nla_type=IFA_UNSPEC}}, 40, 0, NULL, 0) = 40 recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000}, msg_namelen=12, msg_iov=[{iov_base=NULL, iov_len=0}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_TRUNC}, MSG_PEEK|MSG_TRUNC) TODO: 简单的 demo 使用 netlink 获得 ip 地址和配置 ip 地址 netlink-demo ","date":"2022-01-01","objectID":"/linux_config_network/:1:3","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"NetworkNanager 和 network.service 如何并存 TODO: ","date":"2022-01-01","objectID":"/linux_config_network/:1:4","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"netplan The network configuration abstraction renderer netplan 是一个方便在 linux 系统下配置网络的工具。我们通过 yaml 文件描述网络配置，\u0008netplan 会为我们生成我们选择的网络后端所需要的配置。当前支持后端 Systemd-networkd(default) 和 NetworkManager. Ubuntu 20.04 使用 netplan 作为网络配置工具. 配置文件在 ls /{lib,etc,run}/netplan/*.yaml. ls /etc/netplan netplan design netplan reference ls /{lib,etc,run}/netplan/*.yaml # 配置文件位置 netplan generate # 生成配置 ls /run/systemd/network/ ip a # 各接口信息 lshw -class network # 硬件更详细的信息 ethtool enp0s3 # 修改配置 /etc/netplan/*.yaml netplan try # 测试是否生效 netplan apply # 应用配置 resolvectl status # resolve name netplan 一些通用的配置模版 netplan 配置 dhcp 自动获取 ip cat /etc/netplan/50-cloud-init.yaml # This file is generated from information provided by the datasource. Changes # to it will not persist across an instance reboot. To disable cloud-init's # network configuration capabilities, write a file # /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following: # network: {config: disabled} network: ethernets: enp0s3: dhcp4: true match: macaddress: 02:f6:7f:02:5c:72 set-name: enp0s3 version: 2 netplan 配置 static ip: network: version: 2 ethernets: enp0s3: match: macaddress: 02:f6:7f:02:5c:72 addresses: - 10.10.10.2/24 gateway4: 10.10.10.1 set-name: enp0s3 nameservers: search: [mydomain, otherdomain] addresses: [10.10.10.1, 1.1.1.1] routes: - to: default via: 10.10.10.1 # netplan generate --root-dir /vagrant Name resolve # /run/systemd/resolve/stub-resolv.conf man 8 systemd-resolved systemd-resolve --statistics # resolve 服务 systemd-resolve --reset-statistics # 重置统计信息 systemctl restart systemd-resolved # 重启 ubuntu resolved 服务 resolvectl flush-caches # flush dns cache (old: systemd-resolve --flush-caches) # flush dns cache # https://www.linuxuprising.com/2019/07/how-to-flush-dns-cache-on-linux-for.html ","date":"2022-01-01","objectID":"/linux_config_network/:1:5","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"总结 网络配置最终都是跟内核用 netlink 沟通，告诉 \u0008linux 内核网络配置，由 kernel 生效并工作。用户态管理程序典型代表： NetworkManager 和 systemd-networkd。熟悉了这些，便有了手段指挥 linux kernel 网络如何工作。剩下的就需要我们慢慢了解和学习 linux 网络提供了哪些功能。 ","date":"2022-01-01","objectID":"/linux_config_network/:2:0","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"ref network_configuration linux kernel network docs Netplan configuration examples ","date":"2022-01-01","objectID":"/linux_config_network/:3:0","tags":["Linux","network"],"title":"Linux Network 如何配置及工作原理","uri":"/linux_config_network/"},{"categories":null,"content":"文章简介：linux 设置开机自启动脚本的几种方式 ","date":"2022-01-01","objectID":"/linux_boot_script/:0:0","tags":["Linux","commands","shell"],"title":"Linux 开机自启动脚本机器实现原理","uri":"/linux_boot_script/"},{"categories":null,"content":"rc.local ","date":"2022-01-01","objectID":"/linux_boot_script/:1:0","tags":["Linux","commands","shell"],"title":"Linux 开机自启动脚本机器实现原理","uri":"/linux_boot_script/"},{"categories":null,"content":"使用方法 # 文件 /etc/rc.d/rc.local # centos 或者已经配置过此文件 echo \"echo 'do HelloWord'\" \u003e\u003e /etc/rc.local # 已经有这个文件的时候 # ubuntu 没有 /etc/rc.local，需要创建文件，正确的配置 shebang printf '%s\\n' '#!/bin/bash' 'exit 0' | sudo tee -a /etc/rc.local # 没有这个文件的时候 # Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure # that this script will be executed during boot. chmod +x /etc/rc.local systemctl enable --now rc-local systemctl status rc-local.service 工作原理 systemctl cat rc-local.service [Unit] Description=/etc/rc.local Compatibility Documentation=man:systemd-rc-local-generator(8) ConditionFileIsExecutable=/etc/rc.local After=network.target [Service] Type=forking ExecStart=/etc/rc.local start TimeoutSec=0 RemainAfterExit=yes GuessMainPID=no 可以看到，\u0008ConditionFileIsExecutable=/etc/rc.local，当 /etc/rc.local 可执行的时候，systemd 拉起服务的过程中，会执行 /etc/rc.local. ","date":"2022-01-01","objectID":"/linux_boot_script/:1:1","tags":["Linux","commands","shell"],"title":"Linux 开机自启动脚本机器实现原理","uri":"/linux_boot_script/"},{"categories":null,"content":"crontab reboot echo '@reboot /root/helloworld.sh' \u003e\u003e /etc/crontab crontab /etc/crontab crontab -e @reboot /root/helloworld.sh ","date":"2022-01-01","objectID":"/linux_boot_script/:2:0","tags":["Linux","commands","shell"],"title":"Linux 开机自启动脚本机器实现原理","uri":"/linux_boot_script/"},{"categories":null,"content":"文章简介：学习 systemd 笔记 Linux 的启动 很长一段时间采用 init 进程。init 是系统启动后的第一个用户级程序，pid=1. init 程序有两个缺点: 启动时间长。init 进程是串行启动，只有前一个进程启动完，才会启动下一个进程; 启动脚本复杂。init 进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长; 为了解决这个问题，Lennart Poettering 设计了 systemd。 systemd 是 Linux 电脑操作系统之下的一套中央化系统及设置管理程序（init），包括有守护进程（daemon）、程序库以及应用软件.已知的主流 Linux 发行版都把 systemd 作为 init. systemd 还附带很多基础工具，包括 timesync, hostnamectl 等一系列管理系统的工具。具体使用方法 这里 已经讲了很多，在这里不再赘述。 systemd 使用情况如下: 系统 版本 备注 Ubuntu 15.04 及后续版本 1 Debian GNU/Linux Debian 8“Jessie”之后 Red Hat 及其派生品 7 及后续版本 Fedora 15 及后续版本 ","date":"2022-01-01","objectID":"/systemd-usages/:0:0","tags":["Linux","Systemd"],"title":"Systemd 简单介绍，使用及如何 Debug","uri":"/systemd-usages/"},{"categories":null,"content":"如何 debug systemctl 提供了一些工具用于 debug。 systemctl cat multi-user.service # 查看 unit 配置 systemctl show multi-user.service # 显示某个 Unit 的所有底层参数 systemctl show -p CPUShares multi-user.service # 设置某个 Unit 的指定属性 systemctl list-dependencies --all multi-user.service # 列出依赖项 systemctl list-dependencies --all multi-user.service # 列出依赖项 systemd-analyze critical-chain # cli 显示瀑布状的启动过程流 systemd-analyze blame # 分析当前系统各项服务启动时间 systemd-analyze plot \u003e boot.svg # 分析当前系统各项服务启动顺序 ls /etc/systemd/system/ # 修改 unit 文件 # 重新加载 service 信息 # systemd 会将所有 unit 信息缓存在内存，只有重新加载，修改内容才会生效 systemctl daemon-reload journalctl -u ssh.service -f # 查看 unit 日志 ","date":"2022-01-01","objectID":"/systemd-usages/:1:0","tags":["Linux","Systemd"],"title":"Systemd 简单介绍，使用及如何 Debug","uri":"/systemd-usages/"},{"categories":null,"content":"ref Systemd 入门教程：命令篇 Linux 的启动流程 ","date":"2022-01-01","objectID":"/systemd-usages/:2:0","tags":["Linux","Systemd"],"title":"Systemd 简单介绍，使用及如何 Debug","uri":"/systemd-usages/"},{"categories":null,"content":"文章简介: 使用 nginx 制作一个 yum mirror nginx 可以支持缓存文件，故将 nginx 配置成 yum 包 mirror server，带文件自动过期, 并限制最大文件过期时间. #内容存储到 pkg_mirror.conf 文件中 upstream centos_mirror { server mirrors.tuna.tsinghua.edu.cn; } # centos 7 yum local source server { listen 7000; server_name $host; index index.html ; root /opt/nginx/cache/; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Headers X-Requested-With; add_header Access-Control-Allow-Methods GET,POST,OPTIONS; location / { proxy_store on; proxy_temp_path \"/opt/nginx/cache/\"; proxy_set_header Accept-Encoding identity; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host 'mirrors.tuna.tsinghua.edu.cn'; proxy_next_upstream error http_502; if ( !-e $request_filename ) { proxy_pass http://centos_mirror; } expires 6h; # proxy_pass http://centos_mirror; } } # 将文件存储到 docker-compose.yaml 中 version: \"3\" services: repo_mirror: container_name: repo_mirror image: nginx:alpine ports: - 7000:7000 volumes: - ./pkg_mirror.conf:/etc/nginx/conf.d/pkg_mirror.conf - ./mirror_cache:/opt/nginx/cache logging: driver: \"json-file\" options: max-size: \"512m\" restart: always docker-compose up -d 测试效果 # Dockerfile FROM ubuntu:20.04 RUN sed -E -i -e 's/(archive|ports).ubuntu.com/192.168.0.102:7000/g' -e '/security.ubuntu.com/d' /etc/apt/sources.list RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends ca-certificates \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* \u0026\u0026 \\ update-ca-certificates docker build -t tmp:dev . # 日志中 apt 使用了 192.168.0.102:7000 安装软件成功，且 mirror_cache 目录中有相关的文件 ","date":"2021-10-25","objectID":"/build-yum-mirror-with-nginx/:0:0","tags":["nginx","linux"],"title":"使用 nginx 制作一个 yum mirror","uri":"/build-yum-mirror-with-nginx/"},{"categories":null,"content":"文章简介：golang scheduler 相关的内容 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:0:0","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"简介 现在主流的线程模型分三种：内核级线程模型、用户级线程模型和两级线程模型（也称混合型线程模型），他们都很复杂。golang 属于 两级线程模型，是由 goroutine 实现的, goroutine 是 golang 最重要的特性之一，具有使用成本低、消耗资源低、能效高等特点，官方宣称原生 goroutine 并发成千上万不成问题. CSP(通信顺序进程) 是 golang 中的并发模型。本片文档将带领读者深入理解 golang 的 goroutine 的工作方式。 stack 大小： goroutine：2KB 线程：8MB 线程切换需要陷入内核；goroutine 只需要 getcontext 和 setcontext，30+ 个寄存器的读合写，非常快速 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:1:0","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"coroutine 的工作方式 说到 goroutine，就不得不说一下 coroutine。goroutine 是coroutine 在 golang 中的实现和优化。golang 作者之一 Russ Cox 有实现轻量的 libtask ，400 行左右代码描述清楚了 coroutine 是怎么工作的。 建议亲自下载代码，看一下具体的实现逻辑, 需要比较简单的 c 基础 一个 coroutine 比较核心的工作包括： task 管理, 包括创建，销毁，切换等 coroutine 调度算法 先看一个简单的使用例子: // https://github.com/mixinlib/libtask/blob/651d7b69a4f7b10c798dcd544e6a25fd7505632c/primes.c#L12 void primetask(void *arg) { Channel *c, *nc; int p, i; c = arg; p = chanrecvul(c); if(p \u003e goal) taskexitall(0); if(!quiet) printf(\"%d\\n\", p); nc = chancreate(sizeof(unsigned long), buffer); taskcreate(primetask, nc, 32768); for(;;){ i = chanrecvul(c); if(i%p) chansendul(nc, i); } } void taskmain(int argc, char **argv) { int i; Channel *c; if(argc\u003e1) goal = atoi(argv[1]); else goal = 100; printf(\"goal=%d\\n\", goal); c = chancreate(sizeof(unsigned long), buffer); taskcreate(primetask, c, 32768); // 创建 coroutine，开始执行任务 primetask for(i=2;; i++) chansendul(c, i); } 会发现程序中没有常见的 c main 函数，实际是在 libtask 中实现了。整个程序是在一个线程中执行的，main 中创建了第一个 task，并启动 task 调度器 // https://github.com/mixinlib/libtask/blob/main/task.c#L353 static void taskmainstart(void *v) { taskname(\"taskmain\"); taskmain(taskargc, taskargv); } int main(int argc, char **argv) { struct sigaction sa, osa; memset(\u0026sa, 0, sizeof sa); sa.sa_handler = taskinfo; sa.sa_flags = SA_RESTART; sigaction(SIGQUIT, \u0026sa, \u0026osa); #ifdef SIGINFO sigaction(SIGINFO, \u0026sa, \u0026osa); #endif argv0 = argv[0]; taskargc = argc; taskargv = argv; if(mainstacksize == 0) mainstacksize = 256*1024; taskcreate(taskmainstart, nil, mainstacksize); taskscheduler(); // task 调度器启动 fprint(2, \"taskscheduler returned in main!\\n\"); abort(); return 0; } 调度器的工作是从 runnable_queue 中获得一个可执行的任务，然后切换过去 static void taskscheduler(void) { int i; Task *t; taskdebug(\"scheduler enter\"); for(;;){ if(taskcount == 0) exit(taskexitval); t = taskrunqueue.head; if(t == nil){ fprint(2, \"no runnable tasks! %d tasks stalled\\n\", taskcount); exit(1); } deltask(\u0026taskrunqueue, t); t-\u003eready = 0; taskrunning = t; tasknswitch++; taskdebug(\"run %d (%s)\", t-\u003eid, t-\u003ename); contextswitch(\u0026taskschedcontext, \u0026t-\u003econtext); //print(\"back in scheduler\\n\"); taskrunning = nil; if(t-\u003eexiting){ if(!t-\u003esystem) taskcount--; i = t-\u003ealltaskslot; alltask[i] = alltask[--nalltask]; alltask[i]-\u003ealltaskslot = i; free(t); } } } coroutine 是如何切换的呢？由计算机的工作原理知道，程序的执行是根据 pc 寄存器中的位置取指令执行的，执行 task 需要 stack 和寄存器。寄存器中的数据就是一个 coroutine 的 上下文，记录下来，set 成其他的 上下文的 寄存器的值，即可达到切换 coroutine 的目的。由于不同平台的寄存器不同，指令集不同，需要区分平台做不同的事情: // linux x86 平台的指令 // https://github.com/mixinlib/libtask/blob/651d7b69a4f7b10c798dcd544e6a25fd7505632c/asm.S#L43 #ifdef NEEDX86CONTEXT .globl SET SET: movl 4(%esp), %eax movl 8(%eax), %fs movl 12(%eax), %es movl 16(%eax), %ds movl 76(%eax), %ss movl 20(%eax), %edi movl 24(%eax), %esi movl 28(%eax), %ebp movl 36(%eax), %ebx movl 40(%eax), %edx movl 44(%eax), %ecx movl 72(%eax), %esp pushl 60(%eax) /* new %eip */ movl 48(%eax), %eax ret .globl GET GET: movl 4(%esp), %eax movl %fs, 8(%eax) movl %es, 12(%eax) movl %ds, 16(%eax) movl %ss, 76(%eax) movl %edi, 20(%eax) movl %esi, 24(%eax) movl %ebp, 28(%eax) movl %ebx, 36(%eax) movl %edx, 40(%eax) movl %ecx, 44(%eax) movl $1, 48(%eax) /* %eax */ movl (%esp), %ecx /* %eip */ movl %ecx, 60(%eax) leal 4(%esp), %ecx /* %esp */ movl %ecx, 72(%eax) movl 44(%eax), %ecx /* restore %ecx */ movl $0, %eax ret #endif // c 中的 Context struct Context { ucontext_t uc; }; 到此，用户级 task( coroutine )切换/调度/运行工作基本可以正常工作了。 总结一下： coroutine 是单线程程序，可以串行的跑多个任务；多个任务通过一个 scheduler task 切换；切换的方式是通过替换cpu的寄存器实现的；切换的过程 os kernel 是不知道的; 由于 context switch 只是执行 30+ 个汇编指令，（os context switch 需要至少两次 context switch，以及一些状态维护，相比较来说）开销更小，cpu 的利用率会提高不少；使用同步的方式写出可以异步执行的代码，符合人的普遍思考方式。 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:2:0","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"channel 的工作方式 这一节内容和协程机理没有直接联系，但是因为channel总是伴随着goroutine出现，所以我们 顺便了解一下channel的原理也颇有好处。幸运的是，libtask 中也提供了channel的参考实现. channel 分为有 buf 和 无 buf channel 对于无 buf 的 channel 某一个 task 执行 chansendul 会直接将当前 task 加入 channel 的 send 队列中，然后触发一次 context switch。由于此时 channel 的 task 已经不在 taskrunqueue 中，发送数据到 channel 的 task 不会再被 taskscheduler 调度到，直到 chanrecvul task 被调度，且消费 channel 中的数据，此时 发送端 task 重新被放到可执行队列中。 对于 有 buf 的 channel 某一个 task 执行 chansendul，如果 channel buf 没有满，则会将数据复制到 channel buf 中，task 会继续执行。 如果 channel buf 已满，会将 send task 进 channel send 等待队列，并让出 cpu。 chanrecvul task 接受数据时候，如果 channel buf 不为空，则会直接消费 buf 中的数据；如果 buf 为空，则会 加入 channel 的 recv 等待队列，并让出 cpu，直到有新的数据被send 到 channel 为止。 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:2:1","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"golang goroutine 工作方式 了解了传统 coroutine 的实现方式，我们来看一下 golang 的 goroutine 的工作方式. golang 实现了一种叫做 工作窃取任务调度. 具体的功能介绍见: [翻译]The Go scheduler 具体的更详细的源码分析见：Go 调度器 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:3:0","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"基本的组成元素和工作方式 G、P、M G: 表示 Goroutine，每个 Goroutine 对应一个 G 结构体，G 存储 Goroutine 的运行堆栈、状态以及任务函数，可重用。G 并非执行体，每个 G 需要绑定到 P 才能被调度执行。 P: Processor，表示逻辑处理器， 对 G 来说，P 相当于 CPU 核，G 只有绑定到 P(在 P 的 local runq 中)才能被调度。对 M 来说，P 提供了相关的执行环境(Context)，如内存分配状态(mcache)，任务队列(G)等，P 的数量决定了系统内最大可并行的 G 的数量（前提：物理 CPU 核数 \u003e= P 的数量），P 的数量由用户设置的 GOMAXPROCS 决定，但是不论 GOMAXPROCS 设置为多大，P 的数量最大为 256。 M: Machine，OS 线程抽象，代表着真正执行计算的资源，在绑定有效的 P 后，进入 schedule 循环；而 schedule 循环的机制大致是从 Global 队列、P 的 Local 队列以及 wait 队列中获取 G，切换到 G 的执行栈上并执行 G 的函数，调用 goexit 做清理工作并回到 M，如此反复。M 并不保留 G 状态，这是 G 可以跨 M 调度的基础，M 的数量是不定的，由 Go Runtime 调整，为了防止创建过多 OS 线程导致系统调度不过来，目前默认最大限制为 10000 个。 任务队列共有两种，一种是 p 上的本地队列，另一种是 global queue。当使用 go 关键词启动一个新的 goroutine 的时候，这既可能是全局的运行队列，也可能是处理器本地的运行队列。 runtime.runqput 可知， 当 next 为 true 时，会将 Goroutine 设置到处理器的 runnext 作为下一个处理器执行的任务； 当 next 为 false 并且本地队列还有剩余空间，Goroutine 会被放到本地运行队列上； 当处理器的本地运行队列已经没有剩余空间时就会把本地队列中的一部分 Goroutine 和待加入的 Goroutine 通过 runtime.runqputslow 添加到调度器持有的全局运行队列上； 调度可执行任务的时候。 runtime.schedule 函数会从下面几个地方查找待执行的 Goroutine： 为了保证公平，当全局运行队列中有待执行的 Goroutine 时，通过 schedtick 保证有一定几率会从全局的运行队列中查找对应的 Goroutine； 从处理器本地的运行队列中查找待执行的 Goroutine； 如果前两种方法都没有找到 Goroutine，会通过 runtime.findrunnable 进行阻塞地查找 Goroutine. 如果处理器没有任务可处理，它会按以下规则来执行，直到满足某一条规则： 从本地队列获取任务 从全局队列获取任务 从网络轮询器获取任务 从其它的处理器的本地队列窃取任务 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:3:1","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"基于 goroutine 工作方式，有哪些优点点缺点，缺点如何规避 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:4:0","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"优缺点 优点： 由于 golang goroutine 实现的 scheduler 比较简单，在大量 goroutine 调度时候性能消耗相比较 os thread 的调度来说会低至少一个数量级。 缺点： scheduler 的实现细节非常多，复杂 用户想 hack 自己的线程模型基本不可能 (当然 golang 的 scheduler 已经足够优秀了) ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:4:1","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"goroutine pool 10M(百万) goroutine 执行轻量操作会出现瓶颈，需要考虑 goroutine pool 来做优化了 golang 有 gc，goroutine 的创建与删除还是需要申请内存的，如果可以，尽量使用 pool，让 goroutine调度压力尽量小一点，用户程序复用 goroutine。可以使用 goroutine pool 实现 goroutine 的复用。同时不好的 pool 实现也会让一把锁破坏掉好的性能。是否使用，需要视情况而定. ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:4:2","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"ref GMP 并发调度器深度解析之手撸一个高性能 goroutine pool Scheduler structures golang/proposal Scalable Go Scheduler Design Doc Scalable Go Scheduler Design Doc -zh Scalable Go Scheduler Design Doc -en 第一代 Go Preemptive Scheduler Design Doc Scheduling Multithreaded Computations by Work Stealing 万字长文深入浅出 Golang Runtime Go’s work-stealing scheduler The Go runtime scheduler source go-scheduler go-scheduler-zh Go 语言十年而立，Go2 蓄势待发 ","date":"2021-08-08","objectID":"/go-scheduler-intenal-and-gmp/:5:0","tags":null,"title":"golang 调度器底层原理及 GMP 工作原理","uri":"/go-scheduler-intenal-and-gmp/"},{"categories":null,"content":"TODO: golang scheduler 相关的内容 ","date":"2021-08-02","objectID":"/go-abstract/:0:0","tags":null,"title":"Go 语言十年而立，Go2 蓄势待发","uri":"/go-abstract/"},{"categories":null,"content":"Go 语言十年而立，Go2 蓄势待发 在 21 世纪的第一个十年，计算机在中国大陆才逐渐开始普及，高校的计算机相关专业也逐渐变得热门。当时学校主要以 C/C++和 Java 语言学习为主，而这些语言大多是上个世纪 90 年代或更早诞生的，因此这些计算机领域的理论知识或编程语言仿佛是上帝创世纪时的产物，作为计算机相关专业的学生只能仰望这些成果。 Go 语言诞生在 21 世纪新一波工业编程语言即将爆发的时期。在 2010 年前后诞生了编译型语言 Rust、Kotlin 和 Swift 语言，前端诞生了 Dart、TypeScript 等工业型语言，最新出现的 V 语言更甚至尝试站在 Go 和 Rust 语言肩膀之上创新。而这些变化都发生在我们身边，让中国的计算机爱好者在学习的过程中见证历史的发展，甚至有机会参与其中。 2019 年是 CSDN 的二十周年，也是 Go 语言面世十周年。感谢 CSDN 平台提供的机会，让笔者可以跟大家分享十年来中国 Go 语言社区的一些故事。 Go 语言诞生 Go 语言最初由 Google 公司的 Robert Griesemer、Ken Thompson 和 Rob Pike 三位大牛于 2007 年开始设计发明的。**其设计最初的洪荒之力来自于对超级复杂的 C++11 特性的吹捧报告的鄙视，最终目标是设计网络和多核时代的 C 语言。**到 2008 年中期，语言的大部分特性设计已经完成，并开始着手实现编译器和运行，大约在这一年 Russ Cox 作为主力开发者加入。到了 2009 年，Go 语言已经逐步趋于稳定。同年 9 月，Go 语言正式发布并开源了代码。 以上是《Go 语言高级编程》一书中第一章第一节的内容。Go 语言刚刚开源的时候，大家对它的编译速度印象异常深刻：秒级编译完成，几乎像脚本一样可以马上编译并执行。同时 Go 语言的隐式接口让一个编译型语言有了鸭子类型的能力，笔者也第一次认识到原来 C++的虚表 vtab 也可以动态生成！至于大家最愿意讨论的并非特性，其实并不是 Go 语言新发明的基石，早在上个世纪的八九十年代就有诸多语言开始陆续尝试将 CSP 理论引入编程语言（Rob Pike 是其中坚定的实践者）。只不过早期的 CSP 实践的语言没有进入主流开发领域，导致大家对这种并发模式比较陌生。 除了语言特性的创新之外，Go 语言还自带了一套编译和构建工具，同时小巧的标准库携带了完备的 Web 编程基础构建，我们可以用 Go 语言轻松编写一个支持高并发访问的 Web 服务。 作为互联网时代的 C 语言，Go 语言终于强势进入主流的编程领域。 Go 语言十年奋进 Go 从 2007 年开始设计，在 2009 年正式对外公布，至今刚好十年。十年来 Go 语言以稳定著称，Go1.0 的代码在 2019 年依然可以不用修改直接被编译运行。但是在保持语言稳定的同时，Go 语言也在逐步夯实基础，十年来一直向着完美的极限逼近。让我们看看这十年来 Go 语言有哪些变化。 界面变化 首先是看看界面的变化。第一次是在 2009 刚开源的时候，这时候可以说是 Go 语言的上古时代。Go 语言的主页如下： 那个年代的 Gopher 们，使用的是 hg 工具下载代码（而不是 Git），Go 代码是在 Google Code 托管（而不是 GitHub）。随着代码的发展，hg 已经慢慢淡出 Gopher 视野，Google Code 网站也早已经关闭，而 Go1 之前的上古时代的 Go 老代码已经开始慢慢腐化了。 首页中心是 Go 语言最开始的口号：Go 语言是富有表现力的、并发的编程语言，并且是简洁的。同时给了一个“Hello, 世界”的例子（注意，这里的“世界”是日文）。 然后右上角是初学者的乐园：首先是安装环境，然后可能是早期的三日教程，第三个是标准库的使用。右上角的图片是 Russ Cox 的一个视频，在 Youtube 应该还能找到。 左上角是 Go 实战的那个经典文档。此外 FAQ、语言规范、内存模型是非常重要的核心温度。左下角还有 cmd 等文档链接，子页面的内容应该没有什么变化。 然后在 2012 年准备发布第一个正式版本 Go1，在 Go1 之前语言、标准库和 godoc 都进行了大量的改进。Go1 风格的页面效果如下： 新页面刚出来的时候有眼睛一亮的感觉，这个是目前存在时间最长久的页面布局。但是不仅仅是笔者我，甚至 Go 语言官方也慢慢对中国页面有点审美疲劳了。因此，从 2018 年开始 Go 语言开始新的 Logo 和网站的重新设计工作。 下面的是 Go 语言新的 Logo： 2019 年是对 Go 语言发展极其重要的一年，今年 8 月将发布 Go1.13，而这个版本将正式重启 Go 语言语法的进化，向着 Go2 前进。而新的网站已经在 Go1.13 正式发布之前的 7 月份就已经上线： 头部的按钮风格的菜单变成了平铺的风格，显得更加高大上。同时页面的颜色做了调整，保持和新 Logo 颜色一致。页面的布局也做了调整，将下载左右两列做了调换。同时地鼠的脑袋歪到一边，估计是颈椎病复发了。 总的来说，Go 语言官网主页经历了 Go1 前、Go1（1.0 ～ 1.10）、Go1 后（或者叫 Go2 前）三个阶段，分别对应 3 种风格的页面。新的布局或许会成为下个十年 Go2 的主力页面。 语法变化 Go 语言虽然从 2009 年诞生，但是到了 2012 年才发布第一个正式的版本 Go1。其实在 Go1 诞生之前 Go 语言就已经足够稳定了，国内的七牛云从 Go1 之前就开始大力转向 Go 语言开发，是国内第一家广泛采用 Go 语言开发的互联网公司。Go1 的目标是梳理语法和标准库阴暗的角落，为后续的 10 年打下坚实的基础。 从目前的结果看，Go1 无疑是取得了极大的成果，Go1 时代的代码依然可以不用修改就可以用最新的 Go 语言工具编译构建（不包含 CGO 或汇编语言部分，因为这些外延的工具并不在 Go1 的承诺范围）。但是 Go1 之后依然有一些语法的更新，在 Go1.10 前的 Go1 时代语法和标准库部分的重大变化主要有三个： 第一个重大的语法变化是在 2012 年发布的 Go1.2 中，给切片语法增加了容量的控制，这样可以避免不同的切片不小心越界访问有着相同底层数组的其它切片的内存。 **第二个重大的变化是 2016 年发布的 Go1.7 标准库引入了 context 包。**context 包是 Go 语言官方对 Go 进行并发编程的实践成果，用来简化对于处理单个请求的多个 Goroutine 之间与请求域的数据、超时和退出等操作。context 包推出后就被社区快速吸收使用，例如 gRPC 以及很多 Web 框架都通过 context 来控制 Goroutine 的生命周期。 **第三个重大的语法变化是 2017 年发布的 Go1.9 ，引入了类型别名的特性：****type T1 = T2。**其中类型别名 T1 是通过=符号从 T2 定义，这里的 T1 和 T2 是完全相同的类型。之所以引入类型别名，很大的原因是为了解决 Go1.7 将 context 扩展库移动到标准库带来的问题。因为标准库和扩展库中分别定义了 context.Context 类型，而不同包中的类型是不相容的。而 gRPC 等很多开源的库使用的是最开始以来的扩展库中的 context.Context 类型，结果导致其无法和 Go1.7 标准库中的 context.Context 类型兼容。这个问题最终通过类型别名解决了：扩展库中的 context.Context 类型是标准库中 context.Context 的别名类型，从而实现了和标准库的兼容。 此外还有一些语法细节的变化，比如 Go1.4 对 for 循环语法进行了增强、Go1.8 放开对有着相同内存布局的结构体强制转型限制。读者可以根据自己新需要查看相关发布日志的文档说明。 运行时的变化 运行时部分最大的变化是动态栈部分。在 Go1.2 之前 Go 语言采用分段栈的方式实现栈的动态伸缩。但是分段式动态栈有个性能问题，因为栈内存不连续会导致 CPU 缓存命中率下降，从而导致热点的函数调用性能受到影响。因此从 Go1.3 开始该有连续式的动态栈。连续式的动态栈虽然部分缓解了 CPU 缓存命中率问题（依然存在栈的切换问题，这可能导致 CPU 缓存失效），但同时也带来了更大的实现问题：栈上变量的地址可能会随着栈的移动而发生变化。这直接带来了 CGO 编程中，Go 语言内存对象无法直接传递给 C 语言空间使用，因此后来 Go 语言官方针对 CGO 问题制定了复杂的内存使用规范。 总体来说，动态栈如何实现是一个如何取舍的问题，因为没有银弹、鱼和熊掌不可兼得，目前的选择是第一保证纯 Go 程序的性能。 GC 性能改进 Go 语言是一个带自动垃圾回收的语言（Garbage Collection ），简称 GC（注意这是大写的 GC，小写的 gc 表示 Go 语言的编译器）。从 Go 语言诞生开始，GC 的回收性能就是大家关注的热点话题。 Go 语言之所以能够支持 GC 特性，是因为 Go 语言中每个变量都有完备的元信息，通过这些元信息可以很容易跟踪全部指针的声明周期。在 Go1.4 之前，GC 采用的是 STW 停止世界的方式回收内存，停顿的时间经常是几秒甚至达到几十秒。因此早期社区有很多如何规避或降低 GC 操作的技巧文章。 第一次 GC 性能变革发生在 Go1.5 时期，这个时候 Go 语","date":"2021-08-02","objectID":"/go-abstract/:1:0","tags":null,"title":"Go 语言十年而立，Go2 蓄势待发","uri":"/go-abstract/"},{"categories":null,"content":"文章简介：Go 高性能研讨讲座 - High Performance Go Workshop 翻译, 使用正确的工具分析程序行为 原文地址 forked from here ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:0:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"Overview 本次研讨讲座的目标是让您能够诊断 Go 应用程序中的性能问题，并且修复这些问题。 这一天，我们将从小做起 - 学习如何编写基准测试，然后分析一小段代码。然后讨论代码执行跟踪器，垃圾收集器和跟踪运行的应用程序。最后会有剩下的时间，您可以提出问题，并尝试编写您自己的代码。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:1:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"Schedule 这里是这一天的时间安排表（大概）。 开始时间 描述 09:00 欢迎 and 介绍 09:30 Benchmarking 10:45 休息 (15 分钟) 11:00 性能评估和分析 12:00 午餐 (90 分钟) 13:30 编译优化 14:30 执行追踪器 15:30 休息 (15 分钟) 15:45 内存和垃圾回收器 16:15 提示和旅行 16:30 练习 16:45 最后的问题和结论 17:00 结束 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:1:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"欢迎 你好，欢迎! 🎉 该研讨的目的是为您提供诊断和修复 Go 应用程序中的性能问题所需的工具。 在这一天里，我们将从一小部分开始 - 学习如何编写基准测试，然后分析一小段代码。然后扩展到，讨论 执行跟踪器，垃圾收集器 和跟踪正在运行的应用程序。剩下的将是提问的时间，尝试自己使用代码来实践。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:2:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"讲师 Dave Cheney dave@cheney.net ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:2:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"开源许可和材料 该研讨会是 David Cheney 和 Francesc Campoy。 此文章以 Creative Commons Attribution-ShareAlike 4.0 International 作为开源协议。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:2:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"预先工作 下面是您今天需要下载的几个软件 讲习代码库 将源代码下载到本文档，并在以下位置获取代码示例 high-performance-go-workshop 电脑执行环境 该项目工作环境目标为 Go 1.12。 Download Go 1.12 如果您已经升级到 Go 1.13，也可以了。在次要的 Go 版本之间，优化选择总是会有一些小的变化，我会在继续进行时指出。 Graphviz 在 pprof 的部分需要 dot 程序，它附带的工具 graphviz 套件。 Linux: [sudo] apt-get install graphviz OSX: MacPorts: sudo port install graphviz Homebrew: brew install graphviz Windows (untested) Google Chrome 执行跟踪器上的这一部分需要使用 Google Chrome。它不适用于 Safari，Edge，Firefox 或 IE 4.01。 Download Google Chrome 您的代码以进行分析和优化 当天的最后部分将是公开讲座，您可以在其中试验所学的工具。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:2:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"还有一些事 …​ 这不是演讲，而是对话。我们将有很多时间来提问。 如果您听不懂某些内容，或认为听不正确，请提出询问。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:2:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1. 微处理器性能的过去，现在和未来 这是一个有关编写高性能代码的研讨会。在其他研讨会上，我谈到了分离的设计和可维护性，但是今天我们在这里谈论性能。 今天，我想做一个简短的演讲，内容是关于我如何思考计算机发展历史以及为什么我认为编写高性能软件很重要。 现实是软件在硬件上运行，因此要谈论编写高性能代码，首先我们需要谈论运行代码的硬件。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.1. Mechanical Sympathy 目前有一个常用术语，您会听到像马丁·汤普森（Martin Thompson）或比尔·肯尼迪（Bill Kennedy）这样的人谈论 Mechanical Sympathy。 Mechanical Sympathy 这个名字来自伟大的赛车 手杰基·斯图尔特（Jackie Stewart），他曾三度获得世界一级方程式赛车冠军。他认为，最好的驾驶员对机器的工作原理有足够的了解，以便他们可以与机器和谐地工作。 要成为一名出色的赛车手，您不需要成为一名出色的机械师，但您需要对汽车的工作原理有一个粗略的了解。 我相信我们作为软件工程师也是如此。我认为会议室中的任何人都不会是专业的 CPU 设计人员，但这并不意味着我们可以忽略 CPU 设计人员面临的问题。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.2. 六个数量级 有一个常见的网络模型是这样的； 当然这是荒谬的，但是它强调了计算机行业发生了多少变化。 作为软件作者，我们这个会议室的所有人都受益于摩尔定律，即 40 年来，每 18 个月将芯片上可用晶体管的数量增加一倍。没有其他行业经历过 六个数量级 [1] 在一生的时间内改进其工具。 但这一切都在改变。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.3. 计算机还在变快吗？ 因此，最基本的问题是，面对上图所示的统计数据，我们应该问这个问题吗 计算机还在变快吗 ? 如果计算机的速度仍在提高，那么也许我们不需要关心代码的性能，只需稍等一下，硬件制造商将为我们解决性能问题。 1.3.1. 让我们看一下数据 这是经典的数据，您可以在 John L. Hennessy 和 David A. Patterson 的 Computer Architecture, A Quantitative Approach 等教科书中找到。该图摘自第 5 版 在第 5 版中，轩尼诗（Hennessy）和帕特森（Patterson）提出了计算性能的三个时代 首先是 1970 年代和 80 年代初期，这是形成性的年代。我们今天所知道的微处理器实际上并不存在，计算机是由分立晶体管或小规模集成电路制造的。成本，尺寸以及对材料科学理解的限制是限制因素。 从 80 年代中期到 2004 年，趋势线很明显。 计算机整数性能每年平均提高 52％。 计算机能力每两年翻一番，因此人们将摩尔定律（芯片上的晶体管数量增加一倍）与计算机性能混为一谈。 然后我们进入计算机性能的第三个时代。 事情变慢了。 总变化率为每年 22％。 之前的图表仅持续到 2012 年，但幸运的是在 2012 年 Jeff Preshing 写了 tool to scrape the Spec website and build your own graph. 因此，这是使用 1995 年 到 2017 年 的 Spec 数据的同一图。 对我而言，与其说我们在 2012 年 的数据中看到的步伐变化，不如说是 单核 性能已接近极限。 这些数字对于浮点数来说稍好一些，但是对于我们在做业务应用程序的房间中来说，这可能并不重要。 1.3.2. 是的，计算机仍在变得越来越慢 关于摩尔定律终结的第一件事要记住，就是戈登·摩尔告诉我的事情。他说：“所有指数都结束了”。 — John Hennessy 这是轩尼诗在 Google Next 18 及其图灵奖演讲中的引文。 他的观点是肯定的，CPU 性能仍在提高。 但是，单线程整数性能仍在每年提高 2-3％ 左右。 以这种速度，它将需要 20 年的复合增长才能使整数性能翻倍。 相比之下，90 年代的表现每天每两年翻一番。 为什么会这样呢？ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.4. 时针速度 2015 年的这张图很好地说明了这一点。 第一行显示了芯片上的晶体管数量。 自 1970 年代以来，这种趋势一直以大致线性的趋势线持续。 由于这是对数/林线图，因此该线性序列表示指数增长。 但是，如果我们看中线，我们看到时钟速度十年来没有增加，我们看到 CPU 速度在 2004 年左右停滞了。 下图显示了散热功率； 即变成电能的电能遵循相同的模式-时钟速度和 cpu 散热是相关的。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.5. 发热 为什么 CPU 会发热？ 这是一台固态设备，没有移动组件，因此此处的摩擦等效果并不（直接）相关。 该图摘自 data sheet produced by TI。 在此模型中，N 型设备中的开关被吸引到正电压，P 型设备被正电压击退。 CMOS 设备的功耗是三个因素的总和，CMOS 功耗是房间，办公桌上和口袋中每个晶体管的功率。 静态功率。当晶体管是静态的，即不改变其状态时，会有少量电流通过晶体管泄漏到地。 晶体管越小，泄漏越多。 泄漏量随温度而增加。当您拥有数十亿个晶体管时，即使是很小的泄漏也会加起来！ 动态功率。当晶体管从一种状态转换到另一种状态时，它必须对连接到栅极的各种电容进行充电或放电。 每个晶体管的动态功率是电压乘以电容和变化频率的平方。 降低电压可以减少晶体管消耗的功率，但是较低的电压会使晶体管的开关速度变慢。 撬棍或短路电流。我们喜欢将晶体管视为数字设备，无论其处于开启状态还是处于原子状态，都占据一种状态或另一种状态。 实际上，晶体管是模拟设备。 当开关时，晶体管开始几乎全部截止，并转变或切换到几乎全部导通的状态。 这种转换或切换时间非常快，在现代处理器中约为皮秒，但是当从 Vcc 到地的电阻路径很低时，这仍然代表了一段时间。 晶体管切换的速度越快，其频率就会耗散更多的热量。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.6. Dennard 扩展的终结 要了解接下来发生的事情，我们需要查看 Robert H. Dennard 于 1974 年共同撰写的论文。 丹纳德的缩放定律大致上指出，随着晶体管的变小，它们的 power density 保持恒定。 较小的晶体管可以在较低的电压下运行，具有较低的栅极电容，并且开关速度更快，这有助于减少动态功率。 那么，结果如何呢？ 事实并非如此。 当晶体管的栅极长度接近几个硅原子的宽度时，晶体管尺寸，电压与重要的泄漏之间的关系就破裂了。 它是在 Micro-32 conference in 1999 假定的，如果我们遵循了提高时钟速度和缩小晶体管尺寸的趋势线，那么在处理器一代之内晶体管结将接近核反应堆堆芯的温度。显然，这是荒谬的。奔腾 4 marked the end of the line 适用于单核高频消费类 CPU。 返回此图，我们看到时钟速度停止的原因是 CPU 超出了我们冷却时钟的能力。 到 2006 年，减小晶体管的尺寸不再提高其功率效率。 现在我们知道，减小 CPU 功能的大小主要是为了降低功耗。 降低功耗不仅意味着“绿色”，例如回收利用，还可以拯救地球。 主要目标是保持功耗，从而保持散热，below levels that will damage the CPU. 但是，图中的一部分在不断增加，即管芯上的晶体管数量。cpu 的行进具有尺寸特征，在相同的给定面积内有更多的晶体管，既有正面影响，也有负面影响。 同样，如您在插入物中所看到的，直到大约 5 年前，每个晶体管的成本一直在下降，然后每个晶体管的成本又开始回升。 制造较小的晶体管不仅变得越来越昂贵，而且变得越来越困难。 2016 年 的这份报告显示了芯片制造商认为在 2013 年 会发生什么的预测。两年后，他们错过了所有预测，尽管我没有此报告的更新版本，但没有迹象表明他们将能够扭转这一趋势。 英特尔，台积电，AMD 和三星都要花费数十亿美元，因为它们必须建造新的晶圆厂，购买所有新的工艺工具。因此，尽管每个芯片的晶体管数量持续增加，但其单位成本却开始增加。 甚至以纳米为单位的术语 栅极长度 也变得模棱两可。 各种制造商以不同的方式测量其晶体管的尺寸，从而使它们在没有交付的情况下可以展示比竞争对手少的数量。这是 CPU 制造商的非 GAAP 收益报告模型。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:6","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.7. 更多的核心 达到温度和频率限制后，不再可能使单个内核的运行速度快两倍。 但是，如果添加另一个内核，则可以提供两倍的处理能力-如果软件可以支持的话。 实际上，CPU 的核心数量主要由散热决定。 Dennard 缩放的末尾意味着 CPU 的时钟速度是 1 到 4 Ghz 之间的任意数字，具体取决于它的温度。在谈论基准测试时，我们会很快看到这一点。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:7","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.8. 阿姆达尔定律 CPU 并没有变得越来越快，但是随着超线程和多核它们变得越来越宽。 移动部件为双核，台式机部件为四核，服务器部件为数十个内核。 这将是计算机性能的未来吗？ 不幸的是没有。 阿姆达尔定律以 IBM/360 的设计者吉姆·阿姆达尔（Gene Amdahl）的名字命名，它是一个公式，它给出了在固定工作负载下任务执行延迟的理论上的加速，这可以通过改善资源的系统来实现。 阿姆达尔定律告诉我们，程序的最大速度受程序顺序部分的限制。 如果您编写的程序的执行力的 95％ 可以并行运行，即使有成千上万的处理器，则程序执行的最大速度也将限制为 20 倍。 考虑一下您每天使用的程序，它们的执行量中有多少是可以解析的？ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:8","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.9. 动态优化 由于时钟速度停滞不前，并且由于抛出额外的内核而产生的回报有限，因此，提速来自何处？ 它们来自芯片本身的体系结构改进。 这些是五到七年的大型项目，名称如下 Nehalem, Sandy Bridge, and Skylake. 在过去的二十年中，性能的改善大部分来自体系结构的改善: 1.9.1. 乱序执行 乱序，也称为超标量，执行是一种从 CPU 正在执行的代码中提取所谓的 指令级并行性 的方法。 现代 CPU 在硬件级别有效地执行 SSA，以识别操作之间的数据依赖性，并在可能的情况下并行运行独立的指令。 但是，任何一段代码固有的并行性数量是有限的。它也非常耗电。大多数现代 CPU 在每个内核上都部署了六个执行单元，因为在流水线的每个阶段将每个执行单元连接到所有其他执行单元的成本为 n 平方。 1.9.2. 预测执行 除最小的微控制器外，所有 CPU 均使用 指令管道 来重叠指令 获取/解码/执行/提交 周期中的部分。 指令流水线的问题是分支指令，平均每 5 到 8 条指令出现一次。当 CPU 到达分支时，它不能在分支之外寻找其他指令来执行，并且直到知道程序计数器也将在何处分支之前，它才能开始填充其管道。推测执行使 CPU 可以“猜测”分支仍要处理的路径，同时仍在处理分支指令！ 如果 CPU 正确预测了分支，则它可以保持其指令流水线满。如果 CPU 无法预测正确的分支，则当它意识到错误时，必须回滚对其 architectural state 所做的任何更改。由于我们都在通过 Spectre 样式漏洞进行学习，因此有时这种回滚并没有像希望的那样无缝。 当分支预测率较低时，投机执行可能会非常耗电。如果分支预测错误，不仅 CPU 回溯到预测错误的地步，而且浪费在错误分支上的能量也被浪费了。 所有这些优化导致我们看到的单线程性能的提高，但要付出大量晶体管和功率的代价。 Cliff Click 的 精彩演讲 认为乱序，并且推测性执行对于尽早开始缓存未命中最有用，从而减少了观察到的缓存延迟。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:9","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.10. 现代 CPU 已针对批量操作进行了优化 现代处理器就像是由硝基燃料驱动的有趣的汽车，它们在四分之一英里处表现出色。不幸的是，现代编程语言就像蒙特卡洛一样，充满了曲折。- 大卫·昂加（David Ungar） 这是来自有影响力的计算机科学家，SELF 编程语言的开发人员 David Ungar 的引言，在很旧的演讲中就引用了 I found online. 因此，现代 CPU 已针对批量传输和批量操作进行了优化。 在每个级别，操作的设置成本都会鼓励您进行大量工作。 一些例子包括 内存不是按字节加载，而是按高速缓存行的倍数加载，这就是为什么对齐变得不再像以前的计算机那样成为问题的原因。 MMX 和 SSE 等向量指令允许一条指令同时针对多个数据项执行，前提是您的程序可以以这种形式表示。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:10","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.11. 现代处理器受内存延迟而不是内存容量的限制 如果 CPU 占用的状况还不够糟，那么从内存方面来的消息就不会好多了。 连接到服务器的物理内存在几何上有所增加。 我在 1980 年代的第一台计算机具有数千字节的内存。 当我读高中时，我所有的论文都是用 3.8 MB 的 386 写在 386 上的。 现在，查找具有数十或数百 GB RAM 的服务器已变得司空见惯，而云提供商则将其推向了 TB 的 TB。 但是，处理器速度和内存访问时间之间的差距继续扩大。 但是，就丢失处理器等待内存的处理器周期而言，物理内存仍与以往一样遥不可及，因为内存无法跟上 CPU 速度的提高。 因此，大多数现代处理器受内存延迟而不是容量的限制。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:11","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.12. 缓存控制着我们周围的一切 几十年来，解决处理器/内存上限的解决方案是添加缓存-一块较小的快速内存，位置更近，现在直接集成到 CPU 中。 但; 数十年来，L1 一直停留在每个核心 32kb L2 在最大的英特尔部分上已缓慢爬升到 512kb L3 现在在 4-32mb 范围内，但其访问时间可变 受高速缓存限制的大小是因为它们 physically large on the CPU die，会消耗大量功率。 要使缓存未命中率减半，您必须将缓存大小提高 四倍。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:12","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.13. 免费午餐结束了 2005 年，C++ 委员会负责人 Herb Sutter 撰写了一篇题为 免费午餐结束 的文章。 萨特（Sutter）在他的文章中讨论了我涵盖的所有要点，并断言未来的程序员将不再能够依靠较快的硬件来修复较慢的程序或较慢的编程语言。 十多年后的今天，毫无疑问，赫伯·萨特（Herb Sutter）是正确的。内存很慢，缓存太小，CPU 时钟速度倒退了，单线程 CPU 的简单世界早已一去不复返了。 摩尔定律仍然有效，但是对于我们这个房间里的所有人来说，免费午餐已经结束。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:13","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"1.14. 结束 我要引用的数字是到 2010 年：30GHz，100 亿个晶体管和每秒 1 兆指令。— Pat Gelsinger, Intel CTO, April 2002 很明显，如果没有材料科学方面的突破，CPU 性能恢复到同比 52％ 增长的可能性几乎很小。普遍的共识是，故障不在于材料科学本身，而在于晶体管的使用方式。用硅表示的顺序指令流的逻辑模型导致了这种昂贵的最终结果。 在线上有许多演示文稿可以重述这一点。 它们都具有相同的预测-将来的计算机将不会像今天这样编程。 有人认为它看起来更像是带有数百个非常笨拙，非常不连贯的处理器的图形卡。 其他人则认为，超长指令字（VLIW）计算机将成为主流。 所有人都同意，我们当前的顺序编程语言将与此类处理器不兼容。 我的观点是这些预测是正确的，此时硬件制造商挽救我们的前景严峻。 但是，我们可以为今天拥有的硬件优化当前程序的范围是 巨大的。 里克·哈德森（Rick Hudson）在 GopherCon 2015 大会上谈到 以\"良好的循环\"重新参与，该软件可以与我们今天拥有的硬件一起工作，而不是仅适用于这种硬件 。 查看我之前显示的图表，从 2015 年到 2018 年，整数性能最多提高了 5-8％，而内存延迟却最多，Go 团队将垃圾收集器的暂停时间减少了 两个数量级。 与使用 Go 1.6 的相同硬件上的同一程序相比，Go 1.11 程序具有更好的 GC 延迟。 这些都不是来自硬件。 因此，为了在当今世界的当今硬件上获得最佳性能，您需要一种编程语言，该语言应： 之所以编译而不是解释，是因为解释后的编程语言与 CPU 分支预测变量和推测性执行之间的交互作用很差。 您需要一种语言来允许编写有效的代码，它需要能够谈论位和字节，并且必须有效地说明整数的长度，而不是假装每个数字都是理想的浮点数。 您需要一种使程序员能够有效地谈论内存，思考结构与 Java 对象的语言，因为所有的指针追逐都会给 CPU 高速缓存带来压力，而高速缓存未命中会消耗数百个周期。 随应用程序的性能而扩展到多个内核的编程语言取决于它使用缓存的效率以及在多个内核上并行化工作的效率。 显然，我们在这里谈论 Go，我相信 Go 拥有了我刚才描述的许多特征。 1.14.1. 这对我们意味着什么？ 只有三种优化：少做些。少做一次。更快地做。 最大的收益来自 1，但我们将所有时间都花在 3 上。 — Michael Fromberger 本讲座的目的是说明，当您谈论程序或系统的性能时，完全是在软件中。等待更快的硬件来挽救一天真是愚蠢的事情。 但是有个好消息，我们可以在软件上进行大量改进，而这就是我们今天要讨论的。 1.14.2. 进一步阅读 The Future of Microprocessors, Sophie Wilson JuliaCon 2018 50 Years of Computer Architecture: From Mainframe CPUs to DNN TPUs, David Patterson The Future of Computing, John Hennessy The future of computing: a conversation with John Hennessy (Google I/O ‘18) ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:3:14","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2. Benchmarking 测量两次，取一次。 — Ancient proverb 在尝试改善一段代码的性能之前，首先我们必须了解其当前性能。 本节重点介绍如何使用 Go 测试框架构建有用的基准，并提供了避免陷阱的实用技巧。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.1. 标杆基准规则 在进行基准测试之前，必须具有稳定的环境才能获得可重复的结果。 机器必须处于闲置状态-不要在共享硬件上进行配置，不要在等待较长基准测试运行时浏览网络。 注意节能和热缩放。这些在现代笔记本电脑上几乎是不可避免的。 避免使用虚拟机和共享云托管；对于一致的测量，它们可能太嘈杂。 如果负担得起，请购买专用的性能测试硬件。机架安装，禁用所有电源管理和热量缩放功能，并且永远不要在这些计算机上更新软件。 最后一点是从系统管理的角度来看糟糕的建议，但是如果软件更新改变了内核或库的执行方式 -想想 Spectre 补丁- 这将使以前的任何基准测试结果无效。 对于我们其他人，请进行前后采样，然后多次运行以获取一致的结果。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.2. Using the testing package for benchmarking testing 包内置了对编写基准测试的支持。 如果我们有一个简单的函数，像这样： func Fib(n int) int { switch n { case 0: return 0 case 1: return 1 default: return Fib(n-1) + Fib(n-2) } } 我们可以使用 testing 包通过这种形式为函数编写一个 基准。 func BenchmarkFib20(b *testing.B) { for n := 0; n \u003c b.N; n++ { Fib(20) // run the Fib function b.N times } } 基准测试功能与您的测试一起存在于 _test.go 文件中。 Benchmarks are similar to tests, the only real difference is they take a *testing.B rather than a *testing.T. Both of these types implement the testing.TB interface which provides crowd favorites like Errorf(), Fatalf(), and FailNow(). 基准测试类似于测试，唯一的不同是基准测试采用的是 *testing.B，而不是 *testing.T。这两种类型都实现了 testing.TB 接口，该接口提供了诸如 Errorf()，Fatalf() 和 FailNow() 之类的方法。 2.2.1. 运行软件包的基准测试 As benchmarks use the testing package they are executed via the go test subcommand. However, by default when you invoke go test, benchmarks are excluded. 当基准测试使用 测试 软件包时，它们通过 go test 子命令执行。 但是，默认情况下，当您调用 go test 时，将排除基准测试。 要在包中显式运行基准测试，请使用 -bench 标志。-bench 采用与您要运行的基准测试名称匹配的正则表达式，因此调用包中所有基准测试的最常见方法是 -bench=. 这是一个例子： % go test -bench=. ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib20-8 30000 40865 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 1.671s go test 还将在匹配基准之前运行软件包中的所有测试，因此，如果软件包中有很多测试，或者它们花费很长时间，则可以通过 go test 提供的 -run 参数来排除它们，正则表达式不匹配； 即。 go test -run=^$ 2.2.2. 基准如何运作 每个基准函数调用的 b.N 值都不同，这是基准应运行的迭代次数。 如果基准功能在 1 秒内（默认值）在 1 秒内完成，则 b.N 从 1 开始，然后 b.N 增加，基准功能再次运行。 b.N increases in the approximate sequence; 1, 2, 3, 5, 10, 20, 30, 50, 100, and so on. The benchmark framework tries to be smart and if it sees small values of b.N are completing relatively quickly, it will increase the the iteration count faster. b.N 以近似顺序增加；1, 2, 3, 5, 10, 20, 30, 50, 100 等。基准框架试图变得聪明，如果看到较小的 b.N 值相对较快地完成，它将更快地增加迭代次数。 查看上面的示例，BenchmarkFib20-8 发现循环的大约 30,000 次迭代花费了超过一秒钟的时间。从那里基准框架计算得出，每次操作的平均时间为 40865ns。 后缀 -8 与用于运行此测试的 GOMAXPROCS 的值有关。 该数字 GOMAXPROCS 默认为启动时 Go 进程可见的 CPU 数。 您可以使用 -cpu 标志来更改此值，该标志带有一个值列表以运行基准测试。 % go test -bench=. -cpu=1,2,4 ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib20 30000 39115 ns/op BenchmarkFib20-2 30000 39468 ns/op BenchmarkFib20-4 50000 40728 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 5.531s 这显示了以 1、2 和 4 核运行基准测试。 在这种情况下，该标志对结果几乎没有影响，因为该基准是完全顺序的。 2.2.3. 提高基准精度 fib 功能是一个稍作设计的示例-除非您编写 TechPower Web 服务器基准测试-否则您的业务不太可能会因您能够快速计算出斐波那契序列中的第 20 个数字而受到限制。但是，基准确实提供了有效基准的忠实示例。 具体来说，您希望基准测试可以运行数万次迭代，以便每个操作获得良好的平均值。如果您的基准测试仅运行 100 或 10 次迭代，则这些运行的平均值可能会有较高的标准偏差。如果您的基准测试运行了数百万或数十亿次迭代，则平均值可能非常准确，但会受到代码布局和对齐方式的影响。 为了增加迭代次数，可以使用 -benchtime 标志来增加基准时间。 例如： % go test -bench=. -benchtime=10s ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib20-8 300000 39318 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 20.066s 运行相同的基准，直到达到 b.N 的值，并花费了超过 10 秒的时间才能返回。由于我们的运行时间增加了 10 倍，因此迭代的总数也增加了 10 倍。 结果并没有太大变化，这就是我们所期望的。 为什么报告的总时间是 20 秒，而不是 10 秒？ 如果您有一个基准运行数百万或数十亿次迭代，导致每次操作的时间在微秒或纳秒范围内，则您可能会发现基准值不稳定，因为热缩放，内存局部性，后台处理，gc 活动等。 对于每次操作以 10 纳秒或一位数纳秒为单位的时间，指令重新排序和代码对齐的相对论效应将对基准时间产生影响。 要使用 -count 标志来多次运行基准测试： % go test -bench=Fib1 -count=10 ./examples/fib/ goos: darwin goarch: amd64 BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 1000000000 1.95 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 2000000000 1.97 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 2000000000 1.96 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 2000000000 2.01 ns/op BenchmarkFib1-8 2000000000 1.99 ns/op BenchmarkFib1-8 1000000000 2.00 ns/op Fib(1) 基准测试大约需要 2 纳秒，方差为 +/- 2％。 Go 1.12 中的新增功能是 -benchtime 标志，现在需要进行多次迭代，例如。-benchtime=20x，它将准确地运行您的代码 benchtime 的时间。 尝试以10倍，20倍，50倍，100倍和300倍的 `-benchtime` 运行上面的 fib 测试。 你看到了什么？ If you find that the defaults that go test applies need to be tweaked for a particular package, I suggest codifying those settings in a Makefile so everyone who wants to run your benchmarks can do so with the same settings. 如果您发现需要针对特定的软件包调整 go test 的默认设置，我建议将这些设置编入 Makefile 中，这样，每个想要运行基准测试的人都可以使用相同的设","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.3. 将基准与 Benchstat 进行比较 在上一节中，我建议多次运行基准测试以使更多数据平均。由于本章开头提到的电源管理，后台过程和热管理的影响，这对于任何基准测试都是很好的建议。 我将介绍 Russ Cox 的一个工具 benchstat. % go get golang.org/x/perf/cmd/benchstat Benchstat 可以进行一系列基准测试，并告诉您它们的稳定性。 这是有关电池供电的 Fib(20) 示例。 % go test -bench=Fib20 -count=10 ./examples/fib/ | tee old.txt goos: darwin goarch: amd64 BenchmarkFib20-8 50000 38479 ns/op BenchmarkFib20-8 50000 38303 ns/op BenchmarkFib20-8 50000 38130 ns/op BenchmarkFib20-8 50000 38636 ns/op BenchmarkFib20-8 50000 38784 ns/op BenchmarkFib20-8 50000 38310 ns/op BenchmarkFib20-8 50000 38156 ns/op BenchmarkFib20-8 50000 38291 ns/op BenchmarkFib20-8 50000 38075 ns/op BenchmarkFib20-8 50000 38705 ns/op PASS ok _/Users/dfc/devel/high-performance-go-workshop/examples/fib 23.125s % benchstat old.txt name time/op Fib20-8 38.4µs ± 1% benchstat 告诉我们平均值为 38.8 微秒，样本之间的变化为 +/- 2％。这对于电池供电来说相当不错。 第一次运行是最慢的，因为操作系统已降低 CPU 时钟以节省电量。 接下来的两次运行是最快的，因为操作系统确定这不是工作的短暂高峰，并且提高了时钟速度以尽快完成工作，从而希望能够返回睡觉。 其余运行是操作系统和供热生产的 BIOS 交互功耗。 2.3.1. Improve Fib 确定两组基准之间的性能差异可能是乏味且容易出错的。benchstat 可以帮助我们。 Saving the output from a benchmark run is useful, but you can also save the binary that produced it. This lets you rerun benchmark previous iterations. To do this, use the -c flag to save the test binary—​I often rename this binary from .test to .golden. 保存来自基准运行的输出很有用，但是您也可以保存产生它的 二进制文件。 这使您可以重新运行基准测试以前的迭代。 为此，请使用 -c标志保存测试二进制文件我经常将此二进制文件从 .test 重命名为 .golden。 % go test -c % mv fib.test fib.golden 先前的 Fib 功能具有斐波那契系列中第 0 和第 1 个数字的硬编码值。之后，代码以递归方式调用自身。今天晚些时候，我们将讨论递归的成本，但是目前，假设递归的成本是很高的，尤其是因为我们的算法使用的是指数时间。 对此的简单解决方法是对斐波那契数列中的另一个数字进行硬编码，从而将每个可回溯调用的深度减少一个。 func Fib(n int) int { switch n { case 0: return 0 case 1: return 1 case 2: return 1 default: return Fib(n-1) + Fib(n-2) } } 该文件还包含对 Fib 的综合测试。如果没有通过验证当前行为的测试，请勿尝试提高基准。 为了比较我们的新版本，我们编译了一个新的测试二进制文件并对其进行了基准测试，并使用 benchstat 来比较输出。 % go test -c % ./fib.golden -test.bench=. -test.count=10 \u003e old.txt % ./fib.test -test.bench=. -test.count=10 \u003e new.txt % benchstat old.txt new.txt name old time/op new time/op delta Fib20-8 44.3µs ± 6% 25.6µs ± 2% -42.31% (p=0.000 n=10+10) 比较基准时需要检查三件事 旧时代和新时期的方差 ±。 1-2％ 是好的，3-5％ 是可以的，大于 5％，并且您的某些样本将被认为不可靠。 比较一侧差异较大的基准时，请注意不要有所改善。 p 值。 p 值小于 0.05 表示良好，大于 0.05 表示基准可能没有统计学意义。 缺少样本。benchstat 将报告它认为有效的旧样本和新样本中的多少个，有时即使您执行了 -count=10，也可能只报告了 9 个。 10％ 或更低的拒绝率是可以的，高于 10％ 可能表明您的设置不稳定，并且您可能比较的样本太少。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.4. 避免基准化启动成本 有时，您的基准测试具有一次运行设置成本。b.ResetTimer() 将用于忽略设置中的时间。 func BenchmarkExpensive(b *testing.B) { boringAndExpensiveSetup() b.ResetTimer() // (1) for n := 0; n \u003c b.N; n++ { // function under test } } | 1 | 重置基准计时器 | 如果 每个循环 迭代都有一些昂贵的设置逻辑，请使用 b.StopTimer() 和 b.StartTimer() 暂停基准计时器。 func BenchmarkComplicated(b *testing.B) { for n := 0; n \u003c b.N; n++ { b.StopTimer() // (1) complicatedSetup() b.StartTimer() // (2) // function under test } } | 1 | 暂停基准测试计时器 | | 2 | 恢复计时器 | ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.5. 基准分配 分配数量和大小与基准时间密切相关。 您可以告诉 testing 框架记录被测代码分配的数量。 func BenchmarkRead(b *testing.B) { b.ReportAllocs() for n := 0; n \u003c b.N; n++ { // function under test } } 这是一个使用 bufio 软件包基准测试的示例。 % go test -run=^$ -bench=. bufio goos: darwin goarch: amd64 pkg: bufio BenchmarkReaderCopyOptimal-8 20000000 103 ns/op BenchmarkReaderCopyUnoptimal-8 10000000 159 ns/op BenchmarkReaderCopyNoWriteTo-8 500000 3644 ns/op BenchmarkReaderWriteToOptimal-8 5000000 344 ns/op BenchmarkWriterCopyOptimal-8 20000000 98.6 ns/op BenchmarkWriterCopyUnoptimal-8 10000000 131 ns/op BenchmarkWriterCopyNoReadFrom-8 300000 3955 ns/op BenchmarkReaderEmpty-8 2000000 789 ns/op 4224 B/op 3 allocs/op BenchmarkWriterEmpty-8 2000000 683 ns/op 4096 B/op 1 allocs/op BenchmarkWriterFlush-8 100000000 17.0 ns/op 0 B/op 0 allocs/op You can also use the go test -benchmem flag to force the testing framework to report allocation statistics for all benchmarks run. % go test -run=^$ -bench=. -benchmem bufio goos: darwin goarch: amd64 pkg: bufio BenchmarkReaderCopyOptimal-8 20000000 93.5 ns/op 16 B/op 1 allocs/op BenchmarkReaderCopyUnoptimal-8 10000000 155 ns/op 32 B/op 2 allocs/op BenchmarkReaderCopyNoWriteTo-8 500000 3238 ns/op 32800 B/op 3 allocs/op BenchmarkReaderWriteToOptimal-8 5000000 335 ns/op 16 B/op 1 allocs/op BenchmarkWriterCopyOptimal-8 20000000 96.7 ns/op 16 B/op 1 allocs/op BenchmarkWriterCopyUnoptimal-8 10000000 124 ns/op 32 B/op 2 allocs/op BenchmarkWriterCopyNoReadFrom-8 500000 3219 ns/op 32800 B/op 3 allocs/op BenchmarkReaderEmpty-8 2000000 748 ns/op 4224 B/op 3 allocs/op BenchmarkWriterEmpty-8 2000000 662 ns/op 4096 B/op 1 allocs/op BenchmarkWriterFlush-8 100000000 16.9 ns/op 0 B/op 0 allocs/op PASS ok bufio 20.366s ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.6. 注意编译器的优化 这个例子来自 issue 14813. const m1 = 0x5555555555555555 const m2 = 0x3333333333333333 const m4 = 0x0f0f0f0f0f0f0f0f const h01 = 0x0101010101010101 func popcnt(x uint64) uint64 { x -= (x \u003e\u003e 1) \u0026 m1 x = (x \u0026 m2) + ((x \u003e\u003e 2) \u0026 m2) x = (x + (x \u003e\u003e 4)) \u0026 m4 return (x * h01) \u003e\u003e 56 } func BenchmarkPopcnt(b *testing.B) { for i := 0; i \u003c b.N; i++ { popcnt(uint64(i)) } } 您认为该功能将以多快的速度进行基准测试？ 让我们找出答案。 % go test -bench=. ./examples/popcnt/ goos: darwin goarch: amd64 BenchmarkPopcnt-8 2000000000 0.30 ns/op PASS\u003c/pre\u003e 0.3 纳秒；这基本上是一个时钟周期。即使假设每个时钟周期中 CPU 可能正在运行一些指令，该数字似乎也过低。发生了什么？ To understand what happened, we have to look at the function under benchmake, popcnt. popcnt is a leaf function — it does not call any other functions — so the compiler can inline it. 要了解发生了什么，我们必须查看 benchmake 下的函数 popcnt。popcnt 是 叶函数(它不调用任何其他函数) 因此编译器可以内联它。 因为该函数是内联的，所以编译器现在可以看到它没有副作用。 popcnt 不会影响任何全局变量的状态。 因此，消除了该调用。这是编译器看到的： func BenchmarkPopcnt(b *testing.B) { for i := 0; i \u003c b.N; i++ { // optimised away } } 在我测试过的所有 Go 编译器版本中，仍然会生成循环。 但是英特尔 CPU 确实擅长优化循环，尤其是空循环。 2.6.1. 练习，实践 在继续之前，让我们实践一下以确认我们所看到的 % go test -gcflags=-S 使用 `gcflags=\"-l -S\"` 禁用内联，这可以影响程序输出 要消除的事情是与通过消除不必要的计算来使实际代码相同的优化，即消除了没有可观察到的副作用的基准测试。 随着 Go 编译器的改进，这只会变得越来越普遍。 2.6.2. 修复基准 禁用内联以使基准测试有效是不现实的；我们希望在优化的基础上构建代码。 为了修正这个基准，我们必须确保编译器无法 证明 BenchmarkPopcnt 的主体不会引起全局状态的改变。 var Result uint64 func BenchmarkPopcnt(b *testing.B) { var r uint64 for i := 0; i \u003c b.N; i++ { r = popcnt(uint64(i)) } Result = r } 这是确保编译器无法优化循环主体的推荐方法。 首先，我们通过将其存储在 r 中来使用调用 popcnt 的结果。其次，由于一旦基准测试结束，r 就在 BenchmarkPopcnt 的范围内局部声明，所以 r 的结果对于程序的另一部分永远是不可见的，因此作为最后的动作，我们将 r 的值赋值储存到包公共变量 Result。 Because Result is public the compiler cannot prove that another package importing this one will not be able to see the value of Result changing over time, hence it cannot optimise away any of the operations leading to its assignment. What happens if we assign to Result directly? Does this affect the benchmark time? What about if we assign the result of popcnt to _? 由于 Result 是公开的，因此编译器无法证明导入该软件包的另一个包将无法看到 Result 的值随时间变化，因此它无法优化导致其赋值的任何操作。 如果直接分配给 Result 会怎样？这会影响基准时间吗？如果将 popcnt 的结果赋给 _ 会怎么样？ 在我们之前的 Fib 基准测试中，我们没有采取这些预防措施，应该这样做吗？ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:6","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.7. 基准误差 for 循环对于基准测试的运行至关重要。 这是两个不正确的基准，您能解释一下它们有什么问题吗？ func BenchmarkFibWrong(b *testing.B) { Fib(b.N) } func BenchmarkFibWrong2(b *testing.B) { for n := 0; n \u003c b.N; n++ { Fib(n) } } 运行这些基准测试，您会看到什么？ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:7","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.8. 分析基准 testing 包内置了对生成 CPU，内存和块配置文件的支持。 -cpuprofile=\u003cspan class=\"katex\"\u003e$1$\u003c/span\u003eFILE. -memprofile=\u003cspan class=\"katex\"\u003e$1$\u003c/span\u003eFILE, -memprofilerate=N 将配置文件速率调整为 1/N. -blockprofile=\u003cspan class=\"katex\"\u003e$1$\u003c/span\u003eFILE. 使用这些标志中的任何一个也会保留二进制文件。 % go test -run=XXX -bench=. -cpuprofile=c.p bytes % go tool pprof c.p ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:8","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"2.9. 讨论 有没有问题？ 也许是时候休息一下了。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:4:9","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"3. 性能评估和性能分析 在上一节中，我们研究了对单个函数进行基准测试，这对您提前知道瓶颈在哪里很有用。但是，通常您会发现自己有一个问题 为什么该程序需要这么长时间才能运行？ 对 整个 程序进行概要分析，对于回答诸如此类的高级问题很有用。在本部分中，我们将使用 Go 内置的性能分析工具从内部调查程序的运行情况。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:5:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"3.1. pprof 今天我们要讨论的第一个工具是 pprof. pprof 来自 Google Perf Tools 这套工具套件，自最早的公开发布以来已集成到 Go 运行时中。 pprof 由两部分组成: runtime/pprof 每个 Go 程序内置的软件包 go tool pprof 用于调查性能分析。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:5:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"3.2. 性能分析文件类型 pprof 支持多种类型的性能分析，今天我们将讨论其中的三种: CPU profiling. Memory profiling. Block (or blocking) profiling. Mutex contention profiling. 3.2.1. CPU 分析 CPU 分析文件是最常见的配置文件类型，也是最明显的配置文件。 启用 CPU 性能分析后，运行时将每 10 毫秒中断一次，并记录当前正在运行的 goroutine 的堆栈跟踪。 分析文件完成后，我们可以对其进行分析以确定最热门的代码路径。 函数在分析文件中出现的次数越多，代码路径花费的时间就越多。 3.2.2. 内存分析 进行 堆 分配时，内存分析记录堆栈跟踪。 堆栈分配假定为空闲，并且在内存性能分析文件中 未跟踪。 像 CPU 分析一样，内存分析都是基于样本的，默认情况下，每 1000 个分配中的内存分析样本为 1。 此速率可以更改。 由于内存分析是基于样本的，并且由于它跟踪未 使用 的 分配，因此很难使用内存分析来确定应用程序的整体内存使用情况。 个人想法: 我发现内存分析对发现内存泄漏没有帮助。有更好的方法来确定您的应用程序正在使用多少内存。 我们将在演示文稿的后面讨论这些。 3.2.3. 块性能分析 块分析是 Go 特有的。 块概要文件类似于 CPU 概要文件，但是它记录 goroutine 等待共享资源所花费的时间。 这对于确定应用程序中的 并发 瓶颈很有用。 块性能分析可以向您显示何时有大量 goroutine 可以 取得进展，但被 阻塞 了。包括阻止 在无缓冲的通道上发送或接收。 正在发送到完整频道，从空频道接收。 试图 锁定 被另一个 goroutine 锁定的 sync.Mutex。 块分析是一种非常专业的工具，在您确信消除了所有 CPU 和内存使用瓶颈之后，才应该使用它。 3.2.4. Mutex profiling 互斥锁概要分析与块概要分析类似，但专门针对导致互斥锁争用导致延迟的操作。 我对这种类型的个人资料没有很多经验，但是我建立了一个示例来演示它。我们将很快看一下该示例。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:5:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"3.3. 同一时间只使用一种性能分析 性能分析不是免费的。 分析对程序性能有中等但可测量的影响，尤其是如果您增加内存配置文件采样率。 大多数工具不会阻止您一次启用多个性能分析。 一次不要启用多种性能分析。 如果您同时启用多个个人资料，他们将观察自己的互动并放弃您的结果。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:5:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"3.4. 收集性能分析 Go 运行时的配置文件界面位于 runtime/pprof 包中。runtime/pprof 是一个非常基础的工具，由于历史原因，与各种配置文件的接口并不统一。 正如我们在上一节中看到的那样，pprof 概要分析内置于 testing 包中，但是有时将您要分析的代码放在 testing.B 基准测试环境中是不便或困难的，并且必须使用直接使用 runtime/pprof API。 几年前，我写了一个 small package，以便更轻松地描述现有应用程序。 import \"github.com/pkg/profile\" func main() { defer profile.Start().Stop() // ... } 在本节中，我们将使用 profile 包。 稍后，我们将直接使用 runtime/pprof 接口。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:5:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"3.5. Analysing a profile with pprof 既然我们已经讨论了 pprof 可以测量的内容以及如何生成配置文件，那么我们就来讨论如何使用 pprof 分析配置文件。 分析由 go pprof 子命令驱动 % go tool pprof /path/to/your/profile 该工具提供了概要数据的几种不同表示形式。文字，图形甚至火焰图。 如果您使用 Go 已有一段时间，则可能会被告知 pprof 有两个参数。从 Go 1.9 开始，配置文件包含渲染配置文件所需的所有信息。您不再需要生成性能分析的二进制文件。🎉 3.5.1. 进一步阅读 Profiling Go programs (Go Blog) Debugging performance issues in Go programs 3.5.2. CPU profiling (exercise) 让我们编写一个计算单词数的程序: package main import ( \"fmt\" \"io\" \"log\" \"os\" \"unicode\" ) func readbyte(r io.Reader) (rune, error) { var buf [1]byte _, err := r.Read(buf[:]) return rune(buf[0]), err } func main() { f, err := os.Open(os.Args[1]) if err != nil { log.Fatalf(\"could not open file %q: %v\", os.Args[1], err) } words := 0 inword := false for { r, err := readbyte(f) if err == io.EOF { break } if err != nil { log.Fatalf(\"could not read file %q: %v\", os.Args[1], err) } if unicode.IsSpace(r) \u0026\u0026 inword { words++ inword = false } inword = unicode.IsLetter(r) } fmt.Printf(\"%q: %d words\\n\", os.Args[1], words) } 让我们看看赫尔曼·梅尔维尔经典小说中有多少个单词 Moby Dick (来自古腾堡计划) % go build \u0026\u0026 time ./words moby.txt \"moby.txt\": 181275 words real 0m2.110s user 0m1.264s sys 0m0.944s 让我们将其与 unix 的 wc -w 进行比较 % time wc -w moby.txt 215829 moby.txt real 0m0.012s user 0m0.009s sys 0m0.002s 所以数字不一样。 wc 大约高出 19％，因为它认为单词与我的简单程序不同。这并不重要-两个程序都将整个文件作为输入，并在一次通过中计算从单词到非单词的过渡次数。 让我们研究一下为什么使用 pprof 这些程序的运行时间不同。 3.5.3. 添加 CPU 分析 首先，编辑 main.go 并启用分析 import ( \"github.com/pkg/profile\" ) func main() { defer profile.Start().Stop() // ... 现在，当我们运行程序时，将创建一个 cpu.pprof 文件。 % go run main.go moby.txt 2018/08/25 14:09:01 profile: cpu profiling enabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof \"moby.txt\": 181275 words 2018/08/25 14:09:03 profile: cpu profiling disabled, /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof 现在我们有了配置文件，可以使用 go pprof 工具对其进行分析。 % go tool pprof /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof Type: cpu Time: Aug 25, 2018 at 2:09pm (AEST) Duration: 2.05s, Total samples = 1.36s (66.29%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 1.42s, 100% of 1.42s total flat flat% sum% cum cum% 1.41s 99.30% 99.30% 1.41s 99.30% syscall.Syscall 0.01s 0.7% 100% 1.42s 100% main.readbyte 0 0% 100% 1.41s 99.30% internal/poll.(*FD).Read 0 0% 100% 1.42s 100% main.main 0 0% 100% 1.41s 99.30% os.(*File).Read 0 0% 100% 1.41s 99.30% os.(*File).read 0 0% 100% 1.42s 100% runtime.main 0 0% 100% 1.41s 99.30% syscall.Read 0 0% 100% 1.41s 99.30% syscall.read top 命令是您最常使用的命令。 我们可以看到该程序有 99％ 的时间花费在 syscall.Syscall 中，而一小部分花费在main.readbyte 中。 我们还可以使用 web 命令来可视化此调用。这将从配置文件数据生成有向图。 在后台，这使用了 Graphviz 的 dot 命令。 但是，在 Go 1.10(也可能是 1.11)中，Go 附带了本身支持 HTTP 服务器的 pprof 版本 % go tool pprof -http=:8080 /var/folders/by/3gf34_z95zg05cyj744_vhx40000gn/T/profile239941020/cpu.pprof 将会打开网络浏览器; 图形模式 火焰图模式 在图形上，占用 最多 CPU 时间的框是最大的框，我们看到 syscall.Syscall 占程序总时间的 99.3％。导致 syscall.Syscall 的字符串表示立即调用者，如果多个代码路径在同一函数上收敛，则可以有多个。箭头的大小代表在一个盒子的子元素上花费了多少时间，我们可以看到，从 main.readbyte 开始，它们占了该图分支中 1.41 秒所用时间的接近 0。 问题: 谁能猜出为什么我们的版本比 wc 慢得多? 3.5.4. 改进 我们的程序运行缓慢的原因不是因为 Go 的 syscall.Syscall 运行缓慢。 这是因为系统调用通常是昂贵的操作（并且随着发现更多 Spectre 系列漏洞而变得越来越昂贵）。 每次对 readbyte 的调用都会导致一个 syscall.Read 的缓冲区大小为 1。因此，我们的程序执行的 syscall 数量等于输入的大小。我们可以看到，在 pprof 图中，读取输入的内容占主导地位。 func main() { defer profile.Start(profile.MemProfile, profile.MemProfileRate(1)).Stop() // defer profile.Start(profile.MemProfile).Stop() f, err := os.Open(os.Args[1]) if err != nil { log.Fatalf(\"could not open file %q: %v\", os.Args[1], err) } b := bufio.NewReader(f) words := 0 inword := false for { r, err := readbyte(b) if err == io.EOF { break } if err != nil { log.Fatalf(\"could not read file %q: %v\", os.Args[1], err) } if unicode.IsSpace(r) \u0026\u0026 inword { words++ inword = false } inword = unicode.IsLetter(r) } fmt.Printf(\"%q: %d words\\n\", os.Args[1], words) } 通过在 readbyte 之前使用 bufio.Reader 包装输入文件 将此修订程序的时间与 `wc` 比较。有多少差距？进行性能分析，看看还剩下什么。 3.5.5. 内存分析 新的单词配置文件表明在 readbyte 函","date":"2020-09-27","objectID":"/high-performance-go-workshop/:5:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4. 编译优化 本节介绍了 Go 编译器执行的一些优化。 例如; 逃逸分析 内联 消除死代码 全部在编译器的前端处理，而代码仍为 AST 形式； 然后将代码传递给 SSA 编译器进行进一步优化。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4.1. History of the Go compiler Go 编译器大约在 2007 时作为 Plan9 编译器工具链的分支而开始的。当时的编译器与 Aho 和 Ullman 的 Dragon Book 非常相似。 在 2015 年，当时的 Go 1.5 编译器为 C 转换为 Go。 一年后，Go 1.7 引入了一种基于 SSA 技术的 new compiler backend 以前的 Plan9 样式代码生成。这个新的后端为通用和特定于架构的优化引入了许多机会。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4.2. 逃逸优化 我们正在讨论的第一个优化是 逃逸优化。 为了说明逃逸分析的作用，我们回想起 Go spec 并未提及堆或堆栈。它仅提及该语言是在引言中被垃圾收集的，并没有提示如何实现该语言。 Go 规范的兼容 Go 实现 可以 将每个分配存储在堆上。这将对垃圾收集器施加很大压力，但是这绝不是不正确的。多年来，gccgo 对逃逸分析的支持非常有限，因此可以有效地认为它在这种模式下运行。 但是，goroutine 的堆栈作为存储局部变量的廉价场所而存在。无需在堆栈上进行垃圾收集。因此，在安全的情况下，放置在堆栈上的分配将更有效。 在某些语言中，例如 C 和 C++，选择在堆栈上还是在堆上分配是程序员的手动操作。堆分配是通过 malloc 和 free 进行的，栈分配是通过 alloca 进行的。使用这些机制的错误是导致内存损坏错误的常见原因。 在 Go 中，如果值的寿命超出了函数调用的寿命，则编译器会自动将其移动到堆中。 说明该值 逃逸 到堆。 type Foo struct { a, b, c, d int } func NewFoo() *Foo { return \u0026Foo{a: 3, b: 1, c: 4, d: 7} } 在这个例子中，在 NewFoo 中分配的 Foo 将被移到堆中，因此在 NewFoo 返回后其内容仍然有效。 自 Go 成立以来，这种情况就一直存在。与其说是自动纠正功能，还不如说是一种优化。在 Go 中意外返回堆栈分配变量的地址是不可能的。 但是编译器也可以做相反的事情。它可以找到假定在堆上分配的东西，并将它们移到堆栈中。 让我们看一个例子 func Sum() int { const count = 100 numbers := make([]int, count) for i := range numbers { numbers[i] = i + 1 } var sum int for _, i := range numbers { sum += i } return sum } func main() { answer := Sum() fmt.Println(answer) } sum 将 1 与 100 之间的 int 相加并返回结果。 由于 numbers 切片仅在 Sum 内部引用，因此编译器将安排将该切片的 100 个整数存储在堆栈中，而不是堆中。无需垃圾回收 numbers，它会在 Sum 返回时自动释放。 4.2.1. 证明它! 要打印编译器的逃逸分析结果，请使用 -m 标志。 % go build -gcflags=-m examples/esc/sum.go # command-line-arguments examples/esc/sum.go:22:13: inlining call to fmt.Println examples/esc/sum.go:8:17: Sum make([]int, count) does not escape examples/esc/sum.go:22:13: answer escapes to heap examples/esc/sum.go:22:13: io.Writer(os.Stdout) escapes to heap examples/esc/sum.go:22:13: main []interface {} literal does not escape \u003cautogenerated\u003e:1: os.(*File).close .this does not escape 第 8 行显示编译器已正确推断出 make([]int, 100) 的结果不会逸出到堆中。没有的原因 第 22 行报告 answer 转储到堆中是 fmt.Println 是一个可变函数。可变参数函数的参数装在切片中，在本例中为 []interface {}，因此将 answer 放入接口值中，因为它是由对 fmt.Println 的调用引用的。由于 Go 1.6 的垃圾回收器要求通过接口传递的所有值都是指针，因此编译器优化后的代码的大致是： var answer = Sum() fmt.Println([]interface{\u0026answer}...) 我们可以使用 -gcflags=\"-m -m\" 标志来确认。在哪返回了 % go build -gcflags='-m -m' examples/esc/sum.go 2\u003e\u00261 | grep sum.go:22 examples/esc/sum.go:22:13: inlining call to fmt.Println func(...interface {}) (int, error) { return fmt.Fprintln(io.Writer(os.Stdout), fmt.a...) } examples/esc/sum.go:22:13: answer escapes to heap examples/esc/sum.go:22:13: from ~arg0 (assign-pair) at examples/esc/sum.go:22:13 examples/esc/sum.go:22:13: io.Writer(os.Stdout) escapes to heap examples/esc/sum.go:22:13: from io.Writer(os.Stdout) (passed to call[argument escapes]) at examples/esc/sum.go:22:13 examples/esc/sum.go:22:13: main []interface {} literal does not escape 简而言之，不必担心第 22 行的逃逸，这对本次讨论并不重要。 4.2.2. 练习 这种优化对所有 count 的值都适用吗？ 如果 count 是变量而不是常量，此优化是否成立？ 如果 count 是 Sum 的参数，此优化是否成立？ 4.2.3. Escape analysis (continued) 这个例子是人为造的。它不旨在成为真实的代码，仅是示例。 type Point struct{ X, Y int } const Width = 640 const Height = 480 func Center(p *Point) { p.X = Width / 2 p.Y = Height / 2 } func NewPoint() { p := new(Point) Center(p) fmt.Println(p.X, p.Y) } NewPoint creates a new *Point value, p. We pass p to the Center function which moves the point to a position in the center of the screen. Finally we print the values of p.X and p.Y. NewPoint 创建一个新的 *Point 值 p。 我们将 p 传递给 Center 函数，该函数将点移动到屏幕中心的位置。 最后，我们输出 p.X 和 p.Y 的值。 % go build -gcflags=-m examples/esc/center.go # command-line-arguments examples/esc/center.go:11:6: can inline Center examples/esc/center.go:18:8: inlining call to Center examples/esc/center.go:19:13: inlining call to fmt.Println examples/esc/center.go:11:13: Center p does not escape examples/esc/center.go:19:15: p.X escapes to heap examples/esc/center.go:19:20: p.Y escapes to heap examples/esc/center.go:19:13: io.Writer(os.Stdout) escapes to heap examples/esc/center.go:17:10: NewPoint new(Point) does not escape examples/esc/center.go:19:13: NewPoint []interface {} literal does not escape \u003cautogenerated\u003e:1: os.(*File).close .this does not escape 即使使用新函数分配了 p，也不会将其存储在堆中，因为没有引用 p 会逸出 Center 函数。 问题: 那第 19 行，如果 p 不逃逸，那是什么逃逸到了堆呢？ 编写一个基准，以规定 Sum 不分配。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4.3. 内联 在 Go 函数中，调用具有固定的开销；堆栈和抢占检查。 硬件分支预测器可以改善其中的一些功能，但是就功能大小和时钟周期而言，这仍然是一个代价。 内联是避免这些成本的经典优化方法。 直到 Go 1.11 内联仅在 叶函数 上起作用，该函数不会调用另一个函数。这样做的理由是: 如果您的函数做了很多工作，那么前导开销将可以忽略不计。这就是为什么功能要达到一定的大小（目前有一些指令，加上一些阻止全部内联的操作，例如，在 Go 1.7 之前进行切换） 另一方面，小的功能为执行的相对少量的有用工作支付固定的开销。这些是内联目标的功能，因为它们最大程度地受益。 另一个原因是过多的内联使堆栈跟踪更难遵循。 4.3.1. Inlining (example) func Max(a, b int) int { if a \u003e b { return a } return b } func F() { const a, b = 100, 20 if Max(a, b) == b { panic(b) } } 同样，我们使用 -gcflags=-m 标志来查看编译器的优化决策。 % go build -gcflags=-m examples/inl/max.go # command-line-arguments examples/inl/max.go:4:6: can inline Max examples/inl/max.go:11:6: can inline F examples/inl/max.go:13:8: inlining call to Max examples/inl/max.go:20:6: can inline main examples/inl/max.go:21:3: inlining call to F examples/inl/max.go:21:3: inlining call to Max 编译器打印了两行。 第 3 行中的第一个是 Max 的声明，告诉我们可以内联。 第二个报告说，Max 的主体已在第 12 行内联到调用方中。 在不使用 //go:noinline comment 的情况下，重写 Max 使得它仍然返回正确的答案，但是编译器不再认为它是可内联的。 4.3.2. 内联是什么样的？ 编译 max.go，看看 F() 的优化版本是什么。 % go build -gcflags=-S examples/inl/max.go 2\u003e\u00261 | grep -A5 '\"\".F STEXT' \"\".F STEXT nosplit size=2 args=0x0 locals=0x0 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) TEXT \"\".F(SB), NOSPLIT|ABIInternal, $0-0 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:11) FUNCDATA $3, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (/Users/dfc/devel/high-performance-go-workshop/examples/inl/max.go:13) PCDATA \u003cspan class=\"katex\"\u003e$1$\u003c/span\u003e0 一旦将 Max 内联到其中，它就是 F 的主体, 此功能没有任何反应。 我知道屏幕上有很多文本，但是什么也没说，但请您相信，唯一发生的是 RET。 实际上，F 变为： func F() { return } -S 的输出不是二进制文件中的最终机器代码。链接器在最终链接阶段进行一些处理。像 FUNCDATA 和 PCDATA 这样的行是垃圾收集器的元数据，它们在链接时会移到其他位置。如果您正在读取 -S 的输出，则只需忽略 FUNCDATA 和 PCDATA 行；它们不是最终二进制文件的一部分。 4.3.3. 讨论 为什么在 F() 中声明 a 和 b 为常数？ 实验输出以下内容：如果将 a 和 b 声明为变量，会发生什么？ 如果 a 和 b 作为参数传递给 F() 会怎样？ -gcflags=-S 不会阻止在您的工作目录中构建最终的二进制文件。如果发现随后的 go build … 运行没有输出，请删除工作目录中的 ./max 二进制文件。 4.3.4. 调整内联级别 调整 内联级别 是通过 -gcflags = -l 标志执行的。有些令人困惑的传递单个 -l 将禁用内联，而两个或多个将启用更激进的设置的内联。 -gcflags=-l, 禁用内联. 没有，正常内联。 -gcflags='-l -l' 内联级别 2，更具攻击性，可能更快，可能会生成更大的二进制文件。 -gcflags='-l -l -l' 内联 3 级，再次更具攻击性，二进制文件肯定更大，也许再次更快，但也可能有问题。 -gcflags=-l=4 Go 1.11 中的（四个 -l）将启用实验性 mid stack inlining optimisation。我相信从 Go 1.12 开始它没有任何作用。 4.3.5. 中栈内联 由于 Go 1.12 已启用所谓的 中栈 内联（以前在 Go 1.11 中的预览中带有 -gcflags ='-l -l -l -l' 标志）。 我们可以在前面的示例中看到中栈内联的示例。在 Go 1.11 和更早的版本中，F 不会是叶子函数，它称为 max。但是由于内联的改进，现在将 F 内联到其调用方中。这有两个原因。当将 max 内联到 F 中时，F 不包含其他函数调用，因此，如果未超过其复杂性预算，它将成为潜在的 叶函数。由于 F 是一个简单的函数，内联和消除死代码消除了许多复杂性预算-它有资格进行 中栈 内联，而与调用max无关。 中栈内联可用于内联函数的快速路径，从而消除了快速路径中的函数调用开销。最近进入 Go 1.13 的 CL 显示了此技术应用于 sync.RWMutex.Unlock()。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4.4. 消除无效代码 为什么 a 和 b 是常量很重要？ 要了解发生了什么，让我们看一下编译器将其 Max 内联到 F 中后看到的内容。我们很难从编译器中获得此信息，但是直接手动完成是很简单的。 Before: func Max(a, b int) int { if a \u003e b { return a } return b } func F() { const a, b = 100, 20 if Max(a, b) == b { panic(b) } } After: func F() { const a, b = 100, 20 var result int if a \u003e b { result = a } else { result = b } if result == b { panic(b) } } 因为 a 和 b 是常量，所以编译器可以在编译时证明该分支永远不会为假。100 始终大于 20。 因此，编译器可以进一步优化 F 以 func F() { const a, b = 100, 20 var result int if true { result = a } else { result = b } if result == b { panic(b) } } 现在已经知道了分支的结果，那么 result 的内容也就知道了。这就是 消除分支. func F() { const a, b = 100, 20 const result = a if result == b { panic(b) } } 现在消除了分支，我们知道 result 总是等于 a，并且因为 a 是常数，所以我们知道 result 是常数。编译器将此证明应用于第二个分支 func F() { const a, b = 100, 20 const result = a if false { panic(b) } } 再次使用分支消除，最终形式为 F。 func F() { const a, b = 100, 20 const result = a } 最后就是 func F() { } 4.4.1. 消除无效代码（续） 分支消除是称为 无效代码消除 的优化类别之一。实际上，使用静态证明来显示，段代码是永远无法访问的，俗称 无效，因此不需要在最终二进制文件中对其进行编译，优化或发出。 我们看到了无效代码消除如何与内联一起工作，以减少通过删除证明无法访问的循环和分支而减少的代码量。 您可以利用此优势实施昂贵的调试，并将其隐藏 const debug = false 与构建标签结合使用，这可能非常有用。 4.4.2. 进一步阅读 Using // +build to switch between debug and release builds How to use conditional compilation with the go build tool ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4.5. 编译器标识练习 % go build -gcflags=$FLAGS 研究以下编译器功能的操作： -S 打印正在编译的程序包的 (Go 风格) 程序集。 -l 控制内联的行为； -l 禁用内联，-l -l 增加内联（更多 -l 增加编译器对内联代码的需求）。尝试编译时间，程序大小和运行时间的差异。 -m 控制诸如内联，转义分析之类的优化决策的打印。-m -m 打印有关编译器思想的更多详细信息。 -l -N 禁用所有优化。 如果发现随后的 go build …​ 运行没有输出，请删除工作目录中的 ./max 二进制文件。 4.5.1 进一步阅读 Codegen Inspection by Jaana Burcu Dogan ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"4.6. 边界检查消除 Go 是一种边界检查语言。这意味着将检查数组和切片下标操作，以确保它们在相应类型的范围内。 对于数组，这可以在编译时完成。对于切片，这必须在运行时完成。 var v = make([]int, 9) var A, B, C, D, E, F, G, H, I int func BenchmarkBoundsCheckInOrder(b *testing.B) { for n := 0; n \u003c b.N; n++ { A = v[0] B = v[1] C = v[2] D = v[3] E = v[4] F = v[5] G = v[6] H = v[7] I = v[8] } } 使用 -gcflags=-S 来拆解 BenchmarkBoundsCheckInOrder。每个循环执行多少个边界检查操作？ func BenchmarkBoundsCheckOutOfOrder(b *testing.B) { for n := 0; n \u003c b.N; n++ { I = v[8] A = v[0] B = v[1] C = v[2] D = v[3] E = v[4] F = v[5] G = v[6] H = v[7] } } 重新安排我们通过 I 分配 A 的顺序是否会影响装配。分解 BenchmarkBoundsCheckOutOfOrder 并找出。 4.6.1. 练习 重新排列下标操作的顺序是否会影响函数的大小？它会影响功能的速度吗？ 如果将 v 移入 基准 函数内部会怎样？ 如果 v 被声明为数组，var v [9] int 会发生什么？ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:6:6","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5. 执行追踪器 执行跟踪程序是由 Dmitry Vyukov 为 Go 1.5 开发的，并且仍处于记录和使用状态，已有好几年了。 与基于样本的分析不同，执行跟踪器集成到 Go 运行时中，因此它只知道 Go 程 序在特定时间点正在做什么，但是 为什么。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.1. 什么是执行跟踪器，为什么需要它？ 我认为，通过查看一段代码 go tool pprof 的效果不好，可以最容易地解释执行跟踪器的功能，以及为什么这样做很重要。 examples/mandelbrot 目录包含一个简单的 mandelbrot 生成器。该代码来自 Francesc Campoy’s mandelbrot package。 % cd examples/mandelbrot % go build \u0026\u0026 ./mandelbrot 如果我们构建它，然后运行它，它将生成如下内容 5.1.1. 多久时间？ 那么，此程序需要多长时间才能生成 1024 * 1024 像素的图像？ 我知道如何执行此操作的最简单方法是使用 time(1) 之类的东西。 % time ./mandelbrot real 0m1.654s user 0m1.630s sys 0m0.015s 不要使用 time go run mandebrot.go，否则您将花费 编译 程序以及运行该程序所需的时间。 5.1.2. 该程序在做什么？ 在此示例中，程序花费了 1.6 秒来生成 mandelbrot 并写入 png。 这样好吗？我们可以加快速度吗？ 回答该问题的一种方法是使用 Go 内置的 pprof 来分析程序。 让我们尝试一下。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.2. 生成性能分析 要生成性能分析，我们需要 直接使用 runtime/pprof 软件包。 使用类似 github.com/pkg/profile 的包装器自动执行此操作。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.3. 使用 runtime/pprof 生成性能分析 为了告诉您我没有使用黑科技(魔法)，让我们修改程序以将 CPU 性能分析写入 os.Stdout。 import \"runtime/pprof\" func main() { pprof.StartCPUProfile(os.Stdout) defer pprof.StopCPUProfile() By adding this code to the top of the main function, this program will write a profile to os.Stdout. 通过将此代码添加到 main 函数的顶部，该程序会将配置文件写入 os.Stdout。 % cd examples/mandelbrot-runtime-pprof % go run mandelbrot.go \u003e cpu.pprof 在这种情况下，我们可以使用 go run，因为 cpu 配置文件将仅包含 mandelbrot.go 的执行，而不包括其编译。 5.3.1. 使用 github.com/pkg/profile 生成性能分析 上一张幻灯片显示了一种生成配置文件的超级简便的方法，但是存在一些问题。 如果您忘记了将输出重定向到文件，则会破坏该终端会话。😞（提示：reset(1) 是您的朋友） 如果您向 os.Stdout 中写入其他内容 (例如，fmt.Println)，则会破坏跟踪。 推荐使用 runtime/pprof 的方法是 将跟踪信息写入文件。 但是，您必须确保跟踪已停止，并且在程序停止之前 (包括有人 ^C) 关闭了文件。 因此，几年前，我写了一个 package 来处理它。 import \"github.com/pkg/profile\" func main() { defer profile.Start(profile.CPUProfile, profile.ProfilePath(\".\")).Stop() 如果运行此版本，则将配置文件写入当前工作目录 % go run mandelbrot.go 2017/09/17 12:22:06 profile: cpu profiling enabled, cpu.pprof 2017/09/17 12:22:08 profile: cpu profiling disabled, cpu.pprof 使用 pkg/profile 不是强制性的，但是它会处理收集和记录跟踪信息的许多样板，因此我们将在本讲习班的其余部分中使用它。 5.3.2. 分析性能 现在我们有了一个性能分析，我们可以使用 go tool pprof 对其进行分析。 % go tool pprof -http=:8080 cpu.pprof 在此运行中，我们看到程序运行了 1.81 秒 (分析增加了少量开销)。我们还可以看到 pprof 仅捕获了 1.53 秒的数据，因为 pprof 基于示例，它依赖于操作系统的 SIGPROF 计时器。 从 1.9 开始，pprof 跟踪包含分析跟踪所需的所有信息。您不再需要生成跟踪的匹配二进制文件。🎉 我们可以使用 top pprof 函数对跟踪记录的函数进行排序 % go tool pprof cpu.pprof Type: cpu Time: Mar 24, 2019 at 5:18pm (CET) Duration: 2.16s, Total samples = 1.91s (88.51%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 1.90s, 99.48% of 1.91s total Showing top 10 nodes out of 35 flat flat% sum% cum cum% 0.82s 42.93% 42.93% 1.63s 85.34% main.fillPixel 0.81s 42.41% 85.34% 0.81s 42.41% main.paint 0.11s 5.76% 91.10% 0.12s 6.28% runtime.mallocgc 0.04s 2.09% 93.19% 0.04s 2.09% runtime.memmove 0.04s 2.09% 95.29% 0.04s 2.09% runtime.nanotime 0.03s 1.57% 96.86% 0.03s 1.57% runtime.pthread_cond_signal 0.02s 1.05% 97.91% 0.04s 2.09% compress/flate.(*compressor).deflate 0.01s 0.52% 98.43% 0.01s 0.52% compress/flate.(*compressor).findMatch 0.01s 0.52% 98.95% 0.01s 0.52% compress/flate.hash4 0.01s 0.52% 99.48% 0.01s 0.52% image/png.filter 我们看到，当 pprof 捕获堆栈时，main.fillPixel 函数在 CPU 上的数量最多。 在堆栈上找到 main.paint 并不奇怪，这就是程序的作用。它绘制像素。但是，是什么导致 paint 花费大量时间呢？ 我们可以通过将 cummulative 标志设置为 top 来进行检查。 (pprof) top --cum Showing nodes accounting for 1630ms, 85.34% of 1910ms total Showing top 10 nodes out of 35 flat flat% sum% cum cum% 0 0% 0% 1840ms 96.34% main.main 0 0% 0% 1840ms 96.34% runtime.main 820ms 42.93% 42.93% 1630ms 85.34% main.fillPixel 0 0% 42.93% 1630ms 85.34% main.seqFillImg 810ms 42.41% 85.34% 810ms 42.41% main.paint 0 0% 85.34% 210ms 10.99% image/png.(*Encoder).Encode 0 0% 85.34% 210ms 10.99% image/png.Encode 0 0% 85.34% 160ms 8.38% main.(*img).At 0 0% 85.34% 160ms 8.38% runtime.convT2Inoptr 0 0% 85.34% 150ms 7.85% image/png.(*encoder).writeIDATs 这暗示着 main.fillPixed 实际上正在完成大部分工作。 您也可以使用 web 命令来形象化配置文件，如下所示： ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.4. 跟踪和性能分析 希望此示例显示了分析的局限性。性能分析告诉我们探查器看到的内容；fillPixel 正在完成所有工作。似乎没有很多事情可以做。 因此，现在是引入执行跟踪程序的好时机，该跟踪程序可以为同一程序提供不同的视图。 5.4.1. 使用执行跟踪器 使用跟踪程序就只需要改变为 profile.TraceProfile 即可。 import \"github.com/pkg/profile\" func main() { defer profile.Start(profile.TraceProfile, profile.ProfilePath(\".\")).Stop() 当我们运行程序时，我们在当前工作目录中得到一个 trace.out 文件。 % go build mandelbrot.go % time ./mandelbrot 2017/09/17 13:19:10 profile: trace enabled, trace.out 2017/09/17 13:19:12 profile: trace disabled, trace.out real 0m1.740s user 0m1.707s sys 0m0.020s 就像 pprof 一样，go 命令中有一个工具可以分析跟踪。 % go tool trace trace.out 2017/09/17 12:41:39 Parsing trace... 2017/09/17 12:41:40 Serializing trace... 2017/09/17 12:41:40 Splitting trace... 2017/09/17 12:41:40 Opening browser. Trace viewer s listening on http://127.0.0.1:57842 这个工具和 go tool pprof 有点不同。执行跟踪器正在重用 Chrome 内置的许多配置文件可视化基础结构，因此 go tool trace 充当服务器将原始执行跟踪转换为 Chome 可以本地显示的数据。 5.4.2. 分析追踪 从跟踪中我们可以看到该程序仅使用一个 cpu。 func seqFillImg(m *img) { for i, row := range m.m { for j := range row { fillPixel(m, i, j) } } } 这并不奇怪，默认情况下，mandelbrot.go 会按顺序为每一行中的每个像素调用 fillPixel。 绘制完图像后，查看执行切换为写入 .png 文件。这会在堆上生成垃圾，因此跟踪在那时发生了变化，我们可以看到垃圾收集堆的经典锯齿模式。 跟踪性能分析可提供低至 毫秒 级别的时序分辨率。 这是您使用外部性能分析无法获得的。 在继续之前，我们需要谈谈跟踪工具的用法。 抱歉，该工具使用 Chrome 内置的 javascript 调试支持。跟踪配置文件只能在 Chrome 中查看，而不能在 Firefox, Safari, IE/Edge 中使用。 因为这是 Google 产品，所以它支持键盘快捷键。使用 WASD 导航，使用 ? 获取列表。 查看追踪可能会占用大量内存。 认真地说，4Gb 不会削减它，8Gb 可能是最小值，更多肯定更好。 如果您是从 Fedora 之类的 OS 发行版中安装 Go 的，则跟踪查看器的支持文件可能不是主 golang deb/rpm 的一部分，它们可能位于某些 -extra 软件包中。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.5. 使用多个 CPU 从前面的跟踪中我们可以看到程序正在按顺序运行，并且没有利用该计算机上的其他 CPU。 Mandelbrot 的生成称为 embarassingly_parallel 。每个像素彼此独立，都可以并行计算。所以，让我们尝试一下。 % go build mandelbrot.go % time ./mandelbrot -mode px 2017/09/17 13:19:48 profile: trace enabled, trace.out 2017/09/17 13:19:50 profile: trace disabled, trace.out real 0m1.764s user 0m4.031s sys 0m0.865s 因此，运行时间基本上是相同的。我们使用了所有 CPU，因此有更多的用户时间，这是有道理的，但是实 际(挂钟) 时间大致相同。 让我们看一下追踪。 如您所见，此跟踪生成了 更多 的数据。 似乎需要完成很多工作，但是如果您放大放大，就会发现差距。据信这是调度程序。 虽然我们使用所有四个内核，但是由于每个 fillPixel 的工作量相对较小，因此我们在调度开销方面花费了大量时间。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.6. 整理工作 每个像素使用一个 goroutine 太细粒度。没有足够的工作来证明 goroutine 的成本合理。 相反，让我们尝试为每个 goroutine 处理一行。 % go build mandelbrot.go % time ./mandelbrot -mode row 2017/09/17 13:41:55 profile: trace enabled, trace.out 2017/09/17 13:41:55 profile: trace disabled, trace.out real 0m0.764s user 0m1.907s sys 0m0.025s 这看起来是一个不错的改进，我们几乎将程序的运行时间减少了一半。 让我们看一下痕迹。 如您所见，轨迹现在更小，更易于使用。我们可以看到跨度的整个轨迹，这是一个不错的奖励。 在程序开始时，我们看到 goroutine 的数量大约为 1000 这是对上一条跟踪中看到的 1 \u003c\u003c 20 的改进。 放大后，我们看到 onePerRowFillImg 运行时间更长，并且由于 goroutine 生产 工作尽早完成，调度程序有效地处理了其余可运行的 goroutine。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:6","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.7. 使用 workers mandelbrot.go 支持另一种模式，请尝试一下 % go build mandelbrot.go % time ./mandelbrot -mode workers 2017/09/17 13:49:46 profile: trace enabled, trace.out 2017/09/17 13:49:50 profile: trace disabled, trace.out real 0m4.207s user 0m4.459s sys 0m1.284s 因此，运行时比以前任何时候都差。让我们看一下追踪，看看是否可以弄清楚发生了什么。 观察痕迹，您会发现只有一个 worker 处理器，生产者和消费者往往会轮换，因为只有一个 worker 处理器和一个消费者。 让我们增加 worker 处理器数量 % go build mandelbrot.go % time ./mandelbrot -mode workers -workers 4 2017/09/17 13:52:51 profile: trace enabled, trace.out 2017/09/17 13:52:57 profile: trace disabled, trace.out real 0m5.528s user 0m7.307s sys 0m4.311s 这样就更糟了！ 更多实时，更多 CPU 时间。让我们看一下追踪，看看发生了什么。 那条痕迹是一团糟。 有更多的 worker 处理器可用，但是似乎所有的时间都花在处理器执行上。 这是因为通道是 无缓存的。只有在有人准备接收之前，无缓冲的通道才能发送。 在没有 worker 处理器准备接收之前，生产者无法发送作品。 worker 处理器要等到有人准备派遣后才能接受工作，因此他们在等待时会互相竞争。 发送者没有特权，它不能比已经运行的工作者享有优先权。 我们在这里看到的是无缓冲通道带来的大量延迟。调度程序内部有很多停止和启动，并且在等待工作时可能会锁定和互斥，这就是为什么我们看到 sys 时间更长的原因。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:7","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.8. 使用缓冲通道 import \"github.com/pkg/profile\" func main() { defer profile.Start(profile.TraceProfile, profile.ProfilePath(\".\")).Stop() % go build mandelbrot.go % time ./mandelbrot -mode workers -workers 4 2017/09/17 14:23:56 profile: trace enabled, trace.out 2017/09/17 14:23:57 profile: trace disabled, trace.out real 0m0.905s user 0m2.150s sys 0m0.121s 这与上面的每行模式非常接近。 使用缓冲的通道，跟踪显示出： 生产者不必等待 worker 处理器的到来，它可以迅速填补渠道。 worker 处理器可以快速从通道中取出下一个物品，而无需休眠等待生产。 使用这种方法，我们使用通道进行每个像素的工作传递的速度几乎与之前在每行 goroutine 上进行调度的速度相同。 修改 nWorkersFillImg 以每行工作。计时结果并分析轨迹。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:8","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"5.9. Mandelbrot 微服务 在 2019 年，除非您可以将 Internet 作为无服务器微服务提供，否则生成 Mandelbrots 毫无意义。 因此，我向您介绍 Mandelweb % go run examples/mandelweb/mandelweb.go 2017/09/17 15:29:21 listening on http://127.0.0.1:8080/ http://127.0.0.1:8080/mandelbrot 5.9.1. 跟踪正在运行的应用程序 在前面的示例中，我们对整个程序进行了跟踪。 如您所见，即使在很短的时间内，跟踪也可能非常大，因此，连续收集跟踪数据将产生太多的数据。同样，跟踪可能会影响程序的速度，特别是在活动很多的情况下。 我们想要的是一种从正在运行的程序中收集简短跟踪的方法。 幸运的是，net/http/pprof 软件包具有这样的功能。 5.9.2. 通过 http 收集跟踪 希望每个人都知道 net/http/pprof 软件包。 import _ \"net/http/pprof\" 导入后，net/http/pprof 将向 http.DefaultServeMux 注册跟踪和分析路由。从 Go 1.5 开始，这包括跟踪分析器。 net/http/pprof 向 http.DefaultServeMux 注册。如果您隐式或显式地使用该 ServeMux，则可能会无意间将 pprof 端点公开到 Internet。这可能导致源代码泄露。您可能不想这样做。 我们可以使用 curl（或wget）从 mandelweb 中获取五秒钟的跟踪记录 % curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5 5.9.3. 产生一些负载 前面的示例很有趣，但是根据定义，空闲的 Web 服务器没有性能问题。我们需要产生一些负载。为此，我使用的是 hey by JBD。 % go get -u github.com/rakyll/hey 让我们从每秒一个请求开始。 % hey -c 1 -n 1000 -q 1 http://127.0.0.1:8080/mandelbrot 然后运行，在另一个窗口中收集跟踪 % curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 66169 0 66169 0 0 13233 0 --:--:-- 0:00:05 --:--:-- 17390 % go tool trace trace.out 2017/09/17 16:09:30 Parsing trace... 2017/09/17 16:09:30 Serializing trace... 2017/09/17 16:09:30 Splitting trace... 2017/09/17 16:09:30 Opening browser. Trace viewer is listening on http://127.0.0.1:60301 5.9.4. 模拟过载 让我们将速率提高到每秒 5 个请求。 % hey -c 5 -n 1000 -q 5 http://127.0.0.1:8080/mandelbrot 然后运行，在另一个窗口中收集跟踪 % curl -o trace.out http://127.0.0.1:8080/debug/pprof/trace?seconds=5 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 66169 0 66169 0 0 13233 0 --:--:-- 0:00:05 --:--:-- 17390 % go tool trace trace.out 2017/09/17 16:09:30 Parsing trace... 2017/09/17 16:09:30 Serializing trace... 2017/09/17 16:09:30 Splitting trace... 2017/09/17 16:09:30 Opening browser. Trace viewer is listening on http://127.0.0.1:60301 5.9.5. 额外的信誉，Eratosthenes 的筛子 concurrent prime sieve 是最早编写的 Go 程序之一。 Ivan Daniluk 撰写了一篇关于可视化的很棒的文章。 让我们看一下使用执行跟踪器的操作。 5.9.6. 更多资源 Rhys Hiltner, Go’s execution tracer (dotGo 2016) Rhys Hiltner, An Introduction to “go tool trace” (GopherCon 2017) Dave Cheney, Seven ways to profile Go programs (GolangUK 2016) Dave Cheney, High performance Go workshop] Ivan Daniluk, Visualizing Concurrency in Go (GopherCon 2016) Kavya Joshi, Understanding Channels (GopherCon 2017) Francesc Campoy, Using the Go execution tracer ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:7:9","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6. 内存和垃圾收集器 Go 是一种 gc 语言。这是设计原则，不会改变。 作为 gc 语言，Go 程序的性能通常取决于它们与 gc 的交互。 除了选择算法之外，内存消耗是决定应用程序性能和可伸缩性的最重要因素。 本节讨论垃圾收集器的操作，如何测量程序的内存使用情况以及在垃圾收集器性能成为瓶颈的情况下降低内存使用量的策略。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.1. gc 的世界观 任何垃圾收集器的目的都是为了给程序一种幻想，即存在无限数量的可用内存。 您可能不同意此声明，但这是垃圾收集器设计者如何工作的基本假设。 令人震惊的是，就总运行时间而言，标记扫描 GC 是最有效的；很好，适用于批处理，模拟等。但是，随着时间的流逝，Go GC 已从纯粹的停止世界收集器转变为并发的非压缩收集器。这是因为 Go GC 专为低延迟服务器和交互式应用程序而设计。 Go GC 的设计倾向于在 最大吞吐量 上 降低延迟。它将一些分配成本移到了 mutator 上，以减少以后的清理成本。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.2. 垃圾收集器设计 多年来，Go GC 的设计发生了变化 Go 1.0, 停止大量基于 tcmalloc 的世界标记清除收集器。 Go 1.3, 完全精确的收集器，不会将堆上的大数字误认为是指针，从而不会浪费内存。 Go 1.5, 新的 GC 设计，着重于 吞吐量 延迟。 Go 1.6, GC 的改进，以较低的延迟处理较大的堆。 Go 1.7, 较小的 GC 改进，主要是重构。 Go 1.8, 进一步工作以减少 STW 时间，现在已降至 100 微秒范围。 Go 1.10+, 摆脱纯粹的合作 Goroutine 调度 以降低触发整个GC周期时的延迟。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.3. 垃圾收集器监控 一种获得垃圾收集器工作量的总体思路的简单方法是启用 GC 日志记录的输出。 这些统计信息始终会收集，但通常会被禁止显示，您可以通过设置环境变量 GODEBUG 启用它们。 % env GODEBUG=gctrace=1 godoc -http=:8080 gc 1 @0.012s 2%: 0.026+0.39+0.10 ms clock, 0.21+0.88/0.52/0+0.84 ms cpu, 4-\u003e4-\u003e0 MB, 5 MB goal, 8 P gc 2 @0.016s 3%: 0.038+0.41+0.042 ms clock, 0.30+1.2/0.59/0+0.33 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 3 @0.020s 4%: 0.054+0.56+0.054 ms clock, 0.43+1.0/0.59/0+0.43 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 4 @0.025s 4%: 0.043+0.52+0.058 ms clock, 0.34+1.3/0.64/0+0.46 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 5 @0.029s 5%: 0.058+0.64+0.053 ms clock, 0.46+1.3/0.89/0+0.42 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 6 @0.034s 5%: 0.062+0.42+0.050 ms clock, 0.50+1.2/0.63/0+0.40 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 7 @0.038s 6%: 0.057+0.47+0.046 ms clock, 0.46+1.2/0.67/0+0.37 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 8 @0.041s 6%: 0.049+0.42+0.057 ms clock, 0.39+1.1/0.57/0+0.46 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P gc 9 @0.045s 6%: 0.047+0.38+0.042 ms clock, 0.37+0.94/0.61/0+0.33 ms cpu, 4-\u003e4-\u003e1 MB, 5 MB goal, 8 P 跟踪输出给出了GC活性的一般度量。the runtime package documentation 中描述了 gctrace=1 的输出格式。 DEMO: 显示启用了 GODEBUG=gctrace=1 的 godoc 在生产环境中使用此环境变量，不会对性能产生影响。 当您知道有问题时，使用 GODEBUG=gctrace=1 很好，但是对于Go应用程序上的常规遥测，我建议使用 net/http/pprof 接口。 import _ \"net/http/pprof\" 导入 net/http/pprof 软件包将在 /debug/pprof 注册一个具有各种运行时指标的处理程序，包括： A list of all the running goroutines, /debug/pprof/heap?debug=1. A report on the memory allocation statistics, /debug/pprof/heap?debug=1. net/http/pprof 将使用默认的 http.ServeMux 注册自己。 请小心，因为如果您使用 http.ListenAndServe(address, nil)，这将是可见的。 DEMO: godoc -http=:8080, 会显示 /debug/pprof。 6.3.1. 垃圾收集器调整 Go 运行时提供了一个用于调整 GC 的环境变量 GOGC。 GOGC 的公式是 $$ goal = reachabl\\e * (1 + (GOGC)/100) $$ 例如，如果我们当前有一个256MB的堆，并且 GOGC=100 (默认值)，当堆填满时，它将增长到 $$ 512MB = 256MB * (1 + 100/100) $$ GOGC 的值大于100会使堆增长更快，从而减轻了 GC 的压力。 小于 100 的 GOGC 值会导致堆缓慢增长，从而增加了 GC 的压力。 默认值 100 仅作为参考。 在使用生产负载对应用程序进行性能分析 之后，您应该选择自己的值。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.4. 减少分配 确保您的 API 允许调用方减少生成的垃圾量。 考虑这两种读取方法 func (r *Reader) Read() ([]byte, error) func (r *Reader) Read(buf []byte) (int, error) 第一个 Read 方法不带任何参数，并以 []byte 的形式返回一些数据。第二个接收一个 []byte 缓冲区，并返回读取的字节数。 第一个 Read 方法将 始终 分配缓冲区，从而给GC带来压力。第二个填充它给定的缓冲区。 您可以在标准库中命名遵循此模式的示例吗？ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.5. strings 和 []bytes 在 Go 中，string 的值是不可变的，[]byte 是可变的。 大多数程序都喜欢使用 string，但是大多数 IO 是使用 []byte 来完成的。 尽可能避免将 []byte 转换为字符串，这通常意味着选择一种表示形式，即 string 或 []byte 作为值。如果您从网络或磁盘读取数据，通常为 []byte。 bytes 包包含许多与 strings 软件包相同的操作 - Split, Compare, HasPrefix，Trim 等。 在底层，strings 与 bytes包使用相同的汇编原语。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.6. 使用 []byte 作为 map 的 key 使用 string 作为映射键是很常见的，但是通常您会使用 []byte。 编译器针对这种情况实现了特定的优化 var m map[string]string v, ok := m[string(bytes)] 这将避免将字节切片转换为用于映射查找的字符串。这是非常具体的操作，如果您执行以下操作将无法正常工作 key := string(bytes) val, ok := m[key] 让我们看看这是否仍然正确。编写一个基准，比较使用 []byte 作为 string 映射键的这两种方法。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:6","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.7. 避免字符串连接 转到字符串是不可变的。连接两个字符串会生成第三个字符串。 以下哪项是最快的？ s := request.ID s += \" \" + client.Addr().String() s += \" \" + time.Now().String() r = s var b bytes.Buffer fmt.Fprintf(\u0026b, \"%s %v %v\", request.ID, client.Addr(), time.Now()) r = b.String() r = fmt.Sprintf(\"%s %v %v\", request.ID, client.Addr(), time.Now()) b := make([]byte, 0, 40) b = append(b, request.ID...) b = append(b, ' ') b = append(b, client.Addr().String()...) b = append(b, ' ') b = time.Now().AppendFormat(b, \"2006-01-02 15:04:05.999999999 -0700 MST\") r = string(b) var b strings.Builder b.WriteString(request.ID) b.WriteString(\" \") b.WriteString(client.Addr().String()) b.WriteString(\" \") b.WriteString(time.Now().String()) r = b.String() DEMO: go test -bench=. ./examples/concat ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:7","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.8. 如果长度已知，则预分配片 追加很方便，但是很浪费。 切片通过将多达 1024 个元素加倍而增长，然后增加约 25％。我们再追加一项后，b 的容量是多少？ func main() { b := make([]int, 1024) b = append(b, 99) fmt.Println(\"len:\", len(b), \"cap:\", cap(b)) } 如果使用 append 模式，则可能会复制大量数据并创建大量垃圾。 如果知道事先知道切片的长度，则可以预先分配目标，以避免复制并确保目标大小正确。 Before var s []string for _, v := range fn() { s = append(s, v) } return s After vals := fn() s := make([]string, len(vals)) for i, v := range vals { s[i] = v } return s ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:8","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.9. 使用 sync.Pool sync 软件包带有 sync.Pool 类型，用于重用公共对象。 sync.Pool 没有固定大小或最大容量。您添加到它并从中取出直到发生 GC，然后将其无条件清空。这是 by design: 如果在垃圾回收之前为时过早而在垃圾回收之后为时过晚，则排空池的正确时间必须在垃圾回收期间。也就是说，池类型的语义必须是它在每个垃圾回收时都消耗掉。— Russ Cox 使用 sync.Pool var pool = sync.Pool{ New: func() interface{} { return make([]byte, 4096) }, } func fn() { buf := pool.Get().([]byte) // takes from pool or calls New // do work pool.Put(buf) // returns buf to the pool } sync.Pool 不是缓存。它可以并且将在任何时间清空。 不要将重要项目放在 sync.Pool 中，它们将被丢弃。 Go 1.13 中可能会更改在每个 GC 上清空的 sync.Pool 的设计，这将有助于提高其实用性。 此 CL 通过引入受害者缓存机制来解决此问题。代替清除池，将删除受害缓存，并将主缓存移至受害缓存。 结果，在稳定状态下，（几乎）没有新分配，但是如果 Pool 使用率下降，对象仍将在两个 GC（而不是一个）中收集。— Austin Clements https://go-review.googlesource.com/c/go/+/166961/ ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:9","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"6.10. 练习 使用 godoc（或其他程序）观察使用 GODEBUG=gctrace=1 改变 GOGC 的结果。 使用 bytes, string 作为 map key 并检查基准。 不同字符串连接策略的基准分配。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:8:10","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7. 提示和旅行 随机获取提示和建议 最后一部分包含一些微优化 Go 代码的技巧。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.1. Goroutines Goroutines 是使其非常适合现代硬件的关键功能。 Goroutines 易于使用，而且创建成本低廉，您可以将它们视为 几乎 免费。 Go 运行时已针对具有成千上万个 goroutine 作为标准的程序而编写，数十万个并不意外。 但是，每个 goroutine 确实消耗了 goroutine 堆栈的最小内存量，目前至少为2k。 2048 * 1,000,000 个 goroutines == 2GB 的内存，他们还没有做任何事情。 可能很多，但未提供应用程序的其他用法。 7.1.1. 知道何时停止 goroutine Goroutine 的启动方便，运行也很方便，但是在内存占用方面确实有一定的成本。您不能创建无限数量的它们。 每次您在程序中使用go关键字启动 goroutine 时，您都必须 知道 该 goroutine 如何以及何时退出。 在您的设计中，某些 goroutine 可能会运行直到程序退出。这些 goroutine 非常罕见，不会成为规则的例外。 如果您不知道答案，那将是潜在的内存泄漏，因为 goroutine 会将其堆栈的内存以及可从堆栈访问的所有堆分配变量固定在堆栈上。 切勿在不知道如何停止 goroutine 的情况下启动它。 7.1.2. 进一步阅读 Concurrency Made Easy (video) Concurrency Made Easy (slides) Never start a goroutine without knowning when it will stop (Practical Go, QCon Shanghai 2018) ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:1","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.2. Go 对某些请求使用有效的网络轮询 Go 运行时使用有效的操作系统轮询机制（kqueue，epoll，windows IOCP等）处理网络IO。一个单一的操作系统线程将为许多等待的 goroutine 提供服务。 但是，对于本地文件 IO，Go 不会实现任何 IO 轮询。 *os.File 上的每个操作在进行中都会消耗一个操作系统线程。 大量使用本地文件 IO 可能会导致您的程序产生数百或数千个线程。可能超出您的操作系统所允许的范围。 您的磁盘子系统不希望能够处理成百上千的并发 IO 请求。 要限制并发阻塞 IO 的数量，请使用工作程序 goroutine 池或缓冲通道作为信号灯。 var semaphore = make(chan struct{}, 10) func processRequest(work *Work) { semaphore \u003c- struct{}{} // acquire semaphore // process request \u003c-semaphore // release semaphore } ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:2","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.3. 注意您的应用程序中的 IO multipliers 如果您正在编写服务器进程，那么它的主要工作是多路复用通过网络连接的客户端和存储在应用程序中的数据。 大多数服务器程序接受请求，进行一些处理，然后返回结果。这听起来很简单，但是根据结果，它可能会使客户端消耗服务器上大量（可能是无限制的）资源。 这里有一些注意事项: 每个传入请求的 IO 请求数量；单个客户端请求生成多少个IO事件？它可能平均为 1，或者如果从缓存中提供了许多请求，则可能小于一个。 服务查询所需的读取量；它是固定的，N + 1还是线性的（读取整个表以生成结果的最后一页）。 相对而言，如果内存很慢，那么IO太慢了，您应该不惜一切代价避免这样做。最重要的是，避免在请求的上下文中进行IO，不要让用户等待您的磁盘子系统写入磁盘甚至读取磁盘。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:3","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.4. 使用流式 IO 接口 尽可能避免将数据读取到 []byte 中并将其传递。 根据请求，您可能最终将兆字节（或更多！）的数据读取到内存中。这给 GC 带来了巨大压力，这将增加应用程序的平均延迟。 而是使用 io.Reader 和 io.Writer 来构造处理管道，以限制每个请求使用的内存量。 为了提高效率，如果您使用大量的 io.Copy，请考虑实现 io.ReaderFrom/io.WriterTo。 这些接口效率更高，并且避免将内存复制到临时缓冲区中。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:4","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.5. 超时, 超时, 超时 在不知道所需的最长时间之前，切勿启动 IO 操作。 您需要使用 SetDeadline, SetReadDeadline, SetWriteDeadline 对每个网络请求设置超时。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:5","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.6. defer 消耗很大，还是？ defer 消耗很高，因为它必须记录下 defer 的论点。 defer mu.Unlock() 相当于 defer func() { mu.Unlock() }() 如果完成的工作量很小，则 defer 会很昂贵，经典的例子是 defer 围绕结构变量或映射查找进行互斥解锁。在这种情况下，您可以选择避免 defer。 在这种情况下，为了获得性能而牺牲了可读性和维护性。 始终重新审视这些决定。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:6","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.7. 避免 Finalisers Finalisation 是一种将行为附加到即将被垃圾回收的对象的技术。 因此，最终确定是不确定的。 要运行 finalizer，该对象不得通过任何物体到达。 如果您不小心在地图上保留了对该对象的引用，则该对象不会被最终确定。 finalizer 是 gc 周期的一部分，这意味着它们何时运行将是不可预测的，并且与减少 gc 操作的目标相矛盾。 如果堆很大并且已调整应用程序以创建最少的垃圾，则 finalizer 可能不会运行很长时间。 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:7","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.8. 减少 cgo cgo 允许 Go 程序调用 C 库。 C 代码和 Go 代码生活在两个不同的世界中，cgo 穿越了它们之间的边界。 这种转换不是免费的，并且取决于它在代码中的位置，其成本可能很高。 cgo 调用类似于阻塞 IO，它们在操作期间消耗线程。 不要在紧密循环中调用 C 代码。 7.8.1. 其实，也许避免 cgo cgo 的开销很高。 为了获得最佳性能，我建议您在应用程序中避免使用 cgo。 如果C代码花费很长时间，则 cgo 开销并不重要。 如果您使用 cgo 调用非常短的 C 函数（其开销最明显），请在 Go 中重写该代码 - 根据定义，这很短。 如果您在紧密的循环中使用了大量昂贵的 C 代码，那么为什么要使用 Go？ 是否有人使用 cgo 频繁调用昂贵的 C 代码？ 进一步阅读 cgo is not Go ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:8","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.9. 始终使用最新发布的Go版本 旧版本的 Go 永远不会变得更好。他们将永远不会得到错误修复或优化。 Go 1.4 不应该使用。 Go 1.5 和 1.6 编译器速度较慢，但生成的代码更快，GC更快。 Go 1.7 与 1.6 相比，编译速度提高了约 30％，链接速度提高了 2 倍（比任何以前的Go版本都要好）。 Go 1.8 （此时）将提供较小的编译速度改进，但非英特尔架构的代码质量将得到显着改进。 Go 1.9-1.12 继续提高所生成代码的性能，修复错误，并改善内联和改进调试。 旧版本的Go没有更新。请勿使用。使用最新版本，您将获得最佳性能。 7.9.1. 进一步阅读 Go 1.7 toolchain improvements Go 1.8 performance improvements 7.9.2. 将热点字段移动到 struct 顶部 ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:9","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"7.10. 讨论 Any questions? ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:9:10","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"8. 最后的问题和结论 可读意味着可靠 — Rob Pike 从最简单的代码开始。 测量。 分析您的代码以识别瓶颈，请不要猜测。 如果性能良好，请 停止。您无需优化所有内容，只需优化代码中最热的部分。 随着应用程序的增长或流量模式的发展，性能热点将发生变化。 不要留下对性能不重要的复杂代码，如果瓶颈转移到其他地方，请使用更简单的操作将其重写。 始终编写最简单的代码，编译器针对 正规 代码进行了优化。 较短的代码是较快的代码； Go 不是 C++，不要指望编译器能够解开复杂的抽象。 代码越短，代码 越小；这对于CPU的缓存很重要。 非常注意分配，尽可能避免不必要的分配。 如果事情不一定正确，我可以将事情做得很快。— Russ Cox 性能和可靠性同样重要。 我认为制作一个非常快速的服务器但是却定期 panics，死锁或 OOM 毫无价值。 不要为了可靠性而牺牲性能。 1. Hennessy et al: 40 年的年度绩效提高了 1.4 倍。 ref https://github.com/davecheney/gophercon2018-performance-tuning-workshop https://dave.cheney.net/high-performance-go-workshop/dotgo-paris.html#avoid_finalisers https://github.com/davecheney/high-performance-go-workshop ","date":"2020-09-27","objectID":"/high-performance-go-workshop/:10:0","tags":["go","profile","翻译"],"title":"(翻译)Go 高性能研讨讲座 - High Performance Go Workshop","uri":"/high-performance-go-workshop/"},{"categories":null,"content":"文章简介：一种基于浏览器渲染的生成报告技术方案 需求是无外网环境下，汇总当前系统的运行状态，需要绘制大量图表到 word/pdf。 实现语言: golang ","date":"2020-09-21","objectID":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/:0:0","tags":null,"title":"一种基于浏览器渲染的生成报告技术方案","uri":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/"},{"categories":null,"content":"如何更加方便的管理报告模版 主要思路： 报告模版使用 markdown+golang/text/template 渲染，图表使用 datauri 直接内嵌入 markdown。markdown 经过 ast 解析后翻译成对应的元素，渲染成 report.Report 中间接口。report.Report 接口作为中间层，下边实现 pdf/word 渲染实现。 最终达成的目标：样式由 report.Report 的实现决定，上层 markdown 关注内容。 github.com/yuin/goldmark ","date":"2020-09-21","objectID":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/:1:0","tags":null,"title":"一种基于浏览器渲染的生成报告技术方案","uri":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/"},{"categories":null,"content":"报表中大量图表如何渲染 使用前端技术绘制图表，可在浏览器中直接获得结果 渲染服务：控制浏览器打开网页，并指定截图某区域. 有一个关键设计: chromedp 控制浏览器，会没有基础的 post请求方式，需要通过控制浏览器执行 js, 对网页进行控制（创建 dom ，并画出图） 渲染服务无状态，需要管理 chrome headless。对外提供 openapi golang https://github.com/browserless/chrome/issues/52 github.com/chromedp/chromedp ","date":"2020-09-21","objectID":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/:2:0","tags":null,"title":"一种基于浏览器渲染的生成报告技术方案","uri":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/"},{"categories":null,"content":"如何保存最终生成的报告 markdown 直接存储到数据库，优点不需要做大量小文件优化 ","date":"2020-09-21","objectID":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/:3:0","tags":null,"title":"一种基于浏览器渲染的生成报告技术方案","uri":"/a-technical-solution-for-generating-reports-based-on-browser-rendering/"},{"categories":null,"content":"runtime: fatal error: SIGSEGV during C.getaddrinfo 解决方法 ","date":"2020-06-27","objectID":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/:0:0","tags":null,"title":"golang runtime: fatal error: SIGSEGV during C.getaddrinfo","uri":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/"},{"categories":null,"content":"现象 golang runtime: fatal error: SIGSEGV during C.getaddrinfo fatal error: unexpected signal during runtime execution [signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7fd8b41641b9] runtime stack: ... created by net.cgoLookupIP ... /etc/resolv.conf 包含参数 options edns0 ","date":"2020-06-27","objectID":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/:1:0","tags":null,"title":"golang runtime: fatal error: SIGSEGV during C.getaddrinfo","uri":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/"},{"categories":null,"content":"原因 https://github.com/golang/go/issues/30310 glibc 的 bug 导致 go 中 net 模块静态编译后的可执行文件 getaddrinfo 的时候 pinic，无法启动服务 ","date":"2020-06-27","objectID":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/:2:0","tags":null,"title":"golang runtime: fatal error: SIGSEGV during C.getaddrinfo","uri":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/"},{"categories":null,"content":"解决办法 方法1. build 代码的时候，使用 go 自己的 net 实现 -tags netgo: go build -tags netgo main.go 方法2. 已经 build 的程序使用环境变量 GODEBUG=netdns=go 运行程序，临时使用 go实现的 net: GODEBUG=netdns=go main ","date":"2020-06-27","objectID":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/:3:0","tags":null,"title":"golang runtime: fatal error: SIGSEGV during C.getaddrinfo","uri":"/go-runtime-fatal-sigsegv-during-c.getaddrinfo/"},{"categories":null,"content":"文章简介：golang 各种元素整理 ","date":"2020-05-06","objectID":"/go-compiler-overview/:0:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"编译原理 比较好的介绍 golang 的编译器看 这里 代码中对 go compiler 的描述: src/cmd/compile/README.md what is ssa: wiki IR golang Static Single Assignment: src/cmd/compile/internal/ssa/README.md ","date":"2020-05-06","objectID":"/go-compiler-overview/:1:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"编译器入口 src/cmd/compile/internal/gc/main.go ","date":"2020-05-06","objectID":"/go-compiler-overview/:1:1","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"技巧 生成 SSA 整个过程结果展示(html): GOSSAFUNC=hello go build hello/hello.go 生成 plan9 汇编代码 GOOS=linux GOARCH=amd64 go tool compile -S hello/hello.go TODO: 给一个例子，并解释一下(需要了解所有步骤) ","date":"2020-05-06","objectID":"/go-compiler-overview/:1:2","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"plan9 ","date":"2020-05-06","objectID":"/go-compiler-overview/:2:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"基本数据结构 ","date":"2020-05-06","objectID":"/go-compiler-overview/:3:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"数组 https://github.com/golang/go/blob/f07059d949057f414dd0f8303f93ca727d716c62/src/cmd/compile/internal/gc/sinit.go#L875-L967 当元素数量小于或者等于 4 个时，会直接将数组中的元素放置在栈上； 当元素数量大于 4 个时，会将数组中的元素放置到静态区并在运行时取出； 无论是在栈上还是静态存储区，数组在内存中其实就是一连串的内存空间，表示数组的方法就是一个指向数组开头的指针、数组中元素的数量以及数组中元素类型占的空间大小 Go 语言中对数组越界的判断是可以在编译期间由静态类型检查完成的 https://github.com/golang/go/blob/b7d097a4cf6b8a9125e4770b54d33826fa803023/src/cmd/compile/internal/gc/typecheck.go#L327-L2081 ","date":"2020-05-06","objectID":"/go-compiler-overview/:3:1","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"切片 图解 slice NewSlice // [slice](https://github.com/golang/go/blob/4c003f6b780b471afbf032438eb6c7519458855b/src/reflect/value.go#L1973) type SliceHeader struct { Data uintptr Len int Cap int } slice := []int{1, 2, 3} slice := make([]int, 10) [:], a slice referencing the storage of x [:] 操作是创建切片最底层的一种方法 make make 关键词 当切片发生逃逸或者非常大时，我们需要 runtime.makeslice 函数在堆上初始化， 如果当前的切片不会发生逃逸并且切片非常小的时候，make([]int, 3, 4) 会被直接转换成如下所示的代码： var arr [4]int n := arr[:3] golang 对 slice 比较有意义的优化 append runtime.growslice 如果期望容量大于当前容量的两倍就会使用期望容量； 如果当前切片容量小于 1024 就会将容量翻倍； 如果当前切片容量大于 1024 就会每次增加 25% 的容量，直到新容量大于期望容量； ","date":"2020-05-06","objectID":"/go-compiler-overview/:3:2","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"哈希表 哈希函数 冲突解决方法 开放寻址法和拉链法 [hmap](https://github.com/golang/go/blob/ed15e82413c7b16e21a493f5a647f68b46e965ee/src/runtime/map.go#L115-L129) type hmap struct { count int flags uint8 B uint8 noverflow uint16 hash0 uint32 buckets unsafe.Pointer oldbuckets unsafe.Pointer nevacuate uintptr extra *mapextra } 创建 map runtime.makemap golang 通过拉链法处理冲突 读写 增加、删除和修改 mapaccess1 扩容（不是原子操作） runtime.mapdelete Go 语言使用拉链法来解决哈希碰撞的问题实现了哈希表，它的访问、写入和删除等操作都在编译期间转换成了运行时的函数或者方法。 哈希在每一个桶中存储键对应哈希的前 8 位，当对哈希进行操作时，这些 tophash 就成为了一级缓存帮助哈希快速遍历桶中元素，每一个桶都只能存储 8 个键值对，一旦当前哈希的某个桶超出 8 个，新的键值对就会被存储到哈希的溢出桶中。 随着键值对数量的增加，溢出桶的数量和哈希的装载因子也会逐渐升高，超过一定范围就会触发扩容，扩容会将桶的数量翻倍，元素再分配的过程也是在调用写操作时增量进行的，不会造成性能的瞬时巨大抖动。 ","date":"2020-05-06","objectID":"/go-compiler-overview/:3:3","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"字符串 十分钟搞清字符集和字符编码 type StringHeader struct { Data uintptr Len int } type stringStruct struct { str unsafe.Pointer len int } slicebytetostring ","date":"2020-05-06","objectID":"/go-compiler-overview/:3:4","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"OO ","date":"2020-05-06","objectID":"/go-compiler-overview/:4:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"函数调用 C 语言和 Go 语言在设计函数的调用惯例时选择也不同的实现。C 语言同时使用寄存器和栈传递参数，使用 eax 寄存器传递返回值；而 Go 语言使用栈传递参数和返回值。我们可以对比一下这两种设计的优点和缺点： C 语言的方式能够减少大量小函数调用的开销，但是也增加了实现的复杂度； CPU 访问栈(内存)的开销比访问寄存器高几十倍； 需要单独处理函数参数过多的情况； Go 语言的方式能够降低实现的复杂度并支持多返回值，但是牺牲了函数调用的性能； 不需要考虑超过寄存器数量的参数应该如何传递； 不需要考虑不同架构上的寄存器差异； 函数入参和出参的内存空间需要在栈上进行分配； Go 语言使用栈作为参数和返回值传递的方法是综合考虑后的设计，选择这种设计意味着编译器会更加简单、更容易维护。 参数传递 golang 无论是传递基本类型、结构体还是指针，都会对传递的参数进行拷贝 ","date":"2020-05-06","objectID":"/go-compiler-overview/:4:1","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"接口 类型转换、类型断言以及动态派发机制 iface 结构体 是带有一组方法的接口 eface 结构体 不带任何方法的 interface{} type eface struct { // 16 bytes _type *_type data unsafe.Pointer } type iface struct { // 16 bytes tab *itab data unsafe.Pointer } 动态派发的过程只是放大了参数拷贝带来的影响,\u0008\u0008 用结构体实现接口会有更多消耗(125%) ","date":"2020-05-06","objectID":"/go-compiler-overview/:4:2","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"反射 ","date":"2020-05-06","objectID":"/go-compiler-overview/:4:3","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"常用关键字 ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"for 和 range 一些 golang buildin func 是由原始的汇编写成，比如runtime·memclrNoHeapPointers ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:1","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"select 当 select 中的两个 case 同时被触发时，就会随机选择一个 case 执行。 select 能在 Channel 上进行非阻塞的收发操作； select 在遇到多个 Channel 同时响应时会随机挑选 case 执行；(如果我们按照顺序依次判断，那么后面的条件永远都会得不到执行，而随机的引入就是为了避免饥饿问题的发生) 非阻塞的收发: select { case err := \u003c-errCh: return err default: return nil } scase type scase struct { c *hchan // chan elem unsafe.Pointer // data element kind uint16 pc uintptr // race pc (for race detector / msan) releasetime int64 } ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:2","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"defer ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:3","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"panic 和 recover Defer, Panic, and Recover panic 只会触发当前 Goroutine 的延迟函数调用； recover 只有在 defer 函数中调用才会生效； panic 允许在 defer 中嵌套多次调用； 跨协程失效 首先要展示的例子就是 panic 只会触发当前 Goroutine 的延迟函数调用 多个 Goroutine 之间没有太多的关联 runtime._panic ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:4","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"总结 编译器会负责做转换关键字的工作； 将 panic 和 recover 分别转换成 runtime.gopanic 和 runtime.gorecover； 将 defer 转换成 deferproc 函数； 在调用 defer 的函数末尾调用 deferreturn 函数； 在运行过程中遇到 gopanic 方法时，会从 Goroutine 的链表依次取出 _defer 结构体并执行； 如果调用延迟执行函数时遇到了 gorecover 就会将 _panic.recovered 标记成 true 并返回 panic 的参数； 在这次调用结束之后，gopanic 会从 _defer 结构体中取出程序计数器 pc 和栈指针 sp 并调用 recovery 函数进行恢复程序； recovery 会根据传入的 pc 和 sp 跳转回 deferproc； 编译器自动生成的代码会发现 deferproc 的返回值不为 0，这时会跳回 deferreturn 并恢复到正常的执行流程； 如果没有遇到 gorecover 就会依次遍历所有的 _defer 结构，并在最后调用 fatalpanic 中止程序、打印 panic 的参数并返回错误码 2； ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:5","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"make 和 new make 关键字的作用是创建切片、哈希表和 Channel 等内置的数据结构，而 new 的作用是为类型申请一片内存空间，并返回指向这片内存的指针。 ","date":"2020-05-06","objectID":"/go-compiler-overview/:5:6","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"运行时 ","date":"2020-05-06","objectID":"/go-compiler-overview/:6:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"并发 上下文 Context Deadline — 返回 context.Context 被取消的时间，也就是完成工作的截止日期； Done — 返回一个 Channel，这个 Channel 会在当前工作完成或者上下文被取消之后关闭，多次调用 Done 方法会返回同一个 Channel； Err — 返回 context.Context 结束的原因，它只会在 Done 返回的 Channel 被关闭时才会返回非空的值； 如果 context.Context 被取消，会返回 Canceled 错误； 如果 context.Context 超时，会返回 DeadlineExceeded 错误； Value — 从 context.Context 中获取键对应的值，对于同一个上下文来说，多次调用 Value 并传入相同的 Key 会返回相同的结果，该方法可以用来传递请求特定的数据； Go Concurrency Patterns: Context 同步原语与锁 Mutex、RWMutes、WaitGroup、Once x/sync/errgroup.Group、x/sync/semaphore.Weighted、x/sync/singleflight.Group 和 x/sync/syncmap.Map sync.Mutex type Mutex struct { state int32 sema uint32 } 正常模式 =\u003e 在正常模式下，锁的等待者会按照先进先出的顺序获取锁。但是刚被唤起的 Goroutine 与新创建的 Goroutine 竞争时，大概率会获取不到锁，为了减少这种情况的出现，一旦 Goroutine 超过 1ms 没有获取到锁，它就会将当前互斥锁切换饥饿模式，防止部分 Goroutine 被『饿死』。 饥饿模式 =\u003e 在饥饿模式中，互斥锁会直接交给等待队列最前面的 Goroutine。新的 Goroutine 在该状态下不能获取锁、也不会进入自旋状态，它们只会在队列的末尾等待。如果一个 Goroutine 获得了互斥锁并且它在队列的末尾或者它等待的时间少于 1ms，那么当前的互斥锁就会被切换回正常模式。 相比于饥饿模式，正常模式下的互斥锁能够提供更好地性能，饥饿模式的能避免 Goroutine 由于陷入等待无法获取锁而造成的高尾延时。 Go 语言还在子仓库 sync 中提供了四种扩展原语，x/sync/errgroup.Group、x/sync/semaphore.Weighted、x/sync/singleflight.Group 和 x/sync/syncmap.Map，其中的 x/sync/syncmap.Map 在 1.9 版本中被移植到了标准库中。 定时器 而在 10ms 的这个粒度下，作者在社区中也没有找到能够使用的计时器实现，一些使用时间轮算法的开源库也不能很好地完成这个任务。 Channel 创建、发送、接收和关闭 不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存 CSP 无锁（lock-free）队列更准确的描述是使用乐观并发控制的队列 runtime.hchan impl lock free 论文 社区 lock free chan send runtime.chansend1 当存在等待的接收者时，通过 runtime.send 直接将数据发送给阻塞的接收者； 当缓冲区存在空余空间时，将发送的数据写入 Channel 的缓冲区； 当不存在缓冲区或者缓冲区已满时，等待其他 Goroutine 从 Channel 接收数据； recv 如果 Channel 为空，那么就会直接调用 runtime.gopark 挂起当前 Goroutine； 如果 Channel 已经关闭并且缓冲区没有任何数据，runtime.chanrecv 函数会直接返回； 如果 Channel 的 sendq 队列中存在挂起的 Goroutine，就会将 recvx 索引所在的数据拷贝到接收变量所在的内存空间上并将 sendq 队列中 Goroutine 的数据拷贝到缓冲区； 如果 Channel 的缓冲区中包含数据就会直接读取 recvx 索引对应的数据； 在默认情况下会挂起当前的 Goroutine，将 runtime.sudog 结构加入 recvq 队列并陷入休眠等待调度器的唤醒； 调度器 每一次线程上下文的切换都需要消耗 ~1us 左右的时间：Measuring context switching and memory overheads for Linux threads Go 调度器对 Goroutine 的上下文切换约为 ~0.2us，减少了 80% 的额外开销 网络轮询器 系统监控 ","date":"2020-05-06","objectID":"/go-compiler-overview/:6:1","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"内存管理 内存分配器 TCMalloc 隔离适应 垃圾收集器 标记清除 三色抽象 从灰色对象的集合中选择一个灰色对象并将其标记成黑色； 将黑色对象指向的所有对象都标记成灰色，保证该对象和被该对象引用的对象都不会被回收； 重复上述两个步骤直到对象图中不存在灰色对象； 屏障技术 栈内存管理 ","date":"2020-05-06","objectID":"/go-compiler-overview/:6:2","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"其他 ","date":"2020-05-06","objectID":"/go-compiler-overview/:7:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"unsafe ","date":"2020-05-06","objectID":"/go-compiler-overview/:7:1","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"逃逸分析 ","date":"2020-05-06","objectID":"/go-compiler-overview/:7:2","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"build comment //go:nosplit //go:linkname //go:noescape //go:notinheap *(*int)(nil) = 0 // not reached ","date":"2020-05-06","objectID":"/go-compiler-overview/:8:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"reference","date":"2020-05-06","objectID":"/go-compiler-overview/:9:0","tags":[],"title":"Go Overview","uri":"/go-compiler-overview/"},{"categories":null,"content":"文章简介：源码层面分析 Opentracing 一个实现 jaeger 的工作原理 ","date":"2020-04-24","objectID":"/opentracing-jaeger-agent-src-code-introduce/:0:0","tags":["opentracing","jaeger","源码分析","go"],"title":"Opentracing - jaeger 源码分析","uri":"/opentracing-jaeger-agent-src-code-introduce/"},{"categories":null,"content":"agent Agent 处于 jaeger-client 和 collector 之间，属于代理的作用，主要是把 client 发送过来的数据从 thrift 转为 Batch，并通过 RPC 批量提交到 collector jaegertracing/jaeger/cmd/agent/app/flags.go#L62 var defaultProcessors = []struct { model Model protocol Protocol port int }{ {model: \"zipkin\", protocol: \"compact\", port: 5775}, {model: \"jaeger\", protocol: \"compact\", port: 6831}, {model: \"jaeger\", protocol: \"binary\", port: 6832}, } jaegertracing/jaeger/cmd/agent/app/servers/tbuffered_server.go#L82 // Serve initiates the readers and starts serving traffic func (s *TBufferedServer) Serve() { atomic.StoreUint32(\u0026s.serving, 1) for s.IsServing() { readBuf := s.readBufPool.Get().(*ReadBuf) n, err := s.transport.Read(readBuf.bytes) if err == nil { readBuf.n = n s.metrics.PacketSize.Update(int64(n)) select { case s.dataChan \u003c- readBuf: s.metrics.PacketsProcessed.Inc(1) s.updateQueueSize(1) default: s.metrics.PacketsDropped.Inc(1) } } else { s.metrics.ReadError.Inc(1) } } } jaegertracing/jaeger/blob/master/cmd/agent/app/processors/thrift_processor.go#L114 // processBuffer reads data off the channel and puts it into a custom transport for // the processor to process func (s *ThriftProcessor) processBuffer() { for readBuf := range s.server.DataChan() { protocol := s.protocolPool.Get().(thrift.TProtocol) payload := readBuf.GetBytes() protocol.Transport().Write(payload) s.logger.Debug(\"Span(s) received by the agent\", zap.Int(\"bytes-received\", len(payload))) if ok, err := s.handler.Process(protocol, protocol); !ok { s.logger.Error(\"Processor failed\", zap.Error(err)) s.metrics.HandlerProcessError.Inc(1) } s.protocolPool.Put(protocol) s.server.DataRecd(readBuf) // acknowledge receipt and release the buffer } } jaegertracing/jaeger/thrift-gen/agent/agent.go#L187 func (p *agentProcessorEmitBatch) Process(seqId int32, iprot, oprot thrift.TProtocol) (success bool, err thrift.TException) { args := AgentEmitBatchArgs{} if err = args.Read(iprot); err != nil { iprot.ReadMessageEnd() return false, err } iprot.ReadMessageEnd() var err2 error if err2 = p.handler.EmitBatch(args.Batch); err2 != nil { return true, err2 } return true, nil } jaegertracing/jaeger/thrift-gen/jaeger/tchan-jaeger.go#L39 ","date":"2020-04-24","objectID":"/opentracing-jaeger-agent-src-code-introduce/:1:0","tags":["opentracing","jaeger","源码分析","go"],"title":"Opentracing - jaeger 源码分析","uri":"/opentracing-jaeger-agent-src-code-introduce/"},{"categories":null,"content":"Collector ","date":"2020-04-24","objectID":"/opentracing-jaeger-agent-src-code-introduce/:2:0","tags":["opentracing","jaeger","源码分析","go"],"title":"Opentracing - jaeger 源码分析","uri":"/opentracing-jaeger-agent-src-code-introduce/"},{"categories":null,"content":"接收 Agent 的数据 jaegertracing/jaeger/cmd/collector/app/handler/thrift_span_handler.go#L60 比较舒服的维护metrics的场景 ","date":"2020-04-24","objectID":"/opentracing-jaeger-agent-src-code-introduce/:2:1","tags":["opentracing","jaeger","源码分析","go"],"title":"Opentracing - jaeger 源码分析","uri":"/opentracing-jaeger-agent-src-code-introduce/"},{"categories":null,"content":"references jaeger ","date":"2020-04-24","objectID":"/opentracing-jaeger-agent-src-code-introduce/:3:0","tags":["opentracing","jaeger","源码分析","go"],"title":"Opentracing - jaeger 源码分析","uri":"/opentracing-jaeger-agent-src-code-introduce/"},{"categories":null,"content":"整理一下 ElasticSearch 知识点 ElasticSearch 是一个分布式搜索引擎，底层使用 Lucene 来实现其核心搜索功能.其核心是全文检索. ","date":"2020-04-16","objectID":"/how-elastic-works/:0:0","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"全文检索 倒排索引 + TF-IDF为全文搜索的基石。 ","date":"2020-04-16","objectID":"/how-elastic-works/:1:0","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"ElasticSearch 诞生的背景 ","date":"2020-04-16","objectID":"/how-elastic-works/:2:0","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"大规模数据如何检索 当系统数据量上了 10 亿、100 亿条的时候，我们在做系统架构的时候通常会从以下角度去考虑问题： 用什么数据库好？(mysql、postgres、sybase、oracle、达梦、神通、mongodb、hbase…) 如何解决单点故障；(lvs、F5、A10、Zookeep、MQ) 如何保证数据安全性；(热备、冷备、异地多活) 如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale 等;) 如何解决统计分析问题；(离线、近实时) ","date":"2020-04-16","objectID":"/how-elastic-works/:2:1","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"传统数据库的应对解决方案 对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈： 通过主从备份解决数据安全性问题； 通过数据库代理中间件心跳监测，解决单点故障问题； 通过代理中间件将查询语句分发到各个 slave 节点进行查询，并汇总结果 ","date":"2020-04-16","objectID":"/how-elastic-works/:2:2","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"非关系型数据库的解决方案 对于 Nosql 数据库，基本原理类似： 通过副本备份保证数据安全性； 通过节点竞选机制解决单点问题； 先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果 ","date":"2020-04-16","objectID":"/how-elastic-works/:2:3","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"Elastic 理论知识 ","date":"2020-04-16","objectID":"/how-elastic-works/:3:0","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"Elasticsearch vs mysql Mysql -\u003e database -\u003e table -\u003e rows -\u003e columns Elasticsearch -\u003e index -\u003e type -\u003e documents -\u003e fields elastic 在 7.x 之后将去掉 types, 替代方案：index per document type / custom type field ","date":"2020-04-16","objectID":"/how-elastic-works/:3:1","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"ES 的 CRUD 资料 索引新文档 当用户向一个节点提交了一个索引新文档的请求，节点会计算新文档应该加入到哪个分片（shard）中。每个节点都存储有每个分片存储在哪个节点的信息，因此协调节点会将请求发送给对应的节点。注意这个请求会发送给主分片，等主分片完成索引，会并行将请求发送到其所有副本分片，保证每个分片都持有最新数据。 每次写入新文档时，都会先写入内存中，并将这一操作写入一个 translog 文件（transaction log）中，此时如果执行搜索操作，这个新文档还不能被索引到。 ES 会每隔 1 秒时间（这个时间可以修改）进行一次刷新操作（refresh），此时在这 1 秒时间内写入内存的新文档都会被写入一个文件系统缓存（filesystem cache）中，并构成一个分段（segment）。此时这个 segment 里的文档可以被搜索到，但是尚未写入硬盘，即如果此时发生断电，则这些文档可能会丢失。 不断有新的文档写入，则这一过程将不断重复执行。每隔一秒将生成一个新的 segment，而 translog 文件将越来越大。每隔 30 分钟或者 translog 文件变得很大，则执行一次 fsync 操作。此时所有在文件系统缓存中的 segment 将被写入磁盘，而 translog 将被删除（此后会生成新的 translog）。 由上面的流程可以看出，在两次 fsync 操作之间，存储在内存和文件系统缓存中的文档是不安全的，一旦出现断电这些文档就会丢失。所以 ES 引入了 translog 来记录两次 fsync 之间所有的操作，这样机器从故障中恢复或者重新启动，ES 便可以根据 translog 进行还原。 此外，由于每一秒就会生成一个新的 segment，很快将会有大量的 segment。对于一个分片进行查询请求，将会轮流查询分片中的所有 segment，这将降低搜索的效率。因此 ES 会自动启动合并 segment 的工作，将一部分相似大小的 segment 合并成一个新的大 segment。合并的过程实际上是创建了一个新的 segment，当新 segment 被写入磁盘，所有被合并的旧 segment 被清除。 更新（Update）和删除（Delete）文档 ES 的索引是不能修改的，因此更新和删除操作并不是直接在原索引上直接执行。每一个磁盘上的 segment 都会维护一个 del 文件，用来记录被删除的文件。每当用户提出一个删除请求，文档并没有被真正删除，索引也没有发生改变，而是在 del 文件中标记该文档已被删除。因此，被删除的文档依然可以被检索到，只是在返回检索结果时被过滤掉了。每次在启动 segment 合并工作时，那些被标记为删除的文档才会被真正删除。 更新文档会首先查找原文档，得到该文档的版本号。然后将修改后的文档写入内存，此过程与写入一个新文档相同。同时，旧版本文档被标记为删除，同理，该文档可以被搜索到，只是最终被过滤掉。 读操作（Read）：查询过程 查询阶段 当一个节点接收到一个搜索请求，则这个节点就变成了协调节点。第一步是广播请求到索引中每一个节点的分片拷贝。 查询请求可以被某个主分片或某个副本分片处理，协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。 每个分片将会在本地构建一个优先级队列。如果客户端要求返回结果排序中从第 from 名开始的数量为 size 的结果集，则每个节点都需要生成一个 from+size 大小的结果集，因此优先级队列的大小也是 from+size。分片仅会返回一个轻量级的结果给协调节点，包含结果集中的每一个文档的 ID 和进行排序所需要的信息。 协调节点会将所有分片的结果汇总，并进行全局排序，得到最终的查询排序结果。此时查询阶段结束。 取回阶段 查询过程得到的是一个排序结果，标记出哪些文档是符合搜索要求的，此时仍然需要获取这些文档返回客户端。 协调节点会确定实际需要返回的文档，并向含有该文档的分片发送 get 请求；分片获取文档返回给协调节点；协调节点将结果返回给客户端。 ","date":"2020-04-16","objectID":"/how-elastic-works/:3:2","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"分布式一致性原理 资源，有源码分析 ","date":"2020-04-16","objectID":"/how-elastic-works/:4:0","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"ES 集群构成 node.master node.data 两两组合成不同的节点 ","date":"2020-04-16","objectID":"/how-elastic-works/:4:1","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"节点发现 ","date":"2020-04-16","objectID":"/how-elastic-works/:4:2","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"Master 选举 多数派原则 选主 ","date":"2020-04-16","objectID":"/how-elastic-works/:4:3","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"references elasticsearch cheatsheet Elasticsearch Reference Elasticsearch－基础介绍及索引原理分析 Choosing a fast unique identifier (UUID) for Lucene Elasticsearch架构原理 Elasticsearch性能优化 ElasticSearch读写底层原理及性能调优 ","date":"2020-04-16","objectID":"/how-elastic-works/:5:0","tags":["es"],"title":"ElasticSearch 是如何工作的","uri":"/how-elastic-works/"},{"categories":null,"content":"epoll 高性能 network io 模型 由于太多 blog 已经讲 epoll，再此引用看的blog，以及项目。并根据自己的理解，实现出一个 epoll 实现的 server。 TODO: 需要跟一步设计实现跨平台的序列化反序列化方案，进一步实现应用层需求。 ","date":"2020-04-06","objectID":"/linux_epoll/:0:0","tags":["epoll","linux"],"title":"Linux epoll","uri":"/linux_epoll/"},{"categories":null,"content":"epoll 高性能 异步io 模型，在 Linux 是 epoll，mac kqueue，windows IOCP. App: nginx, redis ","date":"2020-04-06","objectID":"/linux_epoll/:1:0","tags":["epoll","linux"],"title":"Linux epoll","uri":"/linux_epoll/"},{"categories":null,"content":"camel 使用 linux epoll 实现的 socket server. ","date":"2020-04-06","objectID":"/linux_epoll/:2:0","tags":["epoll","linux"],"title":"Linux epoll","uri":"/linux_epoll/"},{"categories":null,"content":"references EPOLL的LINUX内核工作机制 https://blog.lucode.net/linux/epoll-tutorial.html https://github.com/millken/c-example/blob/master/epoll-example.c https://www.suchprogramming.com/epoll-in-3-easy-steps/ https://medium.com/@copyconstruct/the-method-to-epolls-madness-d9d2d6378642 https://github.com/angrave/SystemProgramming/wiki/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll https://github.com/angrave/SystemProgramming/wiki https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll simple http server base on epoll https://github.com/cloudwu/cstring ","date":"2020-04-06","objectID":"/linux_epoll/:3:0","tags":["epoll","linux"],"title":"Linux epoll","uri":"/linux_epoll/"},{"categories":null,"content":"文章简介：Latency numbers every programmer should know L1 cache reference ......................... 0.5 ns Branch mispredict ............................ 5 ns L2 cache reference ........................... 7 ns Mutex lock/unlock ........................... 25 ns Main memory reference ...................... 100 ns Compress 1K bytes with Zippy ............. 3,000 ns = 3 µs Send 2K bytes over 1 Gbps network ....... 20,000 ns = 20 µs SSD random read ........................ 150,000 ns = 150 µs Read 1 MB sequentially from memory ..... 250,000 ns = 250 µs Round trip within same datacenter ...... 500,000 ns = 0.5 ms Read 1 MB sequentially from SSD* ..... 1,000,000 ns = 1 ms Disk seek ........................... 10,000,000 ns = 10 ms Read 1 MB sequentially from disk .... 20,000,000 ns = 20 ms Send packet CA-\u003eNetherlands-\u003eCA .... 150,000,000 ns = 150 ms src 2001年夏天典型的1GHz PC的各种操作要花的时间 执行一条指令 1 nsec = (1/1,000,000,000) sec 从L1 cache memory 中取一个字 2 nsec 从内存中取一个字 10 nsec 从磁盘的连续位置取一个字 200 nsec 从磁盘的新位置取一个字(seek) 8,000,000nsec = 8msec ","date":"2019-11-30","objectID":"/how-fast-the-read-write/:0:0","tags":["os","cs"],"title":"读写操作到底有多慢, 操作系统相关一些参数","uri":"/how-fast-the-read-write/"},{"categories":null,"content":"文章简介：自己的 dotfiles 供自己使用，几乎一键换机 简介 使用方法 git clone https://github.com/exfly/dotfiles.git ~/.dotfiles cd ~/.dotfiles make preinstall-arch make bootstrap make install ","date":"2019-09-28","objectID":"/exfly-dotfiles/:0:0","tags":["tools","dev","docker"],"title":"整理了一套 dotfiles 自用","uri":"/exfly-dotfiles/"},{"categories":null,"content":"备注 对于所有的 *.zsh 都会加载到 .zshrc 中，所有的 *.symlink 都会对应的在$HOME生成一个软连接 .*, 文件夹也可以直接使用这种方式进行加载。 比如 zsh/zshrc.symlink 会生成软连接 $HOME/.zshrc,具体如何工作可以看一下 script/bootstrap references my dotfiles ","date":"2019-09-28","objectID":"/exfly-dotfiles/:1:0","tags":["tools","dev","docker"],"title":"整理了一套 dotfiles 自用","uri":"/exfly-dotfiles/"},{"categories":null,"content":"文章简介：Docker Cgroup , Namespace , union fs 运行机制 Namespaces 命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法. Linux 的命名空间机制提供了以下七种不同的命名空间，包括 CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS(分别对应 Cgroup, IPC, Network, Mount, PID, User, UTS)，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。 the Network namespace encapsulates system resources related to networking such as network interfaces (e.g wlan0, eth0), route tables etc, the Mount namespace encapsulates files and directories in the system, PID contains process IDs and so on. So two instances of a Network namespace A and B (corresponding to two boxes of the same type in our analogy) can contain different resources - maybe A contains wlan0 while B contains eth0 and a different route table copy – related $ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 cgroup -\u003e 'cgroup:[4026531835]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 ipc -\u003e 'ipc:[4026531839]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 mnt -\u003e 'mnt:[4026531840]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 net -\u003e 'net:[4026531992]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 pid -\u003e 'pid:[4026531836]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 pid_for_children -\u003e 'pid:[4026531836]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 user -\u003e 'user:[4026531837]' lrwxrwxrwx 1 vagrant vagrant 0 Sep 19 00:46 uts -\u003e 'uts:[4026531838]' $ sudo unshare -u bash $ ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 root root 0 Sep 19 01:00 cgroup -\u003e 'cgroup:[4026531835]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 ipc -\u003e 'ipc:[4026531839]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 mnt -\u003e 'mnt:[4026531840]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 net -\u003e 'net:[4026531992]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 pid -\u003e 'pid:[4026531836]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 pid_for_children -\u003e 'pid:[4026531836]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 user -\u003e 'user:[4026531837]' lrwxrwxrwx 1 root root 0 Sep 19 01:00 uts -\u003e 'uts:[4026532239]' unshare 会在一个新的 namespace 执行新的程序，flag -u 指定新的 UTS namespace, sudo unshare -u bash即是在新的UTS命名空间执行 bash linux 以默认的 namespace 启动新的程序，除非明确的指定 ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:0:0","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"user namespace P$ Q$ 不同的字母代表不同的 user namespace P$ whoami vagrant P$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),977(docker) P$ unshare -U bash # Enter a new shell that runs within a nested user namespace, the shell is bash which is your cmd shell C$ id uid=65534(nobody) gid=65534(nobody) groups=65534(nobody) C$ ls -l ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:1:0","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"/proc/$pid/uid_map the map file /proc/$pid/uid_map returns a mapping from UIDs in the user namespace to which the process pid belongs， user_namespace linux man page 每一行的格式为 $fromID $toID $length， C$ echo $$ 8683 # 新的 PID P$ echo $$ 7898 # 原始 PID C$ cat /proc/8683/uid_map C$ cat /proc/7898/uid_map 0 0 4294967295 P$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),977(docker) P$ ip link add type veth RTNETLINK answers: Operation not permitted # 在一个不同的username 和 net namespace中尝试 P$ unshare -nU bash C$ ip link add type veth RTNETLINK answers: Operation not permitted C$ echo $$ 9078 P$ echo \"0 1000 1\" \u003e /proc/9078/uid_map C$ id uid=0(root) gid=65534(nobody) groups=65534(nobody) C$ ip link add type veth # Success! P$ echo deny \u003e /proc/9078/setgroups P$ echo \"0 1000 1\" \u003e /proc/9078/gid_map C$ id uid=0(root) gid=0(root) groups=0(root),65534(nobody) ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:1:1","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"mount namespace cat /proc/$$/mounts proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 sys /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 dev /dev devtmpfs rw,nosuid,relatime,size=496792k,nr_inodes=124198,mode=755 0 0 run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 /dev/sda2 / btrfs rw,relatime,space_cache,subvolid=5,subvol=/ 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup2 /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 bpf /sys/fs/bpf bpf rw,nosuid,nodev,noexec,relatime,mode=700 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 configfs /sys/kernel/config configfs rw,nosuid,nodev,noexec,relatime 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=46,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10711 0 0 mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /tmp tmpfs rw,nosuid,nodev 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=100836k,mode=700,uid=1000,gid=1000 0 0 vagrant /vagrant vboxsf rw,relatime 0 0 mount point $ unshare -m bash $ cat /proc/$$/mounts /dev/sda2 / btrfs rw,relatime,space_cache,subvolid=5,subvol=/ 0 0 dev /dev devtmpfs rw,nosuid,relatime,size=496792k,nr_inodes=124198,mode=755 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=46,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10711 0 0 sys /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup2 /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nos","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:1:2","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"进程 $ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:17 ? 00:00:00 /sbin/init root 2 0 0 01:17 ? 00:00:00 [kthreadd] root 3 2 0 01:17 ? 00:00:00 [rcu_gp] root 4 2 0 01:17 ? 00:00:00 [rcu_par_gp] root 5 2 0 01:17 ? 00:00:00 [kworker/0:0-events] root 6 2 0 01:17 ? 00:00:00 [kworker/0:0H-kblockd] root 7 2 0 01:17 ? 00:00:00 [kworker/u4:0-btrfs-endio-write] root 8 2 0 01:17 ? 00:00:00 [mm_percpu_wq] 有两个进程很特殊，pid={1,2}, 这两个进程都是被 Linux 中的上帝进程 idle 创建出来的 pid=1 的 /sbin/init, 执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程 pid=2 的kthreadd, 管理和调度其他的内核进程 ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:2:0","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"网络 在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口： $ brctl show bridge name bridge id STP enabled interfaces br-43ee5ed53f84 8000.024297630322 no docker0 8000.0242bf0282f8 no docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。 $ iptables -t nat -L Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- anywhere anywhere ADDRTYPE match dst-type LOCAL Chain DOCKER (2 references) target prot opt source destination RETURN all -- anywhere anywhere ip $ ip link list 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:d4:b6:c8 brd ff:ff:ff:ff:ff:ff 3: eth1: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc fq_codel state DOWN mode DEFAULT group default qlen 1000 link/ether 08:00:27:b7:f5:14 brd ff:ff:ff:ff:ff:ff $ ip netns add coke $ ip netns list coke ip netns list 只显示命名的 netns，初始 netns 是非命名 netns。并且每一个命名 netns 都会在/var/run/netns创建同名文件，这个文件可以让进程切换到此 netns C$ 代表在一个子命名空间中执行 $ ip netns exec coke bash C$ ip link list 1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 C$ ping 127.0.0.1 connect: Network is unreachable C$ ip link set dev lo up C$ ip link list 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 C$ ping 127.0.0.1 # OK 现在在此 netns 中只可以与同 netns 的进程进行沟通(localhost),我们可以尝试与 init netns 的程序进行沟通 ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:3:0","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"Veth Devices $ ip link add veth0 type veth peer name veth1 # # Create a veth pair (veth0 \u003c=\u003e veth1) $ ip link set veth1 netns coke # # Move the veth1 end to the new namespace C$ ip link list 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 7: veth1@if5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether ee:16:0c:23:f3:af brd ff:ff:ff:ff:ff:ff link-netnsid 0 $ ip addr add 10.1.1.1/24 dev veth0 现在 veth1 设备已经可以在两个 netns 中看到，为了是他们都可以工作，我们需要给它们两个ip addresses 和让interface up $ ip addr add 10.1.1.1/24 dev veth0 # In the initial namespace $ ip link set dev veth0 up C$ ip addr add 10.1.1.2/24 dev veth1 # # In the coke namespace C$ ip link set dev veth1 up C$ ip addr show veth1 4: veth1@if5: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 1a:e7:9f:4e:d4:db brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.1.1.2/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::18e7:9fff:fe4e:d4db/64 scope link valid_lft forever preferred_lft forever 现在 veth0 和 veth1 已经 up 和赋予了 ip address 10.1.1.1 10.1.1.2 $ ping -I veth0 10.1.1.2 C$ ping 10.1.1.1 ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:3:1","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"netlink ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:3:2","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"libnetwork ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:3:3","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"chroot https://www.ibm.com/developerworks/cn/linux/l-cn-chroot/index.html CGroups https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html UnionFS docker export $(docker create busybox) | tar -C rootfs -xvf - AUFS overlay2 查看系统是否支持相应的文件系统 $ uname -a Linux archlinux 5.1.15-arch1-1-ARCH #1 SMP PREEMPT Tue Jun 25 04:49:39 UTC 2019 x86_64 GNU/Linux $ grep btrfs /proc/filesystems btrfs 有一些系统这种方式找不到对应的系统，但是同样支持此文件系统，比如 archlinux 发行版，Overlay_filesystem, overlay-docker-doc $ mkdir b0 b1 b2 upper work merged $ mount -t overlay overlay -o lowerdir=./b0:./b1:./b2,upperdir=./upper,workdir=./work ./merged overlay2 将 lowerdir、upperdir、workdir 联合挂载，形成最终的 merged 挂载点，其中 lowerdir 是镜像只读层，upperdir 是容器可读可写层，workdir 是执行涉及修改 lowerdir 执行 copy_up 操作的中转层（例如，upperdir 中不存在，需要从 lowerdir 中进行复制，该过程暂未详细了解，遇到了再分析） $ tree . ├── b0 ├── b1 ├── b2 ├── merged ├── README.md ├── upper └── work ├── index └── work 获得的文件系统是有层次的，当前的层次关系为： /upper /b0 /b1 /b2 $ echo '192.168.0.1' \u003e b0/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 ├── b2 ├── merged │ └── ip.txt ├── README.md ├── upper └── work ├── index └── work $ cat merged/ip.txt 192.168.0.1 $ echo '192.168.0.2' \u003e merged/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 ├── b2 ├── merged │ └── ip.txt ├── README.md ├── upper │ └── ip.txt └── work ├── index └── work $ cat upper/ip.txt 192.168.0.2 $ echo '192.168.0.3' \u003e upper/ip.txt $ cat merged/ip.txt 192.168.0.3 $ rm -rf merged/ip.txt $ tree . ├── b0 │ └── ip.txt ├── b1 │ └── ip.txt ├── b2 ├── merged ├── README.md ├── upper │ └── ip.txt └── work ├── index └── work $ ls upper/ip.txt c--------- 1 root root 0, 0 Sep 18 04:35 upper/ip.txt references container-lab https://draveness.me/docker 调试网络 docker run -it --net container:vibrant_blackburn nicolaka/netshoot https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt overlayfs的一些限制/兼容性问题 linux-namespace-isolate-programing 有一个例子程序 Namespaces in operation cgroup/linux-man-page user-namespace/linum-man-page mount namespaces proc special filesystem management ip netns ip command cheatsheet veth Netlink-doc man netlink A deep dive into Linux namespaces 100 个容器周边项目，点亮你的容器集群技能树 ","date":"2019-09-17","objectID":"/comtainerd-cgroup-namespace/:4:0","tags":["cgroup","namespace"],"title":"Comtainerd Cgroup Namespace","uri":"/comtainerd-cgroup-namespace/"},{"categories":null,"content":"文章简介：整理 Git 学习资源 TODO references 图解 Git Git 交互式学习环境-演示 Git 交互式学习环境-学习中使用 Google Engineering Practices Documentation Conventional Commits 有工具可以生成 changelog ","date":"2019-09-09","objectID":"/share-git-resources/:0:0","tags":["工具","生产力"],"title":"整理 Git 学习资源","uri":"/share-git-resources/"},{"categories":null,"content":"文章简介：消息队列中需要解决的问题 网络通信可能会包含，成功、失败以及超时三种情况 ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:0:0","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"消息投递语义 最多一次（At-Most Once）、最少一次（At-Least Once）以及恰好一次（Exactly Once） 最多一次在 TCP/UDP 传输层协议就是保证最多一次消息投递，消息的发送者只会尝试发送该消息一次，并不会关心该消息是否得到了远程节点的响应。 最少一次引入超时重试的机制。同时引入新的问题，消息重复。 恰好一次从理论上来说，在分布式系统中想要解决消息重复的问题是不可能的，很多消息服务提供了正好一次的 QoS 其实是在接收端进行了去重。 ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:1:0","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"投递顺序 由于一些网络的问题，消息在投递时可能会出现顺序不一致性的情况，在网络条件非常不稳定时，我们就可能会遇到接收方处理消息的顺序和生产者投递的不一致；消费者就需要对顺序不一致的消息进行处理，常见的两种方式就是使用序列号或者状态机。 ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:2:0","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"序列号 用阻塞的方式保证序列号的递增或者忽略部分『过期』的消息。 ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:2:1","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"状态机 虽然消息投递的顺序是乱序的，但是资源最终还是通过状态机达到了我们想要的正确状态，不会出现不一致的问题。 ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:2:2","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"协议 AMQP： StormMQ、RabbitMQ， 支持最多一次和最少一次的投递语义，当我们选择最少一次时，需要幂等或者重入机制保证消息重复不会出现问题。 MQTT ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:3:0","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"reference 分布式事务的实现原理 分布式系统与消息的投递 浅谈数据库并发控制 - 锁和 MVCC 消息队列设计的精髓基本都藏在本文里了 消息队列设计精要 消息队列设计精要 ","date":"2019-07-29","objectID":"/problems-mq-need-to-be-solved/:4:0","tags":["消息队列","Architecture","Distributed"],"title":"消息队列中需要解决的问题","uri":"/problems-mq-need-to-be-solved/"},{"categories":null,"content":"《微服务设计》学习笔记：高可用系统设计，有哪些问题需要解决 ","date":"2019-07-07","objectID":"/microservicedesign/:0:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"什么是高可用系统 关于高可用的系统 ","date":"2019-07-07","objectID":"/microservicedesign/:1:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"高可用系统需要解决的问题 ","date":"2019-07-07","objectID":"/microservicedesign/:2:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"挑战 成倍的 API 数量 引入网络延迟 CAP 理论，处理跨多个服务的事务复杂 调试分布式系统十分复杂 服务雪崩 大量请求堆积，故障恢复慢 微服务技术选型 API 版本管理混乱 TCC、事务消息队列 ","date":"2019-07-07","objectID":"/microservicedesign/:2:1","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"需要做的事 可扩展：水平扩展、垂直扩展。 通过冗余部署，避免单点故障。 隔离：避免单一业务占用全部资源。避免业务之间的相互影响 2. 机房隔离避免单点故障。 解耦：降低维护成本，降低耦合风险。减少依赖，减少相互间的影响。 限流：滑动窗口计数法、漏桶算法、令牌桶算法等算法。遇到突发流量时，保证系统稳定。 降级：紧急情况下释放非核心功能的资源。牺牲非核心业务，保证核心业务的高可用。 熔断：异常情况超出阈值进入熔断状态，快速失败。减少不稳定的外部依赖对核心服务的影响。 系统监控：对 CPU 利用率，load，内存，带宽，系统调用量，应用错误量，PV，UV 和业务量进行监控 自动化测试：通过完善的测试，减少发布引起的故障。 灰度发布(+回滚)：灰度发布是速度与安全性作为妥协，能够有效减少发布故障。 异步消息系统 ","date":"2019-07-07","objectID":"/microservicedesign/:2:2","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"需要深入的技术栈 分布式系统、DevOps、基础架构即代码(IaC)、不同类型的数据库、前端组件化和复合化、单元测试、全自动发布、迭代、小版本发布计划、测试工具、多版本管理 ","date":"2019-07-07","objectID":"/microservicedesign/:2:3","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"分布式一致性与共识算法 Consensus from wiki, 总结下来一致性就是，在分布式系统中，在给定的一系列操作，即使系统内部出错，最终整个系统对外提供的数据都是可靠的。在协调一致性的过程中，对于一个 Proposal 整个系统达成共识，共识算法起着很重要的作用。 ","date":"2019-07-07","objectID":"/microservicedesign/:3:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"CAP 加州伯克利大学的教授 Eric Brewer 论文Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services 阐述了 CAP 理论 CAP from wiki, 论文中提出的观点：在异步的网络模型中，所有的节点由于没有时钟仅仅能根据接收到的消息作出判断，这时完全不能同时保证一致性(Consistency)、可用性(Availability)和分区容错性(Partition tolerance)，每一个系统只能在这三种特性中选择两种。 由于网络有一定的延迟，并不能做到强一致性，所以大部分时候采用最终一致性的方式，容忍一定时间内数据不一致，在一定的时间内系统内部各节点可以在有限的时间内解决冲突，使数据恢复准确的状态。 ","date":"2019-07-07","objectID":"/microservicedesign/:3:1","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"拜占庭将军问题 拜占庭将军问题论文 拜占庭将军问题是对分布式系统容错的最高要求，然而这不是日常工作中使用的大多数分布式系统中会面对的问题，我们遇到更多的还是节点故障宕机或者不响应等情况，这就大大简化了系统对容错的要求 ","date":"2019-07-07","objectID":"/microservicedesign/:3:2","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"FLP FLP 不可能定理是分布式系统领域最重要的定理之一，它给出了一个非常重要的结论：在网络可靠并且存在节点失效的异步模型系统中，不存在一个可以解决一致性问题的确定性算法。 In this paper, we show the surprising result that no completely asynchronous consensus protocol can tolerate even a single unannounced process death. We do not consider Byzantine failures, and we assume that the message system is reliable it delivers all messages correctly and exactly once. 相关论文 ","date":"2019-07-07","objectID":"/microservicedesign/:3:3","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"共识算法 ","date":"2019-07-07","objectID":"/microservicedesign/:4:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"paxos, raft 包括 Paxos、Raft The Raft Consensus Algorithm raft 工作原理动图 Paxos 算法难以理解、难以实现，难道什么程度呢？在 raft 的论文中有提及。比如 zookeeper 的 zab 就是在 paxos 的基础上进行设计的。每一种 Paxos 的实现，都需要重新设计实现一套算法。而 raft 相对难度降低很多。 基于 raft 的 etcd，consul 等。 ","date":"2019-07-07","objectID":"/microservicedesign/:4:1","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"POW(Proof-of-Work) 无论是 Paxos 还是 Raft 其实都只能解决非拜占庭将军容错的一致性问题，不能够应对分布式网络中出现的极端情况，但是这在传统的分布式系统都不是什么问题，无论是分布式数据库还是消息队列集群，它们内部的节点并不会故意的发送错误信息，在类似系统中，最常见的问题就是节点失去响应或者失效，所以它们在这种前提下是有效可行的，也是充分的。 工作量证明是一个用于阻止拒绝服务攻击和类似垃圾邮件等服务错误问题的协议 ","date":"2019-07-07","objectID":"/microservicedesign/:4:2","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"POS(Proof-of-Stake) 权益证明是区块链网络中的使用的另一种共识算法，在基于权益证明的密码货币中，下一个区块的选择是根据不同节点的股份和时间进行随机选择的。 ","date":"2019-07-07","objectID":"/microservicedesign/:4:3","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"DPOS(Delegated Proof-of-Stake) ","date":"2019-07-07","objectID":"/microservicedesign/:4:4","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"《微服务设计》脑图 脑图如下： 脑图地址 ","date":"2019-07-07","objectID":"/microservicedesign/:5:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"references 后端架构师技术图谱 hystrix bilibili/kratos Go Microservices blog series microservices-in-golang shippy-demo 20 个好用的 Go 语言微服务开发框架 micro/micro micro/go-micro CAP theorem grpc-example 分布式一致性与共识算法 ","date":"2019-07-07","objectID":"/microservicedesign/:6:0","tags":["Architecture","微服务","分布式","笔记"],"title":"高可用系统设计问题、微服务设计笔记 脑图","uri":"/microservicedesign/"},{"categories":null,"content":"文章简介：中文翻译 The Log: What every software engineer should know about real-time data’s unifying abstraction ","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:0:0","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"正文 六年前我在一个特别有趣的时间加入了 LinkedIn。 我们刚刚开始遇到单体集中式数据库的极限，需要开始向分布式系统过渡。 这是一个有趣的经历：我们构建，部署并运行分布式图形数据库，分布式搜索后端，Hadoop 安装以及第一代和第二代键值存储一直到今天。 我在这一切中学到的最有用的东西之一就是我们正在构建的许多东西都有一个非常简单的概念：日志。 有时称为预写日志或提交日志或事务日志，日志几乎与计算机一样长，并且是许多分布式数据系统和实时应用程序体系结构的核心。 在不了解日志的情况下，您无法完全理解数据库，NoSQL 存储，键值存储，复制，paxos，hadoop，版本控制或几乎任何软件系统; 然而，大多数软件工程师并不熟悉它们。 我想改变这一点。 在这篇文章中，我将向您介绍有关日志的所有信息，包括日志以及如何使用日志进行数据集成，实时处理和系统构建。 ","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:1:0","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"Part One: What is Log A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time. 记录附加到日志的末尾，读取从左到右进行。 每个条目都分配有唯一的顺序日志条目号。 记录的顺序定义了“时间”的概念，因为左边的条目被定义为比右边的条目更旧。 日志条目号可以被认为是条目的“时间戳”。 将这种排序描述为时间概念起初看起来有点奇怪，但它具有与任何特定物理时钟分离的便利特性。 当我们进入分布式系统时，此属性将变得至关重要。 出于本讨论的目的，记录的内容和格式并不重要。 此外，我们不能只是继续向日志添加记录，因为我们最终将耗尽空间。 我会稍微回过头来看看。 因此，日志与文件或表格完全不同。 文件是一个字节数组，一个表是一个记录数组，一个日志实际上只是一种表或文件，其中记录按时间排序。 在这一点上，您可能想知道为什么值得谈论这么简单的事情？ 如何以仅附加的记录序列与数据系统相关联？ 答案是日志有一个特定的目的：它们记录发生的事情和时间。 对于分布式数据系统，这在许多方面都是问题的核心。 但在我们走得太远之前，让我澄清一些有点令人困惑的事情。 每个程序员都熟悉另一个日志记录定义 - 应用程序可能使用 syslog 或 log4j 写入本地文件的非结构化错误消息或跟踪信息。 为清楚起见，我将其称为“应用程序日志记录”。 应用程序日志是我描述的日志概念的退化形式。 最大的区别是文本日志主要是供人阅读，而我所描述的“日志”或“数据日志”是为编程访问而构建的。 （实际上，如果你对它进行深入的思考，那么人们读取某个机器上的日志这种理念有些不顺应时代。当涉及到许多服务和服务器的时候，这种方法很快就变成一个难于管理的方式，而且为了认识多个机器的行为，日志的目标很快就变成查询和图形化这些行为的输入了-对多个机器的某些行为而言，文件里的英文形式的文本同这儿所描述的这种结构化的日志相比几乎就不适合了。） Logs in databases 我不知道日志概念的起源 - 可能是二元搜索这样的事情之一，对于发明者来说太简单了，不能发现它是一项发明。 它早在 IBM 的 System R 就已存在。数据库中的使用与在出现崩溃时保持同步的各种数据结构和索引有关。 为了使这种原子性和持久性，数据库使用日志来写出有关他们将要修改的记录的信息，然后将更改应用于它维护的所有各种数据结构。 日志是发生的事件的记录，每个表或索引是将该历史记录投影到一些有用的数据结构或索引中。 由于日志会立即保留，因此在崩溃时将其用作恢复所有其他持久性结构的权威来源。 随着时间的推移，日志的使用从 ACID 的实现细节发展到在数据库之间复制数据的方法。 事实证明，数据库上发生的更改顺序正是保持远程副本数据库同步所需的。 Oracle，MySQL 和 PostgreSQL 包括日志传送协议，用于将部分日志传输到充当从属服务器的副本数据库。 Oracle 已将日志产品化为非 oracle 数据订阅者的通用数据订阅机制，其XStream和GoldenGate以及 MySQL 和 PostgreSQL 中的类似工具是许多数据架构的关键组件。 由于这个起源，机器可读日志的概念主要局限于数据库内部。 使用日志作为数据订阅的机制似乎几乎是偶然出现的。 但这种抽象是支持各种消息传递，数据流和实时数据处理的理想选择。 Logs in distributed systems 日志解决的两个问题 - 排序更改(ordering changes)和分发数据(distributing data) - 在分布式数据系统中更为重要。 同意订购更新（或同意不同意和应对副作用）是这些系统的核心设计问题。 分布式系统的以日志为中心的方法源于一个简单的讨论，我称之为状态机复制原则： If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state. 如果两个相同的确定性过程以相同的状态开始并以相同的顺序获得相同的输入，则它们将产生相同的输出并以相同的状态结束。 这可能看起来有点难懂的(obtuse)，所以让我们深入了解它的含义。 Deterministic(确定性)意味着处理不依赖于时间，并且不允许任何其他“外部的”输入影响其结果。 例如，一个程序的输出受到线程执行的特定顺序的影响，或者通过调用 gettimeofday 或其他一些不可重复的东西，通常被认为是非确定性的。 进程的 state 状态是处理结束时机器上的任何数据，无论是在内存中还是在磁盘上。 以相同的顺序获得相同输入的地方应当引起注意-这就是引入日志的地方。这儿有一个重要的常识：如果给两段确定性代码相同的日志输入，那么它们就会生成相同的输出。 分布式计算这方面的应用就格外明显。你可以把用多台机器一起执行同一件事情的问题缩减为实现分布式一致性日志为这些进程输入的问题。这日志的目的是把所有非确定性的东西排除在输入流之外，来确保每个复制进程能够同步地处理输入。 当你理解了这个以后，状态机复制原理就不再复杂或者说不再深奥了：这或多或少的意味着\"deterministic processing is deterministic\"(确定性的处理过程就是确定性的)。不管怎样，我都认为它是分布式系统设计里较常用的工具之一。 这种方式的一个美妙之处就在于索引日志的时间戳就像时钟状态的一个副本——你可以用一个单独的数字描述每一个副本，这就是经过处理的日志的时间戳。时间戳与日志一一对应着整个副本的状态。 由于写进日志的内容的不同，也就有许多在系统中应用这个原则的不同方式。举个例子，我们记录一个服务的请求，或者服务从请求到响应的状态变化，或者它执行命令的转换。理论上来说，我们甚至可以为每一个副本记录一系列要执行的机器指令或者调用的方法名和参数。只要两个进程用相同的方式处理这些输入，这些进程就会保持副本的一致性。 一千个人眼中有一千种日志的用法。数据库工作者通常区分物理日志和逻辑日志。物理日志就是记录每一行被改变的内容。逻辑日志记录的不是改变的行而是那些引起行的内容被改变的 SQL 语句（insert，update 和 delete 语句）。 分布式系统通常可以宽泛分为两种方法来处理数据和完成响应。“状态机器模型”通常引用一个主动-主动的模型——也就是我们为之记录请求和响应的对象。对此进行一个细微的更改，称之为“主备模型”，就是选出一个副本做为 leader，并允许它按照请求到达的时间来进行处理并从处理过程中输出记录其状态改变的日志。其他的副本按照 leader 状态改变的顺序而应用那些改变，这样他们之间达到同步，并能够在 leader 失败的时候接替 leader 的工作。 为了理解两种方式的不同，我们来看一个不太严谨的例子。假定有一个算法服务的副本，保持一个独立的数字作为它的状态（初始值为 0），并对这个值进行加法和乘法运算。主动-主动方式应该会输出所进行的变换，比如“+1”，“*2”等。每一个副本都会应用这些变换，从而得到同样的解集。主动-被动方式将会有一个独立的主体执行这些变换并输出结果日志，比如“1”，“3”，“6”等。这个例子也清楚的展示了为什么说顺序是保证各副本间一致性的关键：一次加法和乘法的顺序的改变将会导致不同的结果。 分布式日志可以理解为一致性问题模型的数据结构。因为日志代表了后续追加值的一系列决策。你需要重新审视 Paxos 算法簇，尽管日志模块是他们最常见的应用。 在 Paxos 算法中，它通常通过使用称之为多 paxos 的协议，这种协议将日志建模为一系列的问题，在日志中每个问题都有对应的部分。在ZAB， Raft等其它的协议中，日志的作用尤为突出，它直接对维护分布式的、一致性的日志的问题建模。 我怀疑的是，我们就历史发展的观点是有偏差的，可能是由于过去的几十年中，分布式计算的理论远超过了其实际应用。在现实中，共识的问题是有点太简单了。计算机系统很少需要决定单个值，他们几乎总是处理成序列的请求。这样的记录，而不是一个简单的单值寄存器，自然是更加抽象。 此外，专注于算法掩盖了 抽象系统需要的底层的日志。我怀疑，我们最终会把日志中更注重作为一个商品化的基石，不论其是否以同样的方式 实施的，我们经常谈论一个哈希表而不是纠结我们 得到是不是具体某个细节的哈希表，例如线性或者带有什么什么其它变体哈希表。日志将成为一种大众化的接口，为大多数算法和其实现提升提供最好的保证和最佳的性能。 Changelog 101: Tables and Events are Dual 让我们继续聊数据库。数据库中存在着大量变更日志和表之间的二相性。这些日志有点类似借贷清单和银行的流程，数据库表就是当前的盈余表。如果你有大量的变更日志，你就可以使用这些变更用以创建捕获当前状态的表。这张表将记录每个关键点（日志中一个特别的时间点）的状态信息。这就是为什么日志是非常基本的数据结构的意义所在：日志可用来创建基本表，也可以用来创建各类衍生表。同时意味着可以存储非关系型的对象。 这个流程也是可逆的：如果你正在对一张表进行更新，你可以记录这些变更，","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:1:1","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"Part Two: Data Integration 请让我首先解释 一下“数据集成”是什么意思，还有为什么我觉得它很重要，之后我们再来看看它和日志有什么关系。 Data integration is making all the data an organization has available in all its services and systems. 数据集成就是将数据组织起来，使得在与其有关的服务和系统中可以访问它们 而更常见的术语 ETL 通常只是覆盖了数据集成的一个有限子集(译注：ETL，Extraction-Transformation-Loading 的缩写，即数据提取、转换和加载)——相对于关系型数据仓库。但我描述的东西很大程度上可以理解为，将 ETL 推广至实时系统和处理流程。 对数据的高效使用遵循一种 马斯洛的需要层次理论 。金字塔的基础部分包括捕获所有相关数据，能够将它们全部放到适当的处理环境（那个环境应该是一个奇妙的实时查询系统，或者仅仅是文本文件和 python 脚本）。这些数据需要以统一的方式建模，这样就可以方便读取和数据处理。如果这种以统一的方式捕获数据的基本需求得到满足，那么就可以在基础设施上以若干种方法处理这些数据——映射化简（MapReduce），实时查询系统，等等。 很明显，有一点值得注意：如果没有可靠的、完整的数据流，Hadoop 集群除了象昂贵的且难于安装的空间取暖器哪样外不会做更多事情了。一旦数据和处理可用，人们就会关心良好数据模型和一致地易于理解的语法哪些更细致的问题。最后，人们才会关注更加高级的处理-更好的可视化、报表以及处理和预测算法。 以我的经验，大多数机构在数据金字塔的底部存在巨大的漏洞-它们缺乏可靠的、完整的数据流-而是打算直接跳到高级数据模型技术上。这样做完全是反着来做的。 因此，问题是我们如何构建通过机构内所有数据系统的可靠的数据流。 Data Integration: Two complications 两种趋势使数据集成变得更困难。 第一个趋势是增长的事件数据(event data)。事件数据记录的是发生的事情，而不是存在的东西。在 web 系统中，这就意味着用户活动日志，还有为了可靠的操作以及监控数据中心的机器的目的，所需要记录的机器级别的事件和统计数字。人们倾向称它们为“日志数据”，因为它们经常被写到应用的日志中，但是这混淆了形式与功能。这种数据位于现代 web 的中心：归根结底，Google 的资产是由这样一些建立在点击和映像基础之上的相关管道所生成的——那也就是事件。 这些东西并不是仅限于网络公司，只是网络公司已经完全数字化，所以它们更容易用设备记录。财务数据一直是面向事件的。RFID(无线射频识别)将这种跟踪能力赋予物理对象。我认为这种趋势仍将继续，伴随着这个过程的是传统商务活动的数字化。 这种类型的事件数据记录下发生的事情，而且往往比传统数据库应用要大好几个数量级。这对于处理提出了重大挑战。 TThe explosion of specialized data systems 第二个趋势来自于专门的数据系统的爆发，通常这些数据系统在最近的五年中开始变得流行，并且可以免费获得。专门的数据系统是为OLAP, 搜索, 简单 在线 存储, 批处理, 图像分析, 等 等 而存在的。 更多的不同类型数据的组合，以及将这些数据存放到更多的系统中的愿望，导致了一个巨大的数据集成问题。 Log-structured data flow 为了处理系统之间的数据流，日志是最自然的数据结构。其中的秘诀很简单： 将所有组织的数据提取出来，并将它们放到一个中心日志，以便实时查阅。 每个逻辑数据源都可以建模为它自己的日志。一个数据源可以是一个应用程序的事件日志（如点击量或者页面浏览量），或者是一个接受修改的数据库表。每个订阅消息的系统都尽可能快的从日志读取信息，将每条新的记录保存到自己的存储，并且提升其在日志中的地位。订阅方可以是任意一种数据系统 —— 一个缓存，Hadoop，另一个网站中的另一个数据库，一个搜索系统，等等。 例如，日志针对每个更改给出了逻辑时钟的概念，这样所有的订阅方都可以被测量。推导不同的订阅系统的状态也因此变得相对简单的多，因为每个系统都有一个读取动作的“时间点”。 为了让这个显得更具体，我们考虑一个简单的案例，有一个数据库和一组缓存服务器集群。日志提供了一种同步更新所有这些系统，并推导出每一个系统的接触时间点的方法。我们假设写了一条日志 X，然后需要从缓存做一次读取。如果我们想保证看到的不是陈旧的数据，我们只需保证没有从任何尚未复制 X 的缓存中读取即可。 日志也起到缓存的作用，使数据生产与数据消费相同步。由于许多原因这个功能很重要，特别是在多个订阅方消费数据的速度各不相同的时候。这意味着一个订阅数据系统可以宕机，或者下线维护，之后重新上线以后再赶上来：订阅方按照自己控制的节拍来消费数据。批处理系统，如 Hadoop 或者是一个数据仓库，或许只是每小时或者每天消费一次数据，而实时查询系统可能需要及时到秒。由于无论是原始数据源还是日志，都没有各种目标数据系统的相关知识，因此消费方系统可以被添加和删除，而无需传输管道的变化。 Eache working data pipeline is designed like a log; each broken data pipeline is broken is its own way – Anna Karenina principle 特别重要的是：目标系统只知道日志而不知道原始系统的任何细节。 消费者系统不需要关心数据是来自 RDBMS，新的键值存储，还是没有任何类型的实时查询系统。 这似乎是一个小问题，但实际上是需要仔细鉴别的(critical)。 这里我使用术语“日志”取代了“消息系统”或者“发布-订阅”，因为它在语义上更明确，并且对支持数据复制的实际实现这样的需求，有着更接近的描述。我发现“发布订阅”并不比间接寻址的消息具有更多的含义——如果你比较任何两个发布-订阅的消息传递系统的话，你会发现他们承诺的是完全不同的东西，而且大多数模型在这一领域都不是有用的。你可以认为日志是一种消息系统，它具有持久性保证和强大的订阅语义。在分布式系统中，这个通信模型有时有个(有些可怕的)名字叫做atomic broadcast。 值得强调的是，日志仍然只是基础设施。这并不是管理数据流这个故事的结束：故事的其余部分围绕着元数据，模式，兼容性，以及处理数据结构的所有细节及其演化。除非有一种可靠的，一般的方法来处理数据流运作，语义在其中总是次要的细节。 At LinkedIn 在 LinkedIn 从集中式关系数据库向分布式系统集合转化的过程中，我看到这个数据集成问题迅速演变。 主要的数据系统包括： Search Social Graph Voldemort (key-value store) Espresso (document store) Recommendation engine OLAP query engine Hadoop Terradata Ingraphs (monitoring graphs and metrics services) 这种使用日志作为数据流的思想，甚至在我到这里之前就已经与 LinkedIn 相伴了。我们开发的一个最早的基础设施之一，是一种称为 databus 的服务，它在我们早期的 Oracle 表上提供了一种日志缓存抽象，可伸缩订阅数据库修改,这样我们就可以很好支持我们的社交网络和搜索索引。 我会给出一些历史并交代一下上下文。我首次参与到这些大约是在 2008 年左右，在我们转移键值存储之后。我的下一个项目是让一个工作中的 Hadoop 配置演进，并给其增加一些我们的推荐流程。由于缺乏这方面的经验，我们自然而然的安排了数周计划在数据的导入导出方面，剩下的时间则用来实现奇妙的预测算法。这样我们就开始了长途跋涉。 我们本来计划是仅仅将数据从现存的 Oracle 数据仓库中剖离。但是我们首先发现将数据从 Oracle 中迅速取出是一种黑暗艺术。更糟的是，数据仓库的处理过程与我们为 Hadoop 而计划的批处理生产过程不适合——其大部分处理都是不可逆转的，并且与即将生成的报告具体相关。最终我们采取的办法是，避免使用数据仓库，直接访问源数据库和日志文件。最后，我们为了加载数据到键值存储 并生成结果，实现了另外一种管道。 这种普通的数据复制最终成为原始开发项目的主要内容之一。糟糕的是，在任何时间任意管道都有一个问题，Hadoop 系统很大程度上是无用的——在错误的数据基础上运行奇特的(fancy)算法，只会产生更多的错误数据。 虽然我们已经以一种通用的方式创建事物，但是每个数据源都需要自定义配置安装。这也被证明是巨量错误与失败的根源。我们在 Hadoop 上实现的网站功能已经开始流行起来，同时我们发现我们有一长串感兴趣的工程师。每个用户都有他们想要集成的一系列系统，他们想要的一系列新数据源。 ETL in Ancient Greece. Not much has changed. 有些东西在我面前开始渐渐清晰起来。 首先，我们已建成的通道虽然有一些杂乱，但实质上它们是很有价值的。在采用诸如 Hadoop 的新的处理系统生成可用数据的过程，它解锁了大量的可能性。 基于这些数据过去很难实现的计算，如今变为可能。 许多新的产品和分析技术都来源于把分片的数据放在一起，这些数据被锁定在特定的系统中。 第二， 众所周知，可靠的数","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:1:2","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"Part Three: Logs \u0026 Real-time Stream Processing 到此为止，我只是描述从端到端数据复制的理想机制。但是在存储系统中搬运字节不是所要讲述内容的全部。最终我们发现日志是流的另一种说法，日志是流处理的核心。 但是，等等，什么是流处理呢？ 如果你是 90 年代晚期或者 21 世纪初数据库文化或者数据基础架构产品的爱好者，那么你就可能会把流处理与建创 SQL 引擎或者创建“箱子和箭头”接口用于事件驱动的处理等联系起来。 如果你关注开源数据库系统的大量出现，你就可能把流处理和一些开源数据库系统关联起来，这些系统包括了：Storm,Akka,S4和Samza.但是大部分人会把这些系统作为异步消息处理系统，这些系统与支持群集的远程过程调用层的应用没什么差别（而事实上在开源数据库系统领域某些方面确实如此）。 这些视图都有一些局限性。流处理与 SQL 是无关的。它也局限于实时流处理。不存在内在的原因限制你不能处理昨天的或者一个月之前的流数据，且使用多种不同的语言表达计算。 我把流处理视为更广泛的概念：持续数据流处理的基础架构。我认为计算模型可以像 MapReduce 或者分布式处理架构一样普遍，但是有能力处理低时延的结果。 处理模型的实时驱动是数据收集方法。成批收集的数据是分批处理的。数据是不断收集的，它也是按顺序不断处理的。 美国的统计调查就是成批收集数据的良好典范。统计调查周期性的开展，通过挨门挨户的走访，使用蛮力发现和统计美国的公民信息。1790 年统计调查刚刚开始时这种方式是奏效的。那时的数据收集是批处理的，它包括了骑着马悠闲的行进，把信息写在纸上，然后把成批的记录传送到人们统计数据的中心站点。现在，在描述这个统计过程时，人们立即会想到为什么我们不保留出生和死亡的记录，这样就可以产生人口统计信息这些信息或是持续的或者是其它维度的。 这是一个极端的例子，但是大量的数据传送处理仍然依赖于周期性的转储，批量转化和集成。处理大容量转储的唯一方法就是批量的处理。但是随着这些批处理被持续的供给所取代，人们自然而然的开始不间断的处理以平滑的处理所需资源并且消除延迟。 例如 LinkedIn 几乎没有批量数据收集。大部分的数据或者是活动数据或者是数据库变更，这两者都是不间断发生的。事实上，你可以想到的任何商业，正如：Jack Bauer 告诉我们的，低层的机制都是实时发生的不间断的流程事件。数据是成批收集的，它总是会依赖于一些人为的步骤，或者缺少数字化或者是一些自动化的非数字化流程处理的遗留信息。当传送和处理这些数据的机制是邮件或者人工的处理时，这一过程是非常缓慢的。首轮自动化总是保持着最初的处理形式，它常常会持续相当长的时间。 每天运行的批量处理作业常常是模拟了一种一天的窗口大小的不间断计算。当然，低层的数据也经常变化。在 LinkedIn,这些是司空见贯的，并且使得它们在 Hadoop 运转的机制是有技巧的，所以我们实施了一整套管理增量的 Hadoop 工作流的架构。 由此看来，对于流处理可以有不同的观点。流处理包括了在底层数据处理的时间概念，它不需要数据的静态快照，它可以产生用户可控频率的输出，而不用等待数据集的全部到达。从这个角度上讲，流处理就是广义上的批处理，随着实时数据的流行，会儿更加普遍。 这就是为什么从传统的视角看来流处理是利基应用。我个人认为最大的原因是缺少实时数据收集使得不间断的处理成为了学术性的概念。 我想缺少实时数据收集就像是商用流处理系统注定的命运。他们的客户仍然需要处理面向文件的、每日批量处理 ETL 和数据集成。公司建设流处理系统关注的是提供附着在实时数据流的处理引擎，但是最终当时极少数人真正使用了实时数据流。事实上，在我在 LinkedIn 工作的初期，有一家公司试图把一个非常棒的流处理系统销售给我们，但是因为当时我们的全部数据都按小时收集在的文件里，当时我们提出的最好的应用就是在每小时的最后把这些文件输入到流处理系统中。他们注意到这是一个普遍性的问题。这些异常证明了如下规则：流处理系统要满足的重要商业目标之一是：财务， 它是实时数据流已具备的基准，并且流处理已经成为了瓶颈。 甚至于在一个健康的批处理系统中，流处理作为一种基础架构的实际应用能力是相当广泛的。它跨越了实时数据请求-应答服务和离线批量处理之间的鸿沟。现在的互联网公司，大约 25%的代码可以划分到这个类型中。 最终这些日志解决了流处理中绝大部分关键的技术问题。在我看来，它所解决的最大的问题是它使得多订阅者可以获得实时数据。对这些技术细节感兴趣的朋友，我们可以用开源的 Samza,它是基于这些理念建设的一个流处理系统。这些应用的更多技术细节我们在此文档中有详细的描述。 Data flow graphs 流处理最有趣的角度是它与流处理系统内部无关，但是与之密切相关的是如何扩展了我们谈到的早期数据集成的数据获取的理念。我们主要讨论了基础数据的获取或日志–事件和各类系统执行中产生的数据等。但是流处理允许我们包括了计算其它数据的数据。这些衍生的数据在消费者看来与他们计算的原始数据没什么差别。这些衍生的数据可以按任意的复杂度进行压缩。 让我们再深入一步。我们的目标是：流处理作业可以读取任意的日志并把日志写入到日志或者其它的系统中。他们用于输入输出的日志把这些处理关联到一组处理过程中。事实上，使用这种样式的集中日志，你可以把组织全部的数据抓取、转化和工作流看成是一系列的日志和写入它们的处理过程。 流处理器根本不需要理想的框架：它可能是读写日志的任何处理器或者处理器集合，但是额外的基础设施和辅助可以提供帮助管理处理代码。 日志集成的目标是双重的： 首先，它确保每个数据集都有多个订阅者和有序的。让我们回顾一下状态复制原则来记住顺序的重要性。为了使这个更加具体，设想一下从数据库中更新数据流–如果在处理过程中我们把对同一记录的两次更新重新排序，可能会产生错误的输出。 TCP 之类的链接仅仅局限于单一的点对点链接，这一顺序的持久性要优于 TCP 之类的链接，它可以在流程处理失败和重连时仍然存在。 第二，日志提供了流程的缓冲。这是非常基础的。如果处理流程是非同步的，那么上行生成流数据的作业比下行消费流数据的作业运行的更快。这将会导致处理流程阻塞，或者缓冲数据，或者丢弃数据。丢弃数据并不是可行的方法，阻塞将会导致整个流程图立即停止。 日志实际上是一个非常大的缓冲，它允许流程重启或者停止但不会影响流程图其它部分的处理速度。如果要把数据流扩展到更大规模的组织，如果处理作业是由多个不同的团队提供的，这种隔离性是极其重的。我们不能容忍一个错误的作业引发后台的压力，这种压力会使得整个处理流程停止。 Storm和Sama这两者都是按非同步方式设计的，可以使用 Kafka 或者其它类似的系统作为它们的日志。 Stateful Real-Time Processing 一些实时流处理在转化时是无状态的记录。在流处理中大部分的应用会是相当复杂的统计、聚合、不同窗口之间的关联。例如有时人们想扩大包含用户操作信息的事件流（一系列的单击动作）–实际上关联了用户的单击动作流与用户的账户信息数据库。不变的是这类流程最终会需要由处理器维护的一些状态信息。例如数据统计时，你需要统计到目前为止需要维护的计数器。如果处理器本身失败了，如何正确的维护这些状态信息呢？ 最简单的替换方案是把这些状态信息保存在内存中。但是如果流程崩溃，它就会丢失中间状态。如果状态是按窗口维护的，流程就会回退到日志中窗口开始的时间点上。但是，如果统计是按小时进行的，那么这种方式就会变得不可行。 另一个替换方案是简单的存储所有的状态信息到远程的存储系统，通过网络与这些存储关联起来。这种机制的问题是没有本地数据和大量的网络间通信。 我们如何支持处理过程可以像表一样分区的数据呢? 回顾一下关于表和日志二相性的讨论。这一机制提供了工具把数据流转化为与处理过程协同定位的表，同时也提供了这些表的容错处理的机制。 流处理器可以把它的状态保存在本地的表或索引–bdb,或者leveldb,甚至于类似于 Lucene 或 fastbit 一样不常见的索引。这些内容存储在它的输入流中（或许是使用任意的转化）。生成的变更日志记录了本地的索引，它允许存储事件崩溃、重启等的状态信息。流处理提供了通用的机制用于在本地输入流数据的随机索引中保存共同分片的状态。 当流程运行失败时，它会从变更日志中恢复它的索引。每次备份时，日志把本地状态转化成一系列的增量记录。 这种状态管理的方法有一个优势是把处理器的状态也做为日志进行维护。我们可以把这些日志看成与数据库表相对应的变更日志。事实上，这些处理器同时维护着像共同分片表一样的表。因为这些状态它本身就是日志，其它的处理器可以订阅它。如果流程处理的目标是更新结点的最后状态，这种状态又是流程的输出，那么这种方法就显得尤为重要。 为了数据集成，与来自数据库的日志关联，日志和数据库表的二象性就更加清晰了。变更日志可以从数据库中抽取出来，日志可以由不同的流处理器（流处理器用于关联不同的事件流）按不同的方式进行索引。 我们可以列举在 Samza 中有状态流处理管理的更多细节和大量实用的例子。 Log Compaction 当然，我们不能奢望保存全部变更的完整日志。除非想要使用无限空间，日志不可能完全清除。为了澄清它，我们再来聊聊 Kafka 的实现。在 Kafka 中,清理有两种选择，这取决于数据是否包括关键更新和事件数据。对于事件数据，Kafka 支持仅维护一个窗口的数据。通常，配置需要一些时间，窗口可以按时间或空间定义。虽然对于","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:1:3","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"Part Four: System Building 我们最后要讨论的是在线数据系统设计中日志的角色。 在分布式数据库数据流中日志的角色和在大型组织机构数据完整中日志的角色是相似的。在这两个应用场景中，日志是对于数据源是可靠的，一致的和可恢复的。组织如果不是一个复杂的分布式数据系统呢，它究竟是什么？ 分类计价吗？(Unbundling) 如果换个角度，你可以看到把整个组织系统和数据流看做是单一的分布式数据系统。你可以把所有的子查询系统（诸如 Redis, SOLR,Hive 表等）看成是数据的特定索引。你可以把 Storm 或 Samza 一样的流处理系统看成是发展良好的触发器和视图具体化机制。我已经注意到，传统的数据库管理人员非常喜欢这样的视图，因为它最终解释了这些不同的数据系统到底是做什么用的–它们只是不同的索引类型而已。 不可否认这类数据库系统现在大量的出现，但是事实上，这种复杂性一直都存在。即使是在关系数据库系统的鼎盛时期，组织中有大量的关系数据库系统。或许自大型机时代开始，所有的数据都存储在相同的位置，真正的集成是根本不存在的。存在多种外在需求，需要把数据分解成多个系统，这些外在需求包括：规模、地理因素、安全性，性能隔离是最常见的因素。这些需求都可以由一个优质的系统实现：例如，组织可以使用单一的 Hadoop 聚簇，它包括了全部的数据，可以服务于大型的和多样性的客户。 因此在向分布式系统变迁的过程中，已经存在一种处理数据的简便的方法：把大量的不同系统的小的实例聚合成为大的聚簇。许多的系统还不足以支持这一方法：因为它们不够安全，或者性能隔离性得不到保证，或者规模不符合要求。不过这些问题都是可以解决的。 依我之见，不同系统大量出现的原因是建设分布式数据库系统很困难。通过削减到单一的查询或者用例，每个系统都可以把规模控制到易于实现的程度。但是运行这些系统产生的复杂度依然很高。 未来这类问题可能的发展趋势有三种： 第一种可能是保持现状：孤立的系统还会或长或短的持续一段时间。这是因为建设分布式系统的困难很难克服，或者因为孤立系统的独特性和便捷性很难达到。基于这些原因，数据集成的核心问题仍然是如何恰当的使用数据。因此，集成数据的外部日志非常的重要。 第二种可能是重构：具备通用性的单一的系统逐步融合多个功能形成超极系统。这个超级系统表面看起来类似关系数据库系统，但是在组织中你使用时最大的不同是你只需要一个大的系统而不是无数个小系统。在这个世界里，除了在系统内已解决的这个问题不存在什么真正的数据集成问题。我想这是因为建设这样的系统的实际困难。 虽然另一种可能的结果对于工程师来说是很有吸引力的。新一代数据库系统的特征之一是它们是完全开源的。开源提供了一种可能性：数据基础架构不必打包成服务集或者面向应用的系统接口。在 Java 栈中，你可以看到在一定程度上，这种状况已经发生了。 Zookeeper用于处理多个系统之间的协调，或许会从诸如Helix 或者Curator等高级别的抽象中得到一些帮助。 Mesos和YARN用于处理流程可视化和资源管理。 Lucene和LevelDB等嵌入式类库做为索引。 Netty,Jetty和Finagle,rest.li等封装成高级别的用于处理远程通信。 Avro,Protocol Buffers,Thrift和umpteen zillion等其它类库用于处理序列化。 Kafka和Bookeeper提供支持日志。 如果你把这些堆放在一起，换个角度看，它有点像是简化版的分布式数据库系统工程。你可以把这些拼装在一起，创建大量的可能的系统。显而易见，现在探讨的不是最终用户所关心的 API 或者如何实现，而是在不断多样化和模块化的过程中如何设计实现单一系统的途径。因为随着可靠的、灵活的模块的出现，实施分布式系统的时间周期由年缩减为周，聚合形成大型整体系统的压力逐步消失。 The place of the log in system architecture 那些提供外部日志的系统如今已允许个人电脑抛弃他们自身复杂的日志系统转而使用共享日志。在我看来，日志可以做到以下事情： 通过对节点的并发更新的排序处理数据的一致性（无论在及时还是最终情况下） 提供节点之间的数据复制 提供”commit“语法（只有当写入器确保数据不会丢失时才会写入） 位系统提供外部的数据订阅资源 提供存储失败的复制操作和引导新的复制操作的能力 处理节点间的数据平衡 这实际上是一个数据分发系统最重要的部分，剩下的大部分内容与终端调用的 API 和索引策略相关。这正是不同系统间的差异所在，例如：一个全文本查询语句需要查询所有的分区，而一个主键查询只需要查询负责键数据的单个节点就可以了。 下面我们来看下该系统是如何工作的。系统被分为两个逻辑区域：日志和服务层。日志按顺序捕获状态变化，服务节点存储索引提供查询服务需要的所有信息（键-值的存储可能以 B-tree 或 SSTable 的方式进行，而搜索系统可能存在与之相反的索引）。写入器可以直接访问日志，尽管需要通过服务层代理。在写入日志的时候会产生逻辑时间戳（即 log 中的索引），如果系统是分段式的，那么就会产生与段数目相同数量的日志文件和服务节点，这里的数量和机器数量可能会有较大差距。 服务节点订阅日志信息并将写入器按照日志存储的顺序尽快应用到它的本地索引上。 客户端只要在查询语句中提供对应的写入器的时间戳，它就可以从任何节点中获取”读写“语义。服务节点收到该查询语句后会将其中的时间戳与自身的索引比较，如果必要，服务节点会延迟请求直到对应时间的索引建立完毕，以免提供旧数据。 服务节点或许根本无需知道”控制“或”lerder 选举（leader election）“的概念，对很多简单的操作，服务节点可以爱完全脱离领导的情况下提供服务，日志即是信息的来源。 分发系统所需要做的其中一个比较复杂的工作，就是修复失败节点并移除几点之间的隔离。保留修复的数据并结合上各区域内的数据快照是一种较为典型的做法，它与保留完整的数据备份并从垃圾箱内回收日志的做法几乎等价。这就使得服务层简单了很多，日志系统也更有针对性。 有了这个日志系统，你可以订阅到 API，这个 API 提供了把 ETL 提供给其它系统的数据内容。事实上，许多系统都可以共享相同的日志同时提供不同的索引，如下所示： 这个系统的视图可以清晰的分解到日志和查询 API,因为它允许你从系统的可用性和一致性角度分解查询的特征。这可以帮助我们对系统进行分解，并理解那些并没按这种方式设计实施的系统。 虽然 Kafka 和 Bookeeper 都是一致性日志，但这不是必须的，也没什么意义。你可以轻松的把 Dynamo 之类的数据构分解为一致性的 AP 日志和键值对服务层。这样的日志使用起来灵活，因为它重传了旧消息，像 Dynamo 一样，这样的处理取决于消息的订阅者。 在很多人看来，在日志中另外保存一份数据的完整复本是一种浪费。事实上，虽然有很多因素使得这件事并不困难。首先，日志可以是一种有效的存储机制。我们在 Kafka 生产环境的服务器上存储了 5 TB 的数据。同时有许多的服务系统需要更多的内存来提供有效的数据服务，例如文本搜索，它通常是在内存中的。服务系统同样也需样硬盘的优化。例如，我们的实时数据系统或者在内存外提供服务或者使用固态硬盘。相反，日志系统只需要线性的读写，因此，它很乐于使用 TB 量级的硬盘。最终，如上图所示，由多个系统提供的数据，日志的成本分摊到多个索引上，这种聚合使得外部日志的成本降到了最低点。 LinkedIn 就是使用了这种方式实现它的多个实时查询系统的。这些系统提供了一个数据库（使用数据总线做为日志摘要，或者从 Kafka 去掉专用的日志），这些系统在顶层数据流上还提供了特殊的分片、索引和查询功能。这也是我们实施搜索、社交网络和 OLAP 查询系统的方式。事实上这种方式是相当普遍的：为多个用于实时服务的服务系统提供单一的数据（这些来自 Hadoop 的数据或是实时的或是衍生的）。这种方式已被证实是相当简洁的。这些系统根本不需要外部可写入的 API，Kafka 和数据库被用做系统的记录和变更流，通过日志你可以查询系统。持有特定分片的结点在本地完成写操作。这些结点盲目的把日志提供的数据转录到它们自己的存储空间中。通过回放上行流日志可以恢复转录失败的结点。 这些系统的程度则取决于日志的多样性。一个完全可靠的系统可以用日志来对数据分片、存储结点、均衡负载，以及用于数据一致性和数据复制等多方面。在这一过程中，服务层实际上只不过是一种缓存机制，这种缓存机制允许直接写入日志的流处理。 ","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:1:4","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"结束语 如果你对于本文中所谈到的关于日志的大部内容，如下内容是您可以参考的其它资料。对于同一事务人们会用不同的术语，这会让人有一些困惑，从数据库系统到分布式系统，从各类企业级应用软件到广阔的开源世界。无论如何，在大方向上还是有一些共同之处。 ","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:1:5","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"references 参考翻译 部分自己翻译，大段的抄这里的翻译～ 参考文献见此 ","date":"2019-07-04","objectID":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/:2:0","tags":["database","distributed","log","翻译"],"title":"The Log: What every software engineer should know about real-time data's unifying abstraction 中文翻译","uri":"/the-log-every-software-engineer-should-know-about-real-time-data-unifying-abstraction/"},{"categories":null,"content":"文章简介：简单介绍最近自己最近设计实现 newdb 的工作 项目地址 anydemo/newdb 最近一段时间花了很大的力气学习数据库的知识，设计实现了简单的 RDBMS, 收获比较丰富 开始先从数据库底层的数据持久化开始。再此设计的数据库主要是固定类型大小的关系型数据库。一个 table 中有固定大小的一定数量的数据列，每一个 tuple 的大小是固定不变的，也就是每一个 page 的可以容纳的 tuple 是一定的，这样很容易在记录某一个 tuple 的位置、以及其在文件中的位置。在这过程中收获最大的部分就是对于 Marshal 于 Unmarshal 的理解更进一步，知道数据如何在数据库中是如何存储的，以及事务的本质。 最后通过一个 Sequences Scan 的例子，手写一个物理执行计划，执行一次全表扫描。 未来还有很多的内容可以做，暂时告一段落，最近抽时间沉淀一下，思考一下未来，回头有时间了再回头继续加新的内容。毕竟issues积攒了太多。 ","date":"2019-06-29","objectID":"/go-simple-db-storage-engine/:0:0","tags":["db","wheel","go"],"title":"一个简单的数据存储引擎","uri":"/go-simple-db-storage-engine/"},{"categories":null,"content":"文章简介：梳理 k8s scheduler 工作原理 office scheduler info code pkg/scheduler/scheduler.go#scheduleOne 真正的执行逻辑 组件 ","date":"2019-05-10","objectID":"/k8s-scheduler-introduce/:0:0","tags":["k8s","architecture"],"title":"K8s Scheduler","uri":"/k8s-scheduler-introduce/"},{"categories":null,"content":"scheduler 调度会 binding 到 apiserver，异步的等待 pods 启动pkg/scheduler/scheduler.go#bind 默认 pod 调度算法：pkg/scheduler/core/generic_scheduler.go#genericScheduler ","date":"2019-05-10","objectID":"/k8s-scheduler-introduce/:1:0","tags":["k8s","architecture"],"title":"K8s Scheduler","uri":"/k8s-scheduler-introduce/"},{"categories":null,"content":"如何调度 “Computing predicates” “Computing predicates”：调用 findNodesThatFit()方法； predicates.FitPredicate implement: NoDiskConflict “Prioritizing” “Prioritizing”：调用 PrioritizeNodes()方法； “Selecting host” “Selecting host”：调用 selectHost()方法。 List-Watch Informer ","date":"2019-05-10","objectID":"/k8s-scheduler-introduce/:1:1","tags":["k8s","architecture"],"title":"K8s Scheduler","uri":"/k8s-scheduler-introduce/"},{"categories":null,"content":"抢占调度 pkg/scheduler/scheduler.go#preempt 总结 kube-scheduler 工作流程 对给定的 pod，先列举所有可调度的 nodes，根据资源是否充足初步过滤；使用优先级算法对 nodes 进行计算排序；最终选择分数最高的一个 node 总的工作流程图： referenced from here Reference kube-scheduler workflow ","date":"2019-05-10","objectID":"/k8s-scheduler-introduce/:1:2","tags":["k8s","architecture"],"title":"K8s Scheduler","uri":"/k8s-scheduler-introduce/"},{"categories":null,"content":"文章简介：搭建 vscode + k8s 本地开发环境 ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:0:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"vagrant env Linux ubuntu-bionic 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:1:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"install golang wget https://dl.google.com/go/go1.12.5.linux-amd64.tar.gz tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz export GOPATH=$HOME/go export GOROOT=/usr/local/go export PATH=$PATH:$GOROOT/bin:\\$GOPATH/bin ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:2:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"install docker ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:3:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"disabled swap swapoff -a echo \"vm.swappiness = 0\"\u003e\u003e /etc/sysctl.conf sysctl -p ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:4:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"k8s ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:5:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"clone k8s source code mkdir -p $GOPATH/src/k8s.io cd $GOPATH/src/k8s.io/ git clone https://github.com/kubernetes/kubernetes.git ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:5:1","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"startup local cluster hack/install-etcd.sh export PATH=\"$GOPATH/src/github.com/kubernetes/kubernetes/third_party/etcd:${PATH}\" hack/local-up-cluster.sh [-0] open another cmd tag export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig cluster/kubectl.sh make WHAT=cmd/{\\$package_you_want} ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:5:2","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"see k8s office dev guide https://github.com/kubernetes/community/blob/master/contributors/devel/development.md GO111MODULE=on GOPROXY=https://goproxy.io go get github.com/go-delve/delve ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:6:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"others vscode, git ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:7:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"debug by dlv 同 golang + dlv + vscode 本地开发 ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:8:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"related links https://farmer-hutao.github.io/k8s-source-code-analysis/prepare/get-code.html 比较老得笔记 iptable 系列文章 ","date":"2019-05-10","objectID":"/k8s-vscode-dev-env-prepare/:9:0","tags":["k8s"],"title":"vscode 搭建 k8s 本地开发环境","uri":"/k8s-vscode-dev-env-prepare/"},{"categories":null,"content":"文章简介：k8s 架构描述 https://kubernetes.io/docs/concepts/ ","date":"2019-05-10","objectID":"/k8s-architecture-introduce/:0:0","tags":["k8s","architecture"],"title":"K8s 架构介绍","uri":"/k8s-architecture-introduce/"},{"categories":null,"content":"架构图 ","date":"2019-05-10","objectID":"/k8s-architecture-introduce/:1:0","tags":["k8s","architecture"],"title":"K8s 架构介绍","uri":"/k8s-architecture-introduce/"},{"categories":null,"content":"组件 etcd + docker/rkt/containerd + Kubernetes + kubelet + kube-proxy + kube-apiserver + kube-controller-manager + kube-scheduler ","date":"2019-05-10","objectID":"/k8s-architecture-introduce/:2:0","tags":["k8s","architecture"],"title":"K8s 架构介绍","uri":"/k8s-architecture-introduce/"},{"categories":null,"content":"基本概念 copied from here Cluster : 集群是指由 Kubernetes 使用一系列的物理机、虚拟机和其他基础资源来运行你的应用程序。 Node : 一个 node 就是一个运行着 Kubernetes 的物理机或虚拟机，并且 pod 可以在其上面被调度。. Pod : 一个 pod 对应一个由相关容器和卷组成的容器组 （了解 Pod 详情） Label : 一个 label 是一个被附加到资源上的键/值对，譬如附加到一个 Pod 上，为它传递一个用户自定的并且可识别的属性.Label 还可以被应用来组织和选择子网中的资源（了解 Label 详情） selector: 是一个通过匹配 labels 来定义资源之间关系得表达式，例如为一个负载均衡的 service 指定所目标 Pod.（了解 selector 详情） Replication Controller : replication controller 是为了保证一定数量被指定的 Pod 的复制品在任何时间都能正常工作.它不仅允许复制的系统易于扩展，还会处理当 pod 在机器在重启或发生故障的时候再次创建一个（了解 Replication Controller 详情） Service : 一个 service 定义了访问 pod 的方式，就像单个固定的 IP 地址和与其相对应的 DNS 名之间的关系。（了解 Service 详情） Volume: 一个 volume 是一个目录，可能会被容器作为未见系统的一部分来访问。（了解 Volume 详情） Kubernetes volume 构建在 Docker Volumes 之上,并且支持添加和配置 volume 目录或者其他存储设备。 Secret : Secret 存储了敏感数据，例如能允许容器接收请求的权限令牌。 Name : 用户为 Kubernetes 中资源定义的名字 Namespace : Namespace 好比一个资源名字的前缀。它帮助不同的项目、团队或是客户可以共享 cluster,例如防止相互独立的团队间出现命名冲突 Annotation : 相对于 label 来说可以容纳更大的键值对，它对我们来说可能是不可读的数据，只是为了存储不可识别的辅助数据，尤其是一些被工具或系统扩展用来操作的数据 ","date":"2019-05-10","objectID":"/k8s-architecture-introduce/:2:1","tags":["k8s","architecture"],"title":"K8s 架构介绍","uri":"/k8s-architecture-introduce/"},{"categories":null,"content":"related links Kubernetes 设计架构-完整文档 ","date":"2019-05-10","objectID":"/k8s-architecture-introduce/:3:0","tags":["k8s","architecture"],"title":"K8s 架构介绍","uri":"/k8s-architecture-introduce/"},{"categories":null,"content":"文章简介：Go 程序启动 RPC 子进程，通过 pipe 进行交互，以及通过 RPC 交互 代码见 exfly/go-ipc 引言 为什么要写这篇文章。最近看了一些 Docker 源码。dockerd 的架构长这个样子。 一条 docker 命令的执行，比如docker run，是先由 containerd 执行，containerd 也同样不是真正运行容器，他会将执行请求发给 runc，有 runc 真正去执行。dockerd、containderd、runc 分别是三个可执行文件，他们是通过 管道（IPC）以及 rest、RPC 进行交互的。 为了展示三者交互方式是如何进行的，这里写一个简单的 demo 来解释。 正文 ","date":"2019-05-05","objectID":"/go-rpc-ipc-demo/:0:0","tags":["go","IPC","RPC"],"title":"Go 启动多个程序，及 IPC 和 RPC 交互例子,以及 Gracefully Shutdown","uri":"/go-rpc-ipc-demo/"},{"categories":null,"content":"RPC 首先说一下 RPC。Go 标准库便有net/rpc，写一个 Go 的 rpc 很简单： package main import ( \"log\" \"net\" \"net/http\" \"net/rpc\" ) type Task []string type Todo {ID string} func (t Task) Get(id string, reply *Todo) error { *reply=Todo{ID:id} return nil } func main() { task := new(Task) // Publish the receivers methods err := rpc.Register(task) if err != nil { log.Fatal(\"Format of service Task isn't correct. \", err) } // Register a HTTP handler rpc.HandleHTTP() listener, e := net.Listen(\"unix\", \"rpc.sock\") if e != nil { log.Fatal(\"Listen error: \", e) } err = http.Serve(listener, nil) if err != nil { log.Fatal(\"Error serving: \", err) } } // client client, err := rpc.DialHTTP(\"unix\", \"rpc.sock\") reply := TODO{} err = client.Call(\"Task.Get\", \"new_id\", \u0026reply) ","date":"2019-05-05","objectID":"/go-rpc-ipc-demo/:1:0","tags":["go","IPC","RPC"],"title":"Go 启动多个程序，及 IPC 和 RPC 交互例子,以及 Gracefully Shutdown","uri":"/go-rpc-ipc-demo/"},{"categories":null,"content":"如何进行进程间通信 IPC 呢 首先 linux 下的 FIFO（有名管道）是什么样子的 mkfifo tpipe ll # total 8 # drwx------ 2 vagrant vagrant 4096 May 18 17:28 ./ # drwxrwxrwt 10 root root 4096 May 18 17:28 ../ # prw-rw-r-- 1 vagrant vagrant 0 May 18 17:28 tpipe| # 现在一个terminal cat tpipe # 另一个terminal echo tttttttttt \u003e tpipe 此时第一个 terminal 会输出 tttttttttt，另一个命令行会返回 在 Go 中应该如何使用 rpcSvrCmd := exec.Command(conf.RPCSvrBinPath) rpcSvrStdinPipe, err := rpcSvrCmd.StdinPipe() rpcSvrStdoutPipe, err := rpcSvrCmd.StdoutPipe() rpcSvrStderrPipe, err := rpcSvrCmd.StderrPipe() rpcSvrCmd.Start() 如此即可获得子进程的各种 FIFO 完整例子 代码见 exfly/go-ipc make task rpc \u0026\u0026 ./bin/task # ▶ running gofmt… # ▶ running golint… # ▶ building executable… # ▶ building executable… # 2019/05/19 01:33:12 sleep 1s to wait rpc server startup # 2019/05/19 01:33:13 2019/05/19 01:33:12 Serving RPC server on {unix rpc.sock} # 2019/05/19 01:33:13 Finish App: {Finish App Started} # 2019/05/19 01:33:13 2019/05/19 01:33:13 stop the server # 2019/05/19 01:33:13 2019/05/19 01:33:13 deleted socket file: rpc.sock # 2019/05/19 01:33:13 pipe rpc_srv_stderr has Closed # 2019/05/19 01:33:13 pipe rpc_srv_stdout has Closed # 2019/05/19 01:33:13 stop subproc ./bin/task-rpc success 当前更新主要以发布代码为主，没有详细解释，详细见代码 exfly/go-ipc ","date":"2019-05-05","objectID":"/go-rpc-ipc-demo/:2:0","tags":["go","IPC","RPC"],"title":"Go 启动多个程序，及 IPC 和 RPC 交互例子,以及 Gracefully Shutdown","uri":"/go-rpc-ipc-demo/"},{"categories":null,"content":"文章简介：描述 docker volume 的原理 docker 存储基于 UnionFS 实现，所有容器存储是将多个 layout 层通过类似 aufs 或者 overlay2 将多个层联合到一起，mount 成新的目录，在通过 namespaces 将新的文件目录 chroot 进入新启动的进程，达到文件隔离的目的。 对于读请求，由于写实复制技术，会直接读取底层文件 对于写请求，会先将文件复制到读写层，然后进行修改文件 对于删除，没有真正删除文件，只是讲文件标记删除，没有真正删除下层文件。 命令 docker volume create volume-for-test可以创建新的 volume docker volume ls 查看新创建的 volume docker volume inspect volume-for-test docker run -d --name devtest --mount source=volume-for-test,target=/app alpine /bin/sh docker inspect \u003ccontainer_id\u003e| grap Mounts 可以看到刚刚 mount 进去的 volume /var/lib/docker/volumes/volume-for-test/_data related links Manage data in Docker Use volumes docker 存储基础 ","date":"2019-05-05","objectID":"/docker-volume/:0:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Volume","uri":"/docker-volume/"},{"categories":null,"content":"文章简介：docker build 源码阅读 阅读源码顺序 api/server/router/build/build_routes.go#postBuild api/server/backend/build/backend.go#Build builder/builder-next/builder.go#Build builder/dockerfile/builder.go#Build builder/dockerfile/builder.go#dispatchDockerfileWithCancellation builder/dockerfile/evaluator.go#dispatch，在这里，有所有命令的执行方式，可以仔细研究一下 在dispatch中有 dockerfile 支持的指令，如RUN等。以 RUN 为例，dockerd 会读取 CLI 发来的 dockerfile，解析后。如果为 RUN，则会启动一个新的 container,然后在容器中执行，执行结束后将当前层 commit，继续执行下一个指令. 其他 从 v18.09开始，docker build 依赖于 buildkit ","date":"2019-05-05","objectID":"/docker-build/:0:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Build","uri":"/docker-build/"},{"categories":null,"content":"文章简介：通过分析 docker run 命令，理解 Docker 的工作原理 docker run执行流程分析 整体执行流程: 从本地镜像中寻找是否存在命令指定镜像，如果存在则正常返回，执行接下来流程。如果不存在镜像，deamon 返回错误，由 cli 重新发起 pull 请求，之后重试。 ","date":"2019-04-29","objectID":"/docker-run/:0:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Run","uri":"/docker-run/"},{"categories":null,"content":"总体描述 createContainer cli 通过 rest 接口请求 dockerd 创建容器，dockerd 设置基础配置后记录创建的容器 ContainerStart cli 通过 rest 接口请求 dockerd 运行容器，dockerd 使用 rpc 链接 containnerd 进行运行容器 代码 lanything/code-read forked moby/moby, lanything/cli forked docker/cli ","date":"2019-04-29","objectID":"/docker-run/:0:1","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Run","uri":"/docker-run/"},{"categories":null,"content":"createContainer docker/cli cli/command/container/run.go#NewRunCommand docker/cli cli/command/container/run.go#runContainer // cli/command/container/create.go#createContainer func createContainer(ctx context.Context, dockerCli command.Cli, containerConfig *containerConfig, opts *createOptions) (*container.ContainerCreateCreatedBody, error) { config := containerConfig.Config hostConfig := containerConfig.HostConfig networkingConfig := containerConfig.NetworkingConfig stderr := dockerCli.Err() warnOnOomKillDisable(*hostConfig, stderr) warnOnLocalhostDNS(*hostConfig, stderr) var ( trustedRef reference.Canonical namedRef reference.Named ) containerIDFile, err := newCIDFile(hostConfig.ContainerIDFile) if err != nil { return nil, err } defer containerIDFile.Close() ref, err := reference.ParseAnyReference(config.Image) if err != nil { return nil, err } if named, ok := ref.(reference.Named); ok { namedRef = reference.TagNameOnly(named) if taggedRef, ok := namedRef.(reference.NamedTagged); ok \u0026\u0026 !opts.untrusted { var err error trustedRef, err = image.TrustedReference(ctx, dockerCli, taggedRef, nil) if err != nil { return nil, err } config.Image = reference.FamiliarString(trustedRef) } } //create the container response, err := dockerCli.Client().ContainerCreate(ctx, config, hostConfig, networkingConfig, opts.name) //if image not found try to pull it if err != nil { if apiclient.IsErrNotFound(err) \u0026\u0026 namedRef != nil { fmt.Fprintf(stderr, \"Unable to find image '%s' locally\\n\", reference.FamiliarString(namedRef)) // we don't want to write to stdout anything apart from container.ID if err := pullImage(ctx, dockerCli, config.Image, opts.platform, stderr); err != nil { return nil, err } if taggedRef, ok := namedRef.(reference.NamedTagged); ok \u0026\u0026 trustedRef != nil { if err := image.TagTrusted(ctx, dockerCli, trustedRef, taggedRef); err != nil { return nil, err } } // Retry var retryErr error response, retryErr = dockerCli.Client().ContainerCreate(ctx, config, hostConfig, networkingConfig, opts.name) if retryErr != nil { return nil, retryErr } } else { return nil, err } } for _, warning := range response.Warnings { fmt.Fprintf(stderr, \"WARNING: %s\\n\", warning) } err = containerIDFile.Write(response.ID) return \u0026response, err } docker/cli cli/command/container/create.go#createContainer 调用 ContainerCreate moby/moby api/server/router/container/container.go#initRoutes.postContainersCreate moby/moby api/server/router/container/container_routes.go#postContainersCreate moby/moby daemon/create.go#ContainerCreate ","date":"2019-04-29","objectID":"/docker-run/:0:2","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Run","uri":"/docker-run/"},{"categories":null,"content":"containerCreate // `moby/moby` daemon/create.go#containerCreate func (daemon *Daemon) containerCreate(opts createOpts) (containertypes.ContainerCreateCreatedBody, error) { start := time.Now() if opts.params.Config == nil { return containertypes.ContainerCreateCreatedBody{}, errdefs.InvalidParameter(errors.New(\"Config cannot be empty in order to create a container\")) } os := runtime.GOOS if opts.params.Config.Image != \"\" { img, err := daemon.imageService.GetImage(opts.params.Config.Image) if err == nil { os = img.OS } } else { // This mean scratch. On Windows, we can safely assume that this is a linux // container. On other platforms, it's the host OS (which it already is) if runtime.GOOS == \"windows\" \u0026\u0026 system.LCOWSupported() { os = \"linux\" } } warnings, err := daemon.verifyContainerSettings(os, opts.params.HostConfig, opts.params.Config, false) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } err = verifyNetworkingConfig(opts.params.NetworkingConfig) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } if opts.params.HostConfig == nil { opts.params.HostConfig = \u0026containertypes.HostConfig{} } err = daemon.adaptContainerSettings(opts.params.HostConfig, opts.params.AdjustCPUShares) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err) } container, err := daemon.create(opts) if err != nil { return containertypes.ContainerCreateCreatedBody{Warnings: warnings}, err } containerActions.WithValues(\"create\").UpdateSince(start) if warnings == nil { warnings = make([]string, 0) // Create an empty slice to avoid https://github.com/moby/moby/issues/38222 } return containertypes.ContainerCreateCreatedBody{ID: container.ID, Warnings: warnings}, nil } ","date":"2019-04-29","objectID":"/docker-run/:0:3","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Run","uri":"/docker-run/"},{"categories":null,"content":"GetImages // GetImage returns an image corresponding to the image referred to by refOrID. func (i *ImageService) GetImage(refOrID string) (*image.Image, error) { ref, err := reference.ParseAnyReference(refOrID) if err != nil { return nil, errdefs.InvalidParameter(err) } namedRef, ok := ref.(reference.Named) if !ok { digested, ok := ref.(reference.Digested) if !ok { return nil, ErrImageDoesNotExist{ref} } id := image.IDFromDigest(digested.Digest()) if img, err := i.imageStore.Get(id); err == nil { return img, nil } return nil, ErrImageDoesNotExist{ref} } if digest, err := i.referenceStore.Get(namedRef); err == nil { // Search the image stores to get the operating system, defaulting to host OS. id := image.IDFromDigest(digest) if img, err := i.imageStore.Get(id); err == nil { return img, nil } } // Search based on ID if id, err := i.imageStore.Search(refOrID); err == nil { img, err := i.imageStore.Get(id) if err != nil { return nil, ErrImageDoesNotExist{ref} } return img, nil } return nil, ErrImageDoesNotExist{ref} } ImageService.imageStore 在mobydaemon/daemon.go#NewDaemon 中有设置 ImageService.imageStore 在mobyimage/store.go#NewImageStore 有初始化 ","date":"2019-04-29","objectID":"/docker-run/:0:4","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Run","uri":"/docker-run/"},{"categories":null,"content":"ContainerStart moby daemon/start.go#ContainerStart func (daemon *Daemon) ContainerStart(name string, hostConfig *containertypes.HostConfig, checkpoint string, checkpointDir string) error { if checkpoint != \"\" \u0026\u0026 !daemon.HasExperimental() { return errdefs.InvalidParameter(errors.New(\"checkpoint is only supported in experimental mode\")) } container, err := daemon.GetContainer(name) if err != nil { return err } validateState := func() error { container.Lock() defer container.Unlock() if container.Paused { return errdefs.Conflict(errors.New(\"cannot start a paused container, try unpause instead\")) } if container.Running { return containerNotModifiedError{running: true} } if container.RemovalInProgress || container.Dead { return errdefs.Conflict(errors.New(\"container is marked for removal and cannot be started\")) } return nil } if err := validateState(); err != nil { return err } // Windows does not have the backwards compatibility issue here. if runtime.GOOS != \"windows\" { // This is kept for backward compatibility - hostconfig should be passed when // creating a container, not during start. if hostConfig != nil { logrus.Warn(\"DEPRECATED: Setting host configuration options when the container starts is deprecated and has been removed in Docker 1.12\") oldNetworkMode := container.HostConfig.NetworkMode if err := daemon.setSecurityOptions(container, hostConfig); err != nil { return errdefs.InvalidParameter(err) } if err := daemon.mergeAndVerifyLogConfig(\u0026hostConfig.LogConfig); err != nil { return errdefs.InvalidParameter(err) } if err := daemon.setHostConfig(container, hostConfig); err != nil { return errdefs.InvalidParameter(err) } newNetworkMode := container.HostConfig.NetworkMode if string(oldNetworkMode) != string(newNetworkMode) { // if user has change the network mode on starting, clean up the // old networks. It is a deprecated feature and has been removed in Docker 1.12 container.NetworkSettings.Networks = nil if err := container.CheckpointTo(daemon.containersReplica); err != nil { return errdefs.System(err) } } container.InitDNSHostConfig() } } else { if hostConfig != nil { return errdefs.InvalidParameter(errors.New(\"Supplying a hostconfig on start is not supported. It should be supplied on create\")) } } // check if hostConfig is in line with the current system settings. // It may happen cgroups are umounted or the like. if _, err = daemon.verifyContainerSettings(container.OS, container.HostConfig, nil, false); err != nil { return errdefs.InvalidParameter(err) } // Adapt for old containers in case we have updates in this function and // old containers never have chance to call the new function in create stage. if hostConfig != nil { if err := daemon.adaptContainerSettings(container.HostConfig, false); err != nil { return errdefs.InvalidParameter(err) } } return daemon.containerStart(container, checkpoint, checkpointDir, true) } moby daemon/start.go#containerStart // containerStart prepares the container to run by setting up everything the // container needs, such as storage and networking, as well as links // between containers. The container is left waiting for a signal to // begin running. func (daemon *Daemon) containerStart(container *container.Container, checkpoint string, checkpointDir string, resetRestartManager bool) (err error) { start := time.Now() container.Lock() defer container.Unlock() if resetRestartManager \u0026\u0026 container.Running { // skip this check if already in restarting step and resetRestartManager==false return nil } if container.RemovalInProgress || container.Dead { return errdefs.Conflict(errors.New(\"container is marked for removal and cannot be started\")) } if checkpointDir != \"\" { // TODO(mlaventure): how would we support that? return errdefs.Forbidden(errors.New(\"custom checkpointdir is not supported\")) } // if we encounter an error during start we need to ensure that any other // setup has been cleaned up properly defer func() { if err != nil { container.SetError(err) // if no one else has set it, make","date":"2019-04-29","objectID":"/docker-run/:0:5","tags":["docker","源码"],"title":"Docker 源码阅读: Docker Run","uri":"/docker-run/"},{"categories":null,"content":"文章简介：介绍 docker 开发环境搭建 docker 官方的贡献引导: moby/docs/contributing/README.md 步骤 moby/docs/contributing/set-up-dev-env.md完整的描述了开发环境如何搭建 ","date":"2019-04-23","objectID":"/docker-source-dev-install/:0:0","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"开发环境搭建 因为网络问题，需要尽快的提速安装 dev 环境，需要配置 Dockerfile 中的配置 APT_MIRROR=mirrors.163.com 运行make --just-print BIND_DIR=. shell简单的看一下 make 都做了那些事情 运行make BIND_DIR=. shell开始安装开发环境 ","date":"2019-04-23","objectID":"/docker-source-dev-install/:0:1","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"编译 上一节已经进入 docker 中，可以开始编译 dockerd hack/make.sh binary make installcopy binary to container’s /usr/local/bin/ dockerd -D \u0026 ","date":"2019-04-23","objectID":"/docker-source-dev-install/:0:2","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"日常工作流 修改代码 hack/make.sh binary install-binary ","date":"2019-04-23","objectID":"/docker-source-dev-install/:0:3","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"常用目录/命令 docker inspect 查看镜像或容器运行配置信息，GraphDriver 为容器挂载信息 /var/lib/docker/overlay2/\u003cimage_id\u003e/{merged, diff, work} /var/lib/docker/containers当前运行中的容器配置信息 /var/lib/docker/image镜像库 /var/lib/docker/volumesvolumes 储存位置 调试 ","date":"2019-04-23","objectID":"/docker-source-dev-install/:0:4","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"调试 makefile make --just-print BIND_DIR=. shell ","date":"2019-04-23","objectID":"/docker-source-dev-install/:1:0","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"调试 shell bash -x hack/make.sh binary ","date":"2019-04-23","objectID":"/docker-source-dev-install/:2:0","tags":["docker","container","源码"],"title":"Docker 源码阅读: 开发环境搭建","uri":"/docker-source-dev-install/"},{"categories":null,"content":"文章简介：docker基本架构，以及在 moby 开源项目相关的一些组件之间如何协同工作的 Docker? ","date":"2019-04-22","objectID":"/docker-architecture/:0:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"我们可以使用 Docker 做什么 快速，一致地交付您的应用程序 Docker 允许开发人员使用提供应用程序和服务的本地容器在标准化环境中工作，从而简化了开发生命周期。 容器非常适合持续集成和持续交付（CI / CD）工作流程。 docker 全局架构 CLI 使用 Docker REST API 通过脚本或直接 CLI 命令控制 Docker 守护程序或与 Docker 守护程序交互。 许多其他 Docker 应用程序使用底层 API 和 CLI。 Docker daemon 创建和管理 Docker 对象，例如 images，containers，networks 和 volumes。 ","date":"2019-04-22","objectID":"/docker-architecture/:1:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"The Docker daemon Docker daemon（dockerd）监听 Docker API 请求并管理 Docker 对象，例如 images，containers，networks 和 volumes。 Docker daemon 还可以与其他守护程序通信以管理 Docker 服务。 Note: 例如 dockerd 通过 rest 接口与 containnerd 进行交互，containerd 与 runc 通过 grpc 进行通信 ","date":"2019-04-22","objectID":"/docker-architecture/:2:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"The Docker client Docker 客户端（docker）是许多 Docker 用户与 Docker 交互的主要方式。 当您使用诸如 docker run 之类的命令时，客户端会将这些命令发送到 dockerd，dockerd 将其执行。 docker (CLI) 命令调用 REST 风格的 Docker API。 Docker 客户端可以与多个守护进程通信。 Note: docker 可以通过配置环境变量 DOCKER_HOST或者修改配置变量，或者命令行参数的方式连接 dockerd ","date":"2019-04-22","objectID":"/docker-architecture/:3:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"Docker registries Docker registry 存储 Docker 镜像。 Docker Hub 是任何人都可以使用的公共注册中心，Docker 配置为默认在 Docker Hub 上查找 images。 您甚至可以运行自己的私人 registry。 如果您使用 Docker Datacenter（DDC），它包含 Docker Trusted Registry（DTR）。 Note: 可以通过配置不同的 Docker registry 作为 images 下载源，比如公司内部搭建 registry 私服，在平时 CI/CD 中提高工作速度。 ","date":"2019-04-22","objectID":"/docker-architecture/:4:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"Docker objects 使用 Docker 时，您正在创建和使用 images，containers，networks 和 volumes，plugins 和其他对象。 本节简要介绍其中一些对象。 ","date":"2019-04-22","objectID":"/docker-architecture/:5:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"IMAGES images 是一个只读模板，其中包含有关创建 Docker 容器的说明。 通常，images 基于另一个 images，并带有一些额外的自定义。 例如，您可以构建基于 ubuntu images 的 images，但安装 Apache Web 服务器和应用程序，以及运行应用程序所需的配置详细信息。 您可以创建自己的 images，也可以只使用其他人创建的 images 并在 registries 中发布。 要构建自己的 images，可以使用简单的语法创建 Dockerfile，以定义创建 images 并运行 images 所需的步骤。 Dockerfile 中的每条指令都在 images 中创建一个 layer。 更改 Dockerfile 并重建 images 时，仅重建已更改的那些层。 与其他虚拟化技术相比，这是使 images 如此轻量，小巧和快速的部分原因。 Note: 比较详细的工作原理可以看下文 Union file systems部分 ","date":"2019-04-22","objectID":"/docker-architecture/:5:1","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"CONTAINERS CONTAINERS 是 images 的可运行实例。 您可以使用 Docker API 或 CLI 创建，启动，停止，移动或删除容器。 您可以将容器连接到一个或多个网络，将存储连接到它，甚至可以根据其当前状态创建新 images。 默认情况下，CONTAINERS 与其他 CONTAINERS 及其主机相对隔离。 您可以控制 CONTAINERS 的网络，存储或其他基础子系统与其他 CONTAINERS 或主机的隔离程度。 CONTAINERS 由其 images 以及您在创建或启动时为其提供的任何配置选项定义。 删除 containers 后，对其状态的任何未存储在持久存储中的更改都将消失。 ","date":"2019-04-22","objectID":"/docker-architecture/:5:2","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"SERVICES services 允许您跨多个 Docker 守护程序扩展容器，这些守护程序一起作为具有多个管理器和工作程序的群组一起工作。 swarm 的每个成员都是 Docker 守护程序，守护进程都使用 Docker API 进行通信。 服务允许您定义所需的状态，例如在任何给定时间必须可用的服务的副本数。 默认情况下，服务在所有工作节点之间进行负载平衡。 对于消费者来说，Docker 服务似乎是一个单独的应用程序。 Docker Engine 支持 Docker 1.12 及更高版本中的 swarm 模式。 Docker 底层技术 Docker 是用 Go 编写的，它利用 Linux 内核的几个功能来提供其功能。使用到的内核特性包括 namespaces、cgroups、Union file systems ","date":"2019-04-22","objectID":"/docker-architecture/:5:3","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"namespaces Docker 使用称为 namespaces 的技术来提供称为容器的隔离工作空间。 运行容器时，Docker 会为该容器创建一组 namespaces。 这些 namespaces 提供了一层隔离。 容器的每个方面都在一个单独的命名空间中运行，其访问权限仅限于该 namespaces。 Docker Engine 在 Linux 上使用以下命名空间： pid 命名空间：进程隔离（PID：进程 ID）。 net 命名空间：管理网络接口（NET：Networking）。 ipc 名称空间：管理对 IPC 资源的访问（IPC：进程间通信）。 mnt 名称空间：管理文件系统挂载点（MNT：Mount）。 uts 命名空间：隔离内核和版本标识符。 （悉尼科技大学：Unix 分时系统）。 ","date":"2019-04-22","objectID":"/docker-architecture/:6:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"cgroups Linux 上的 Docker Engine 还依赖于另一种称为控制组（cgroups）的技术。 cgroup 将应用程序限制为特定的资源集。 cgroups 允许 Docker Engine 将可用的硬件资源共享给容器，并可选择强制执行限制和约束。 例如，您可以限制特定容器的可用内存。 ","date":"2019-04-22","objectID":"/docker-architecture/:7:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"Union file systems 联合文件系统或 UnionFS 是通过创建 layers 来操作的文件系统，使它们非常轻量和快速。 Docker Engine 使用 UnionFS 为容器提供构建块。 Docker Engine 可以使用多种 UnionFS 变体，包括 AUFS，btrfs，vfs 和 DeviceMapper。 Note: Docker 默认为 overylay2 ","date":"2019-04-22","objectID":"/docker-architecture/:8:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"Container format Docker Engine 将 namespaces，cgroups 和 UnionFS 组合成一个称为容器格式的包装器。 默认容器格式是 libcontainer。 将来，Docker 可以通过与 BSD Jails 或 Solaris Zones 等技术集成来支持其他容器格式。 Note: 如上部分翻译自 docker overview，同时添加自己本人理解。 moby 等代码的依赖介绍，相关调用方式，架构 Docker 从一个单一的软件转移到一组独立的组件和项目。 ","date":"2019-04-22","objectID":"/docker-architecture/:9:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"Docker 如何运行容器？ Docker 引擎创建 images， 把 images 传递给 containerd， containerd 调用 containerd-shim， containerd-shim 使用 runC 来运行 container， containerd-shim 允许运行时（在本例中为 runC）在启动容器后退出 ","date":"2019-04-22","objectID":"/docker-architecture/:10:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"这个模型的两个主要好处是 deamon 运行较少的容器 能够在不破坏正在运行的容器的情况下重启或升级引擎 ","date":"2019-04-22","objectID":"/docker-architecture/:11:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"containerd containerd 架构 related link moby libnetwork containerd runc Visualizing Docker Containers and Images ","date":"2019-04-22","objectID":"/docker-architecture/:12:0","tags":["docker","源码"],"title":"Docker 源码阅读: Docker架构介绍","uri":"/docker-architecture/"},{"categories":null,"content":"文章简介：资料汇总以及源码阅读目录 Contents Docker 架构介绍 Docker 编译开发环境搭建 Docker Run Docker Build Docker Volume todo docker network 资料汇总 Visualizing Docker Containers and Images容器、镜像可视化 Docker 核心技术与实现原理 docker-debug related links Visualizing Docker Containers and Images ","date":"2019-04-22","objectID":"/docker-contents/:0:0","tags":["docker","源码"],"title":"Docker 源码阅读: 目录","uri":"/docker-contents/"},{"categories":null,"content":"文章简介：如何参与开源项目。在这里分享一些思路和开源资源。 ","date":"2019-04-21","objectID":"/contribute-to-opensource/:0:0","tags":["howto"],"title":"如何参与开源项目","uri":"/contribute-to-opensource/"},{"categories":null,"content":"简介 自从学习计算机开始，很多时候希望自己能够也为 opensource 贡献一些什么。这里会总结一些思路为开源做些什么。 ","date":"2019-04-21","objectID":"/contribute-to-opensource/:1:0","tags":["howto"],"title":"如何参与开源项目","uri":"/contribute-to-opensource/"},{"categories":null,"content":"思路 Start Your Open Source Career这里简述了如何参与开源项目。对自己有很多启示。我们在工作学习中也会有一些自己感觉很好的对某个技术问题的解决方式,希望可以分享给大家，或者希望学习新的知识，成为某个工具的核心维护者。 这里会总结一些比较好的参与开源项目的思路。 ","date":"2019-04-21","objectID":"/contribute-to-opensource/:2:0","tags":["howto"],"title":"如何参与开源项目","uri":"/contribute-to-opensource/"},{"categories":null,"content":"good first issue or help wanted 很多开源项目的issue中已经标记出很多类似good first issue or help wanted的 label，这些 label 表示新人可以来帮忙。可以通过一些网站找到打相应 label 的项目，这可能成为你贡献开源项目的开端。 或许这些网站可以帮到你(来自 github)： 开源星期五 - opensourcefriday 统计自己参与的项目，同时推荐如何开始 github-explore github 会为你推荐一些你感兴趣的项目 project-based-learning Curated list of project-based tutorials first timers only 贡献 pr 需要的 git 知识，label 搜索，相关订阅提醒等 codetriage 订阅 github 中的项目，issue 等，方便通知自己感兴趣的项目 issuehub 按照标签搜索项目 pullrequestroulette 检索需要 reviews 的 pr up-for-grabs.net 根据 label 等检索项目 how-to-contribute/#a-checklist-before-you-contribute ","date":"2019-04-21","objectID":"/contribute-to-opensource/:2:1","tags":["howto"],"title":"如何参与开源项目","uri":"/contribute-to-opensource/"},{"categories":null,"content":"思路 构建一些工具 比如构建一些项目模版，比如graphql+mongoboilerplate. 比如编写一些平时可以提高工作效率的工具，alibaba/arthas 成为新的维护者 有很多有价值的项目因为没有维护者渐渐被人遗弃。你是否可以成为新的维护者呢？可以通过邮件、twiter 等联系原作者，成为项目维护者是不是很棒？ 创建自己的项目 如果自己有对新的技术问题的解决办法，可以开源出来，分享自己是如何解决的 发布，推广，分享 为了确保每个有需要的人都乐意来找到你的模块，你必须： 撰写 readme 等 license README 版本徽章 贡献指南 提供ISSUE_TEMPLATE 使用本项目的产品 为项目撰写精心设计的在线网站，和文档，可以使用静态网站工具生成，如vuepress 在 StackOverflow 和 GitHub 等社交媒体中寻找相关问题并贴出你的项目，并解答 将项目发布到汇集开源项目的社区中，如HackerNews、reddit、producthunt、hashnode 参与一些线下分享、讨论会、演讲等中介绍你的项目 ","date":"2019-04-21","objectID":"/contribute-to-opensource/:2:2","tags":["howto"],"title":"如何参与开源项目","uri":"/contribute-to-opensource/"},{"categories":null,"content":"链接 octobox 将你的 GitHub 通知转成邮件的形式，这是避免因堆积「太多问题」以至于影响关注重要问题的很好的方法 probot GitHub App 可以自动化和改善你的工作流程 refined-github 浏览器扩展，简化了 GitHub 界面并添加了有用的功能 Start Your Open Source Career mattermost-server project-based-learning#go awesome-for-beginners merging-vs-rebasing ","date":"2019-04-21","objectID":"/contribute-to-opensource/:3:0","tags":["howto"],"title":"如何参与开源项目","uri":"/contribute-to-opensource/"},{"categories":null,"content":"swagger and openapi 代码生成和文档自动生成一些体会 说在前面 平时使用gqlgen，一般的 workflow 是先写 graphql 的 schema，然后 code generate 对应的 model 和 api 的实现的空接口，自己对应实现对应的 resolver 即可。使用起来很流程。最近调研一下 java 下类似的工具。找到在 java 下 star 最多的项目graphql-java，但并不是这里讨论的。 这里讨论的是基于 spring 的 openapi 的实现和 code generate 方案。 基于 openapi 的 code generate 方案 首先简单介绍一下 openapi，他是语言无关的 restful 描述语言，可以使用 yaml 进行编写接口文档，通过 generate，生成不同语言的 client 和 server。这里有官方的简介。自己比较关注的语言是 python、java、go、rust，js。 踩到的一些坑，这里简单试用了一下 generate java。首先，maven 下有对应的 openapi 的 plugin，openapi-generator-maven-plugin, 但每次 compile 都会进行 generate，并不是我所希望的，所以这里使用了基于 docker 的使用方案。代码见exfly:gorgestar/isn,使用generator.sh生成对应的代码，配置文件可以看generator.json， 我的使用策略是，先修改openapi.yaml，创建或者修改 restful api，然后 bash ./generator.sh，生成对应接口默认空实现，之后由我们对应的实现接口即可。 一些自己没有进行操作的方案： 如何保证 api 和接口文档一致：可以对应的使用 swagger 的 api-doc json 进行转换为 yaml，对原本手写的openapi.yaml进行比较，确认文档的一致性 这个版本的 generate 每次都会将所有的文件覆盖，需要编辑.openapi-generator-ignore，类似 gitignore 的东西，类比修改即可。被忽略的文件，即使被删除，也不会自动生成对应文件，生成逻辑看起来比较傻。 其他资源 exfly:gorgestar/isn,本文项目代码 swagger raml OpenAPI-Specification openapi-generator openapi generator maven plugin spring rest docs ","date":"2019-02-04","objectID":"/openapi-gen-server-client/:0:0","tags":["openapi","codegen"],"title":"Openapi 代码生成 server/client 代码","uri":"/openapi-gen-server-client/"},{"categories":null,"content":"win 子系统安装与 cmder+zsh 开发环境搭建 ","date":"2018-12-08","objectID":"/window-install-wsl-and-usages/:0:0","tags":["工具","wsl","windows"],"title":"WSL(windows subsystem for linux) win 子系统安装与基本使用注意事项","uri":"/window-install-wsl-and-usages/"},{"categories":null,"content":"说在前面 这里只展示可以做到什么程度，具体怎么做，网上教程很多，后边会贴出自己感觉比较好的网址 ","date":"2018-12-08","objectID":"/window-install-wsl-and-usages/:1:0","tags":["工具","wsl","windows"],"title":"WSL(windows subsystem for linux) win 子系统安装与基本使用注意事项","uri":"/window-install-wsl-and-usages/"},{"categories":null,"content":"大致操作思路 先在 windows 下打开 win subsystem for linux 功能，之后去 win store 中下载对应的 linux 发行版.之后就是打开对应的 linux 发行版的 bash、配置 zsh 了，详细步骤见这里。具体 zsh 怎么折腾，可以看一下这里 贴一张自己配置之后，使用 zsh 和 tmux 之后的截图 给我的体验是，基本可以满足大部分日常开发工作 ","date":"2018-12-08","objectID":"/window-install-wsl-and-usages/:2:0","tags":["工具","wsl","windows"],"title":"WSL(windows subsystem for linux) win 子系统安装与基本使用注意事项","uri":"/window-install-wsl-and-usages/"},{"categories":null,"content":"其他 win 下的 CDEF 盘被挂载到/mnt下，为了方便使用，可以将他们ln -s /mnt/d $HOME/windir，这样方便自己使用 因为之前安装过 vscode，可以直接使用code filename 打开系统中的文件 类似 jdk 这种需要在 subsystem 中重新安装才可以在 wsl 中使用 图中使用的 Cmder 是非常远古的版本，所以请忽略 cmd 之前的框 ","date":"2018-12-08","objectID":"/window-install-wsl-and-usages/:2:1","tags":["工具","wsl","windows"],"title":"WSL(windows subsystem for linux) win 子系统安装与基本使用注意事项","uri":"/window-install-wsl-and-usages/"},{"categories":null,"content":"链接 Windows10 终端优化方案：Ubuntu 子系统+cmder+oh-my-zsh ","date":"2018-12-08","objectID":"/window-install-wsl-and-usages/:3:0","tags":["工具","wsl","windows"],"title":"WSL(windows subsystem for linux) win 子系统安装与基本使用注意事项","uri":"/window-install-wsl-and-usages/"},{"categories":null,"content":"使用 Vue 前后端+Go 后端，基于 webpack 代理转发，配置前后端分离架构开发环境 原文地址 Web 研发模式演变 最近研究一下前后端的开发模式，看到一个很好的入门路径developer-roadmap: frontend backend DevOps,可以看一下，效果还是不错的。 之前看到一个说web 研发演进这里总结一下。 很久之前，前后端的分工是，前端从设计师那里拿到设计图纸，转化静态页面模板，由后端工程师进行数据库设计等一系列设计之后，套前端给的模板。如上也即后端渲染。 但是这样的流程，所有工作的 Block 在后端，想进一步提高研发速度，应该如何分工？到如今给出的答案是基于 Nodejs 的前后端分离架构。这时前后端分工是这样的： ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:0:0","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"前端的工作 UI 设计 前端路由设计 处理浏览器层的展现逻辑 通过 CSS 渲染样式，通过 JavaScript 添加交互功能，HTML 的生成也可以放在这层，具体看应用场景 ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:1:0","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"后端的工作 业务逻辑和 API 的设计和实现 数据库设计和维护 后端缓存设计 ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:2:0","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"前后端分离下协作体系 前后端分离下的协作方式一般是，前后端各司其职，互不影响。 首先，对于后端来说，后端的主要工作依然是传统的数据库设计、业务逻辑设计，但不需要套模板了，而是为前端提供数据接口 其次，对于前端来说，前端的主要工作是，前端的 ui，以及获取数据，在前端渲染。 工作流程是，先进行 API 设计。前后端一起设计数据接口以及数据返回的格式，现在比较常见的是 json 数据。可以根据接口生成一些 mock 用的 json 数据文件，供前端开发使用。后端根据这个 API 规范实现真正的接口。两端分别并行开发。开发结束时候联调，打通前后端之后进行调试。 具体，可以看一下网易前后端分离实践. 基于 Vue 前后端分离环境搭建 这里对前后端分离 Vue 的开发环境进行演示。思路是，前后端分离，后端可以设置 cookie，前端可以接收到配置的 cookie ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:3:0","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"node 环境安装 安装方法见这里 ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:3:1","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"Vue 安装 npm config set registry 'https://registry.npm.taobao.org' npm install -g @vue/cli vue init webpack demo cd demo npm dev run 即可在浏览器中看到效果，熟悉的 vue 页面 ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:3:2","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"安装 axios，实现前后端交互，并实现后端设置 cookie，在前端可以生效 安装 axios npm install axios 修改/src/components/HelloWorld.vue 中对应的 srcipt \u003cscript\u003e import axios from 'axios' axios.get('/sc') // 使用ajax export default { name: 'HelloWorld', data () { return { msg: 'Welcome to Your Vue.js App' } } } \u003c/script\u003e 最终完成的时候，访问 ‘/’，可以看到 cookie 添加了一个 kv 对。现在暂时看不到效果，因为接口后端没有实现。 后端接口实现,这里使用的 go，具体 go 编译器的安装方法见这里 package main import ( \"log\" \"net/http\" \"time\" ) func LoggingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { log.Println(\"-\u003e\", r.URL) next.ServeHTTP(w, r) log.Println(\"\u003c-\") }) } func main() { http.Handle(\"/sc\", LoggingMiddleware(http.HandlerFunc(indexHandler))) port := \":8081\" log.Println(\"starting on http://localhost\" + port) log.Fatal(http.ListenAndServe(port, nil)) } func indexHandler(w http.ResponseWriter, req *http.Request) { expire := time.Now().AddDate(0, 0, 1) cookie := http.Cookie{Name: \"csrftoken\", Value: expire.String(), Expires: expire} http.SetCookie(w, \u0026cookie) } 此时前后端都已经实现了，但是因为前端开在了端口 8080，后端开在了 8081，涉及到跨域，相互没法访问。需要配置一下 webpack 的配置才可以。 // 修改一下文件/config/index.js: 13 line proxyTable: { '/': { target: 'http://localhost:8081', changeOrigin: true } }, 之后，开启前后端服务： npm dev run go run server.go 之后访问前端页面，localhost:8080，按 F12 -\u003e Application -\u003e Cookies， 即可看到每次刷新都会改变的 csrftoken 的 cookie 最终的网站见ExFly/FrontBackSep 后记 既然在前后端分离后的后端可以配置cookie，其他的所有操作都可以进行了。进一步可以实现其他的操作。如上。 ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:3:3","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"引用 developer-roadmap: frontend backend DevOps web 研发演进 网易前后端分离实践 ","date":"2018-10-28","objectID":"/front-end-separation-architecture-environment-construction/:4:0","tags":["前后端分离","Vue"],"title":"前后端分离架构的Vue环境搭建指南","uri":"/front-end-separation-architecture-environment-construction/"},{"categories":null,"content":"总结一下数据库、分布式等技术产生的场景的使用场景。包括数据库、缓存、分布式、容器 从使用到的技术来说，如今各大网站平台趋向于使用经过长期检验过的技术，包括不限于动态网站技术、数据库技术、高速缓存技术、负载均衡、分布式相关技术。 首先动态网站技术，开始没有动态网站时期，大部分 BBS 使用基于 Telnet 协议为基础的 BBS 进行交流。HTTP 协议的出现，使得 BS 架构的网站能够展现多媒体等信息如视频、音乐等。起初 HTTP 协议一般仅展示一些静态数据，其表现能力不强，不能更好的动态的为用户交互。之后动态网站技术如 CGI 的出现，使得网站拥有的更加丰富的交互功能。经过数十年的技术积累，动态网站技术已经能够支撑起现如今的大部分信息的获取。 其次是存储技术。开始的时候数据仅仅使用本地文件进行存储。文件的读写效率很高，但是需要程序员重复的使用操作系统提供的较底层的操作来操作文件，对程序员的要求太大。而且对这种结构化的数据来说，可以使用相同的模式进行操作。所以有了数据库关系系统，尤其关系型数据库。从此，对于一些结构化的数据只需要使用 SQL 这种数据操作语言，就可以很方便的操作数据，而真正的数据管理维护工作由数据库管理系统进行维护，人可以通过配置数据库，使数据库获得更好的性能。当用户量很大时候，单机响应出现瓶颈，单台数据库不能够满足这样的请求。可以通过复制的方式，将相同的数据复制到多台机器上，多台机器为用户提供数据，对于读操作进行扩展。对于读多写少的网站，这种方式基本可以实现线性的性能提升。数据量再大一些，使用分库分表方式，把一份数据切片，分布到多台机器上提高性能。进而使用复制与分库分表的方式，进一步压榨单机的性能潜力。单机数据库很强大，但数据量达到一定的数量级，数据库的性能完全不能满足需求。大数据时代，数据量达到 PB、TB 级别。而每一块硬盘仅仅几 T，单机完全不可能将所有的数据都存储下来。如上分库分表的方式基本已经不能完全解决如上的问题，所以出现了分布式数据库，比如 MySQL Cluster 等通过两阶段提交等方式，维护线上数据的强一致性，分布式存储系统，比如 Google 研发的 GFS，现如今正火热的 HDFS 等，基本可以满足海量数据的存储。 高速缓存技术，如一些场景重复读的场景，数据库的查询很昂贵，如何减少数据库的访问次数？使用类似 Redis 的缓存。redis 的数据全部存储在内存中，内存的速度远远超过数据库的查询速度。维护 redis 中的缓存数据一致性，防止缓存穿透、缓存击穿、缓存雪崩的问题，以及如何正确的使用缓存等一系列问题，现如今已经基本有了很好的解决办法。 负载均衡与反向代理，传统中使用 nginx 作为网关，接受的请求分发到多台服务器中，一定程度上分担单台服务器的压力。 对于分布式技术。首先是 Google 研发并使用的 GFS 以及搭建在 GFS 基础上的 BigTable，以及 MapReduce 算法，使得分布式存储与分布式计算成为了可能。对应的开源版本为 HDFS、Hadoop、Spark 等一系列分布式基础设施。存储的分布式，也对应着应用部署的分布式，也就是现如今使用比较广泛的微服务（将一个整体的应用拆分成不同的服务，服务之间分别开发和部署，通过统一的接口进行协作）。微服务部署少则几个，多则成百上千，如此多的服务需要开发部署工作量巨大。同时很常见的问题是开发环境可以正常工作，上线却无法工作。这个问题现如今的解决办法是使用 docker。首先 docker 是一种使用 Linux 内核提供的 cgroup 和 namespace 功能，它可以将计算机的资源进行隔离。使用场景是通过容器编排工具，将服务所依赖的资源通过配置文件进行定义，由编排工具统一对所有的服务进行启动部署。现如今实质上的容器编排标准 Kubernetes，已经被不限于 Google、百度、阿里巴巴、京东等使用。对于 Google 等基本实现容器化、Kubernetes 化。 ","date":"2018-10-25","objectID":"/overview-distributed-technology-and-the-problems-it-solves/:0:0","tags":["bigdata","总结"],"title":"分布式技术产生的背景及其解决的问题概述","uri":"/overview-distributed-technology-and-the-problems-it-solves/"},{"categories":null,"content":"realize 代码分析 Realize realize 是 Go 写的 workfloaw 工具，可以配置自己的工作流。项目在修改之后需要进行编译、测试，可以还有其他的一系列流程需要走，可以使用 realize 进行自动化。抽点时间研究了一下源码。这里总结一下思路，不是很完整的解释，简单说一下思路。 ","date":"2018-10-11","objectID":"/realize/:0:0","tags":["工具","Go"],"title":"Realize代码分析","uri":"/realize/"},{"categories":null,"content":"原理 从 Linux 2.6.13 内核开始，Linux 就推出了 inotify，允许监控程序打开一个独立文件描述符，并针对事件集监控一个或者多个文件，例如打开、关闭、移动/重命名、删除、创建或者改变属性。glib 对对此进行了封装glib/inotify.h,同时各个操作系统都有对应的实现，win 下的 ReadDirectoryChangesW，mac 下的 FSEvents。同时 go 下已经有写好的封装库fsnotify/fsnotify，对不同的平台进行了封装。 简单来讲，内核为应用程序提供了系统级文件修改事件的监视器。当文件进行修改后，会通知应用程序监视的文件已经修改了，之后有realize进行事件的处理即可。 比较有意思的是，yaml文件的marshal和unmarshal。之后可以研究一下。 ","date":"2018-10-11","objectID":"/realize/:1:0","tags":["工具","Go"],"title":"Realize代码分析","uri":"/realize/"},{"categories":null,"content":"核心代码 // github.com/oxequa/realize/realize/projects.go func (p *Project) Watch(wg *sync.WaitGroup) { var err error // change channel p.stop = make(chan bool) // init a new watcher p.watcher, err = NewFileWatcher(p.parent.Settings.Legacy) if err != nil { log.Fatal(err) } defer func() { close(p.stop) p.watcher.Close() }() // before start checks p.Before() // start watcher go p.Reload(\"\", p.stop) L: for { select { case event := \u003c-p.watcher.Events(): if p.parent.Settings.Recovery.Events { log.Println(\"File:\", event.Name, \"LastFile:\", p.last.file, \"Time:\", time.Now(), \"LastTime:\", p.last.time) } if time.Now().Truncate(time.Second).After(p.last.time) { // switch event type switch event.Op { case fsnotify.Chmod: case fsnotify.Remove: p.watcher.Remove(event.Name) if p.Validate(event.Name, false) \u0026\u0026 ext(event.Name) != \"\" { // stop and restart close(p.stop) p.stop = make(chan bool) p.Change(event) go p.Reload(\"\", p.stop) } default: if p.Validate(event.Name, true) { fi, err := os.Stat(event.Name) if err != nil { continue } if fi.IsDir() { filepath.Walk(event.Name, p.walk) } else { // stop and restart close(p.stop) p.stop = make(chan bool) p.Change(event) go p.Reload(event.Name, p.stop) p.last.time = time.Now().Truncate(time.Second) p.last.file = event.Name } } } } case err := \u003c-p.watcher.Errors(): p.Err(err) case \u003c-p.exit: p.After() break L } } wg.Done() } Reference 用 inotify 监控 Linux 文件系统事件 oxequa/realize fsnotify/fsnotify ","date":"2018-10-11","objectID":"/realize/:2:0","tags":["工具","Go"],"title":"Realize代码分析","uri":"/realize/"},{"categories":null,"content":"比较 SparkStream 类似产品如 Samza、Storm，介绍 Spark 和 Spark Stream 安装和简单使用方法 sparkstream-related-product-selection-spark-installation-use ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:0:0","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"各产品比较 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:1:0","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Samza Samza 是一个分布式的流式数据处理框架（streaming processing），Linkedin 开源的产品， 它是基于 Kafka 消息队列来实现类实时的流式数据处理的。更为准确的说法是，Samza 是通过模块化的形式来使用 Apache Kafka 的，因此可以构架在其他消息队列框架上，但出发点和默认实现是基于 Apache Kafka。 本质上说，Samza 是在消息队列系统上的更高层的抽象，是一种应用流式处理框架在消息队列系统上的一种应用模式的实现。 总的来说，Samza 与 Storm 相比，传输上完全基于 Apache Kafka，集群管理基于 Hadoop YARN，即 Samza 只负责处理这一块具体业务，再加上基于 RocksDB 的状态管理。由于受限于 Kafka 和 YARN，所以它的拓扑结构不够灵活。 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:1:1","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Storm Storm 框架与其他大数据解决方案的不同之处，在于它的处理方式。Apcahe Hadoop 本质上来说是一个批处理系统，即目标应用模式是针对离线分析为主。数据被引入 Hadoop 的分布式文件系统 (HDFS)，并被均匀地分发到各个节点进行处理，HDFS 的数据平衡规则可以参照本文作者发表于 IBM 的文章《HDFS 数据平衡规则及实验介绍》，进行深入了解。当处理完成时，结果数据返回到 HDFS，然后可以供处理发起者使用。Storm 则支持创建拓扑结构来转换没有终点的数据流。不同于 Hadoop 作业，这些转换从不会自动停止，它们会持续处理到达的数据，即 Storm 的流式实时处理方式。 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:1:2","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Spark Streaming Spark Streaming 类似于 Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming 有高吞吐量和容错能力强这两个特点。Spark Streaming 支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ 和简单的 TCP 套接字等等。数据输入后可以用 Spark 的高度抽象原语如：map、reduce、join、window 等进行运算。而结果也能保存在很多地方，如 HDFS，数据库等。另外 Spark Streaming 也能和 MLlib（机器学习）以及 Graphx 完美融合。 在 Spark Streaming 中，处理数据的单位是一批而不是单条，而数据采集却是逐条进行的，因此 Spark Streaming 系统需要设置间隔使得数据汇总到一定的量后再一并操作，这个间隔就是批处理间隔。批处理间隔（0.2s-2s）是 Spark Streaming 的核心概念和关键参数，它决定了 Spark Streaming 提交作业的频率和数据处理的延迟，同时也影响着数据处理的吞吐量和性能。 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:1:3","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Kafka Sreeam Kafka Streams 是一个用于处理和分析数据的客户端库。它先把存储在 Kafka 中的数据进行处理和分析，然后将最终所得的数据结果回写到 Kafka 或发送到外部系统去。它建立在一些非常重要的流式处理概念之上，例如适当区分事件时间和处理时间、窗口支持，以及应用程序状态的简单（高效）管理。同时，它也基于 Kafka 中的许多概念，例如通过划分主题进行扩展。此外，由于这个原因，它作为一个轻量级的库可以集成到应用程序中去。这个应用程序可以根据需要独立运行、在应用程序服务器中运行、作为 Docker 容器，或通过资源管理器（如 Mesos）进行操作。 Kafka Sreeam 直接解决了流式处理中的很多困难问题:毫秒级延迟的逐个事件处理。有状态的处理，包括分布式连接和聚合。方便的 DSL。使用类似 DataFlow 的模型对无序数据进行窗口化。具有快速故障切换的分布式处理和容错能力。无停机滚动部署。 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:1:4","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"主要比较 Spark Stream 和 Storm 和选择 比较项 SparkStream Storm 血统 UC Berkeley AMP lab Twitter 开源时间 2011.05 2011.09 依赖环境 Java Zookeeper Java Python 开发语言 Scala Java Clojure 支持语言 Scala Java Python R Any 硬盘 IO 少 一般 集群支持 超过 1000 节点 好 吞吐量 好 较好 使用公司 intel 腾讯 淘宝 中移动 Goole 淘宝 百度 Twitter 雅虎 适用场景 较大数据块\u0026需要高时效性的小批量计算 实时小数据块的分析计算 延时 准实时：一次处理一个即将到达的事件 实时：处理在一定的时间内（时间间隔可自己设置）在窗口中收到的一批事件 容错 在批处理级别进行跟踪处理，因此即使发生节点故障等故障，也可以有效地保证每个小批量都能够被精确处理一次 每个单独的记录必须在其通过系统时被跟踪，因此 Storm 仅保证每个记录至少被处理一次，但是从故障中恢复期间允许出现重复。 这意味着可变状态可能不正确地更新了两次 1.处理模型以及延迟 虽然这两个框架都提供可扩展性(Scalability)和可容错性(Fault Tolerance),但是它们的处理模型从根本上说是不一样的。Storm 处理的是每次传入的一个事件，而 Spark Streaming 是处理某个时间段窗口内的事件流。因此，Storm 处理一个事件可以达到亚秒级的延迟，而 Spark Streaming 则有秒级的延迟。 2.容错和数据保证 在容错数据保证方面的权衡方面，Spark Streaming 提供了更好的支持容错状态计算。在 Storm 中，当每条单独的记录通过系统时必须被跟踪，所以 Storm 能够至少保证每条记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录，这意味着可变状态可能不正确地被更新两次。而 Spark Streaming 只需要在批处理级别对记录进行跟踪处理，因此可以有效地保证每条记录将完全被处理一次，即便一个节点发生故障。虽然 Storm 的 Trident library 库也提供了完全一次处理的功能。但是它依赖于事务更新状态，而这个过程是很慢的，并且通常必须由用户实现。 简而言之,如果你需要亚秒级的延迟，Storm 是一个不错的选择，而且没有数据丢失。如果你需要有状态的计算，而且要完全保证每个事件只被处理一次，Spark Streaming 则更好。Spark Streaming 编程逻辑也可能更容易，因为它类似于批处理程序，特别是在你使用批次(尽管是很小的)时。 3.实现和编程 API Storm 主要是由 Clojure 语言实现，Spark Streaming 是由 Scala 实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm 是由 BackType 和 Twitter 开发，而 Spark Streaming 是在 UC Berkeley 开发的。 Storm 提供了 Java API，同时也支持其他语言的 API。 Spark Streaming 支持 Scala 和 Java 语言(其实也支持 Python)。另外 Spark Streaming 的一个很棒的特性就是它是在 Spark 框架上运行的。这样你就可以想使用其他批处理代码一样来写 Spark Streaming 程序，或者是在 Spark 中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。 4.生产支持 Storm 已经出现好多年了，而且自从 2011 年开始就在 Twitter 内部生产环境中使用，还有其他一些公司。而 Spark Streaming 是一个新的项目，并且在 2013 年仅仅被 Sharethrough 使用(据作者了解)。 Storm 是 Hortonworks Hadoop 数据平台中流处理的解决方案，而 Spark Streaming 出现在 MapR 的分布式平台和 Cloudera 的企业数据平台中。除此之外，Databricks 是为 Spark 提供技术支持的公司，包括了 Spark Streaming。 5.集群管理集成 尽管两个系统都运行在它们自己的集群上，Storm 也能运行在 Mesos，而 Spark Streaming 能运行在 YARN 和 Mesos 上。 这里总结了 Kafka Stream-Spark Streaming-Storm 流式计算框架比较选型的相关资料。 这里由更多的相关产品的差异比较资源： Storm 介绍 Spark Streaming vs. Kafka Stream 哪个更适合你？ 大数据框架对比：Hadoop、Storm、Samza、Spark 和 Flink Spark Streaming 与 Storm 的对比分析 Storm 和 Spark Streaming 的横向比较 Spark Streaming 和 Storm 如何选择？搭建流式实时计算平台，广告日志实时花费 Spark Streaming 新手指南 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:2:0","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Spark 介绍 Spark 生态 Spark 官网简单介绍了 spark 的的优势。 这里非常详细了介绍 Spark 生态、各大厂应用场景、Spark 基本原理。 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:3:0","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Spark 和 Spark Stream 的安装和使用 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:4:0","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Spark 介绍 Spark Streaming 是 Spark Core API 的扩展, 它支持弹性的, 高吞吐的, 容错的实时数据流的处理. 数据可以通过多种数据源获取, 例如 Kafka, Flume, Kinesis 以及 TCP sockets, 也可以通过例如 map, reduce, join, window 等的高级函数组成的复杂算法处理. 最终, 处理后的数据可以输出到文件系统, 数据库以及实时仪表盘中.事实上,你还可以在 data streams（数据流）上使用机器学习以及图计算 算法 在内部, 它工作原理如下, Spark Streaming 接收实时输入数据流并将数据切分成多个 batch（批）数据, 然后由 Spark 引擎处理它们以生成最终的 stream of results in batches（分批流结果）. Spark Streaming 提供了一个名为 discretized stream 或 DStream 的高级抽象, 它代表一个连续的数据流. DStream 可以从数据源的输入数据流创建, 例如 Kafka, Flume 以及 Kinesis, 或者在其他 DStream 上进行高层次的操作以创建. 在内部, 一个 DStream 是通过一系列的 RDDs 来表示. 你可以使用 Scala , Java 或者 Python（Spark 1.2 版本后引进）来编写 Spark Streaming 程序. 这里是一篇官方编程指南 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:4:1","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"Spark 安装 方式 1 wget http://mirror.bit.edu.cn/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz tar -xzf spark-2.3.1-bin-hadoop2.7.tgz # 运行一个例子 cd spark-2.3.1-bin-hadoop2.7 ./bin/run-example SparkPi 方式二 推荐这种方式这里总结了自己搭建各种开发环境的就自动化安装脚本。第一次安装会比较麻烦，之后实现一条命令自动安装。需要 vagrant\u0026virtual。有一些依赖 docker git clone https://github.com/ExFly/ComputSciLab.git cd ComputSciLab vagrant up vagrant ssh cd /vagrant/Java source install-small.sh cd /vagrant/Spark ./install.sh cd /vagrant/.softwenv/spark-2.3.1-bin-hadoop2.7 ./bin/run-example SparkPi 结果图： spark 集群 找到一个中文的文档,可以看一下，部署很简单 总结 如上 ","date":"2018-07-04","objectID":"/sparkstream-related-product-selection-spark-installation-use/:4:2","tags":["BigData","Spark","Stream","技术选型"],"title":"SparkStream等相关产品选型以及Spark安装与简单使用","uri":"/sparkstream-related-product-selection-spark-installation-use/"},{"categories":null,"content":"单体应用，需要借助分库分表、复制技术、读写分离提高服务并发访问量。微服务为代表的分布式系统，其高并发和微服务事务一致性该如何保证？ ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:0:0","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"简介 由于自己刚刚接触，自己理解的也不深。在这里，把我整理的一些资料汇总下来。 ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:1:0","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"微服务架构 微服务架构将单应用放在多个相互独立的服务，这个每个服务能够持续独立的开发和部署，难题是数据该如何存储？ ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:2:0","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"多个应用使用同一数据库 传统的单体应用一般采用的是数据库提供的事务一致性，通过数据库提供的提交以及回滚机制来保证相关操作的 ACID，这些操作要么同时成功，要么同时失败。各个服务看到数据库中的数据是一致的，同时数据库的操作也是相互隔离的，最后数据也是在数据库中持久存储的。这样的架构不具备横向扩展能力，服务之间的耦合程度也比较高，会存在单点故障。 ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:2:1","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"典型微服务架构 在微服务架构中， 有一个 database per service 的模式， 这个模式就是每一个服务一个数据库。 这样可以保证微服务独立开发，独立演进，独立部署， 独立团队。 由于一个应用是由一组相互协作的微服务所组成，在分布式环境下由于各个服务访问的数据是相互分离的， 服务之间不能靠数据库来保证事务一致性。 这就需要在应用层面提供一个协调机制，来保证一组事务执行要么成功，要么失败。 ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:2:2","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"CAP 定律 一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 通过 CAP 理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ CA without P：如果不要求 P（不允许分区），则 C（强一致性）和 A（可用性）是可以保证的。但其实分区不是你想不想的问题，而是始终会存在，因此 CA 的系统更多的是允许分区后各子系统依然保持 CA。 CP without A：如果不要求 A（可用），相当于每个请求都需要在 Server 之间强一致，而 P（分区）会导致同步时间无限延长，如此 CP 也是可以保证的。很多传统的数据库分布式事务都属于这种模式。 AP wihtout C：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的 NoSQL 都属于此类。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃 C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 对于涉及到钱财这样不能有一丝让步的场景，C 必须保证。网络发生故障宁可停止服务，这是保证 CA，舍弃 P。貌似这几年国内银行业发生了不下 10 起事故，但影响面不大，报到也不多，广大群众知道的少。还有一种是保证 CP，舍弃 A。例如网络故障事只读不写。 ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:3:0","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"常用的解决方法 这里总结了一些分布式数据一致性的解决方法。分布式事务保证强一致性，但为了保证数据的一致性，放弃了一些系统性能。另一种保证最终一致性，放弃了时时数据的一致性，但处理效率最好。 这里有一些例子如何解决的。 ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:3:1","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"BASE 这里实验了一个基于 BASE 协议的最终一致性 demo。注意，这里使用到了 Kafka，需要自己在本地开 Kafka 服务。 ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:3:2","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"其他资料 书：大规模分布式存储系统：原理解析与架构实现 书：微服务设计 分布式事务？No, 最终一致性 分布式事务实践 -花钱的，作为目录使用 多研究些架构，少谈些框架（3）– 微服务和事件驱动 消息中间件（一）分布式系统事务一致性解决方案大对比，谁最好使？ Saga 分布式事务解决方案与实践 解决业务代码里的分布式事务一致性问题 分布式事务实践 实战基于 Kafka 消息驱动最终一致事务（二） ","date":"2018-06-28","objectID":"/summary-of-theories-methods-for-consistency-of-distributed-data/:3:3","tags":["Architecture","Distributed"],"title":"分布式数据的一致性的一些理论及方法总结","uri":"/summary-of-theories-methods-for-consistency-of-distributed-data/"},{"categories":null,"content":"文章简介： 1.总结 gradle 和 maven 正确使用方法 2.开箱即用 maven\u0026gradle 同时支持的项目配置。 Gradle 和 Maven 使用起来都比较方便，而 Gradle 使用更灵活，配置更方便。而公司环境一般使用 Maven。因此就有了取舍，是迁移到 Gradle，还是继续使用 Maven？其实不需要纠结，谁说必须取舍的，两个都用起来就是了！！！ 说在前面 Gradle 和 Maven 都是项目自动构建工具，编译源代码只是整个过程的一个方面，更重要的是，你要把你的软件发布到生产环境中来产生商业价值，所以，你要运行测试，构建分布、分析代码质量、甚至为不同目标环境提供不同版本，然后部署。整个过程进行自动化操作是很有必要的。 整个过程可以分成以下几个步骤： 编译源代码 运行单元测试和集成测试 执行静态代码分析、生成分析报告 创建发布版本 部署到目标环境 部署传递过程 执行冒烟测试和自动功能测试 两者都是项目工具，但是 maven 使用的最多，Gradle 是后起之秀，想 spring 等都是使用 gradle 构建的。Gradle 抛弃了 Maven 的基于 XML 的繁琐配置，采用了领域特定语言 Groovy 的配置，大大简化了构建代码的行数。 比如 maven 要 这么写 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework\u003c/groupId\u003e \u003cartifactId\u003espring-core\u003c/artifactId\u003e \u003cversion\u003e${spring.version}\u003c/version\u003e \u003c/dependency\u003e gradle 这么写 compile('org.springframework:spring-core:2.5.6') 详细的 Gradle 和 Maven 比较看这里讲的很好了。gradle 官方也对两个工具进行了比较。 我们可以使用其中一个，或者两个一起使用！！！这是可行的，当然前提是，有一个人在整个过程中维护相同功能的两份配置。实际上并不难。抽一个周末空余时间，自己把这两个都熟悉了一下，整理了一套 Gradle\u0026Maven 日常开发中常用的包和插件的集合，作为项目的开始。比较通用，所以需要根据公司或个人项目实际情况加入私服的配置，以及你想使用的jar 包，如此简单。如果使用过程中遇到什么问题，请联系我。别忘了，帮我 star 一下。 接下来涉及到的内容： maven 正确使用方法 gradle 正确使用方法 gradle 项目和 maven 项目相互转化 一个项目同时支持 maven 和 gradle 配置：一个好的开始 maven 正确使用方法 ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:0:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"maven 版本不相同问题 我们大部分时候使用 IDE 进行项目开发的时候，大部分时候会直接使用 IDE 创建 MAVEN 项目，这是正确的。可是，您有没有发现，大家合作的时候，由于 maven 版本不相同，哪怕是 3.5.1 和 3.5.2 的区别，都会引发一场血案！我的可以正常打开项目，而其他人却会出现问题。除了 IDE 下载包损坏外，就是 maven 的版本不相同。其实通过一些工具，已经可以让这种情况不在发生，那就是Wrapper。请看如下图(图没配错，maven 的 wrapper 和 gradle 的 wrapper 流程上完全相同) 前提条件： 项目创建者系统中已经由 maven 的命令 其他人没有要求，mvn 可有可无（原因之后说） 具体如何做： 项目创建者执行 mvn -N io.takari:maven:wrapper -Dmaven=3.5.3 此时，项目目录会生成mvnw.cmd和mvnw，之后的所有操作都是基于此，也就是说，项目开发者不需要由任何依赖，除了 jdk-_!!! 项目创建者执行 mvnw archetype:generate 此步是自动生成项目目录结构。同时，项目管理者需要搭建好基础的代码框架。之后可以开发了 项目开发者 mvnw.cmd compiler:compile mvnw.cmd exec:java -Dexec.mainClass=\"org.exfly.LombokL.LombokLApplication\" -q mvn.cmd clean mvn.cmd test 。。。 注意: 当第一次执行mvnw.cmd时候，会自动下载对应版本的 Maven，maven 的$HOME/.m2/wrapper/dists/\u003cversion\u003e/下。 初网络问题，如果出现错误，依赖包已经下好，只需要到1所说的位置去掉后缀.pack，重新运行即可。 ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:1:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"使用dependencyManagement集中管理版本依赖 dependencyManagement这里已经很好的解释如何做。同时可以借鉴 springboot-parent ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:2:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"多模块项目管理方法 多模块项目的 POM 重构 通过 parent 的方式，将多模块依赖集中管理， ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:3:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"如何更好的使用 maven 进行项目管理 几点建议 尽量使用 wapper 多 使用dependencyManagement集中管理版本依赖 bin 下有 mvn 和 mvnDebug(运行 mvn 时开始 debug) M2_HOME maven 主程序的安装目录 ~/.m2 本地包下载位置 http 代理 setting.xml 中的 proxies MAVEN_OPTS 运行 mvn 时候相当于运行 java 命令，MAVEN_OPTS 可以配置为任何 java 的命令参数 设置 MAVEN_OPTS 环境变量 配置用户范围 settings.xml %M2_HOME%/conf/settings.xml 为全局配置文件 ~/.m2/settings.xml 为用户配置文件 不要使用 IDE 内嵌的 Maven，应该配置 IDE 中为自己安装的 maven 显示声明所有用到的依赖 ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:4:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"我的 maven 常用命令笔记 我的 maven 常用命令笔记 gradle 正确使用方法 理由同上节，直接说使用方法。可以对照我的笔记查看。 gradle init –type java-library 这里自动生成 gradlew，并创建项目目录结构 之后所有命令使用 gradlew 即可 gradle 项目和 maven 项目相互转化 gradle 和 maven 可以相互转化，意味着，我们可以使用 gradle 为主的开发，之后导出为 maven 项目，供生产环境使用。前提，你足够了解 gradle 和 maven。 ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:5:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"maven -\u003e gradle cd /path/to/mavenproject gradle init gradle wrapper ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:6:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"gradle -\u003e maven cd /path/to/gradleproject gradlew install 将项目转换为 maven 和 gradle 项目后，目录结构如下： 之后，我们习惯使用 mavnw 或者 gradlew，都可以。如此，做到了共存。 一个项目同时支持 maven 和 gradle 配置：一个好的开始 抽时间，做了常用 jar 包和插件整合包，一个项目同时支持 maven 和 gradle。 共同的依赖： 内容包括： 日志、通用工具库、单元测试、代码质量度量、文档生成等 jar: slf4j、logback、lombok、guava、junit、mockito 配置中整合的工具： 代码质量分析报告工具：pmd、findbugs、checkstyle、jdepend 单元测试报告工具、javadoc、依赖管理、项目信息汇总等可视化信息 maven 具体内容 maven-compiler-plugin、maven-javadoc-plugin、cobertura-maven-plugin、maven-checkstyle-plugin、findbugs-maven-plugin、maven-pmd-plugin、jdepend-maven-plugin、maven-jar-plugin、maven-surefire-plugin、maven-surefire-report-plugin gradle 具体内容 java、maven、checkstyle、pmd、findbugs、jdepend、eclipse、idea、javadoc 首先 maven 配置见此文件 其次 gradle 配置见此文件 资料汇总 完整的整合项目，支持 maven 和 gradle，点我下载 我的 Gradle 笔记，点我查看 我的 maven 笔记，点我查看 ","date":"2018-06-24","objectID":"/gradle_maven-introduce/:7:0","tags":["Java"],"title":"Gradle和Maven使用方法总结","uri":"/gradle_maven-introduce/"},{"categories":null,"content":"如果你正在学 Lua 与 openresty，那你就一定知道在开发过程中，调试代码、单元测试是多么的麻烦。这里整理了一些 lua 开发的最佳实践。 简介 openresty 中 lua ide 调试，单元测试比较麻烦；lua 对库的管理比较散漫。在公司生产环境，一般没有外网环境，OpenResty 的安装和 lua 项目的部署都比较麻烦。 结合 Python 的一些经验，在这里整理一下自己对 Lua 的理解，以及 Lua 最佳实践。 OpenResty 安装 对于软件，使用编译方式安装比较好，比如 Ubuntu，apt-get 安装的包一般都会比较旧。如下介绍我的编译参数。这里需要自己下载自己的依赖包：naxsi, nginx-goodies-nginx-sticky-module-ng，pcre，openssl，zlib，并根据我的配置进行修改相应参数 ./configure --prefix=$HOME/openresty \\ --add-module=$HOME/openresty/setupfile/third/naxsi-0.55.3/naxsi_src \\ --add-module=$HOME/openresty/setupfile/third/nginx-goodies-nginx-sticky-module-ng \\ --with-pcre=$HOME/openresty/setupfile/depency/pcre-8.41 \\ --with-openssl=$HOME/openresty/setupfile/depency/openssl-1.0.2k \\ --with-zlib=$HOME/openresty/setupfile/depency/zlib-1.2.11 \\ --with-http_v2_module \\ --with-http_sub_module \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-cc-opt=-O2 \\ --with-luajit 这里是一个比较好的 nginx 的笔记，可以过一遍 我在学习的时候，看了这本书深入理解 Nginx 模块开发与架构解析,毕竟讲的比较系统，可以借鉴一下 有问题，知乎，搜索引擎 ","date":"2018-06-12","objectID":"/openresty-best-practice/:0:0","tags":["Lua","OpenResty"],"title":"OpenResty最佳实践","uri":"/openresty-best-practice/"},{"categories":null,"content":"安装 luarocks 下载地址 http://luarocks.github.io/luarocks/releases/ 编译安装 ./configure --prefix=$HOME/openresty/luajit \\ --with-lua=$HOME/openresty/luajit \\ --lua-suffix=jit \\ --with-lua-include=$HOME/openresty/luajit/include/luajit-2.1 --prefix 设定 luarocks 的安装目录 --with-lua 则是系统中安装的 lua 的根目录 --lua-suffix 版本后缀，此处因为openresyt的lua解释器使用的是 luajit ,所以此处得写 jit --with-lua-include 设置 lua 引入一些头文件头文件的目录 make build \u0026\u0026 make install lua 面向对象 lua 借助 table 以及 metatable 的概念进行 oo 的。这里摘了一个博客的代码，看起来还可以。以后可以使用这个。Lua 中实现面向对象。 这里要说一下 lua 中.运算和:的区别，a={};a.fun(a, arg) 等价于 a:fun(arg)，其实就是:可以省略 self 参数。 local _class={} function class(super) local class_type={} class_type.ctor=false class_type.super=super class_type.new=function(...) local obj={} do local create create = function(c,...) if c.super then create(c.super,...) end if c.ctor then c.ctor(obj,...) end end create(class_type,...) end setmetatable(obj,{ __index=_class[class_type] }) return obj end local vtbl={} _class[class_type]=vtbl setmetatable(class_type,{__newindex= function(t,k,v) vtbl[k]=v end }) if super then setmetatable(vtbl,{__index= function(t,k) local ret=_class[super][k] vtbl[k]=ret return ret end }) end return class_type end 基本编码规范 设计 可以参考 OpenResty 的最佳实践，平时用起来，大部分跟 c 的风格差不多吧。主要是所使用的代码风格要统一。 包管理 lua 下有两个包管理系统，LuaDist 和 LuaRocks 单元测试 重点 如下方法请在命令行中使用类似curl localhost/unittest进行测试，浏览器中看会很痛苦 OpenResty 最佳实践-单元测试给出一种方法。我的处理方法是，在 nginx.conf 中的 server 中建一个单独的 location，content_by_lua_file 设置 unittest.lua。公司用的 verynginx，所以我把此配置放到了 router.lua 中(当然配置方法类似，这个很容易研究，就不放到这里了)。 -- file: unittest.lua local _M = {} local csrf_test = require(\"test.test_csrf\") local tmp_test = require(\"test.tmp_test\") function _M:run_unittest() csrf_test:run() end return _M -- file: test_csrf.lua local iresty_test = require(\"resty.iresty_test\") local json = require(\"json\") local config = require(\"config\") local csrf_config = require(\"csrf_config\") local token = require(\"token\") local tabletls = require(\"tabletls\") local tb = iresty_test.new({unit_name=\"test_csrf\"}) local function assert_eq(wanted, real, msg) if wanted ~= real then error(msg or \"error\", 2) -- 请注意参数 2 end end local function assert_not_eq(wanted, real, msg) if wanted == real then error(msg or \"error\", 2) end end function tb:test_geturl() assert_eq(\"/unittest\", ngx.var.uri, \"the unittest url changed\") end function tb:run_unittest() tb:run() end return tb 如上有一个很有意思的地方，error(msg or \"error\", 2),其中的 2 有些讲究，表示返回调用函数所在行，还有 0（忽略行号），1（error 调用位置行号） 性能测试 代码覆盖率 API 测试等，都可以去OpenResty 最佳实践中找，配置很简单。 远程调试 OpenResty 对于此部分，对于有些人来说，使用日志就已经足够了。可对于有些时候，在代码中太多的日志有不利于维护。这里自己要尽力做好日志和调试的平衡吧。 此调试方法适用于 win linux osx 先贴这里用到的 luaIDE 地址：ZeroBraneStudio 如下为安装步骤： 下载这个项目，ZeroBraneStudio，解压可以直接用【调试方法在下载好的文件中 README.md 中有相应的链接】 启动 ZBS，Project -\u003e Start Debugger Server 复制/lualibs/mobdebug/mobdebug.lua -\u003e nginx lua path, 复制/lualibs/socket.lua -\u003e nginx lua path， 复制/bin/clibs/socket/core -\u003e socket 设为 nginx lua cpath（调试时候，使用的是 require(“socket.core”)形式导入包。这里需要注意 core 文件后缀，win 是 dll，linux 是 so，） nginx 配置好,将如上依赖加到 nginx.conf 中，让 lua 可以找到这些文件即可 创建需要调试的 lua 文件 require('mobdebug').start('192.168.1.22') local name = ngx.var.arg_name or \"Anonymous\" ngx.say(\"Hello, \", name, \"!\") ngx.say(\"Done debugging.\") require('mobdebug').done() 注：start()呼叫需要运行 IDE 的计算机的 IP 。默认情况下使用“localhost”，但是由于您的 nginx 实例正在运行，因此您需要指定运行 IDE 的计算机的 IP 地址（在我的例子中 192.168.1.22） 在 ide 中打开需要调试的如上 lua 文件 Project -\u003e Project Directory -\u003e Set From Current File。 此时，打开浏览器，访问需要此文件处理的链接 此时开始调试 注：在最下侧有 Remote console，在这里可以执行任何 ngx lua 语句 如上流程没有截图，或者没有说清楚，可以来这里 nginx 一些技巧 看我配置的 nginx.conf lua_package_path '$prefix/lua_script/?.lua;;'; 我的笔记 资料 章亦春 OpenResty OpenResty 最佳实践 Lua 5.1 参考手册 Lua 5.3 参考手册 云风 github awesome-lua awesome-resty Nginx-Lua-OpenResty-ResourcesA collection of resources covering Nginx, Nginx + Lua, OpenResty and Tengine ","date":"2018-06-12","objectID":"/openresty-best-practice/:1:0","tags":["Lua","OpenResty"],"title":"OpenResty最佳实践","uri":"/openresty-best-practice/"},{"categories":null,"content":"文章简介：spring 中 bean 的装配有一定规则，在这里进行总结。本文主要讲解一些概念和 java 配置方法。demo 代码见文末。 目录 手动装配 使用@Bean 自动装配 使用@ComponentScan 自动装配的歧义性 条件生效 Bean profile bean 作用域 ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:0:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"手动装配 手动装配可以通过声明 xml 文件和 java 配置文件两种手段。在这两种方式中，更加推荐使用 java 配置的方式。两种配置不是相互替代的关系，一般将业务相关的配置放到 java 配置中，对于非业务，如数据库等，可以放到 xml 中。 通过使用@Bean 注解方法，可以声明并注册一个以方法名为 name 的 bean。@Bean(name= {“sayAndPlayServiceNewName”, “sayAndPlayService”}) public interface SayAndPlayService { String say(); String play(); } public class PeopleSayAndPlayServiceImpl implements SayAndPlayService { @Override public String say() { return \"People Service implements say.\"; } @Override public String play() { return \"People Service implements play.\"; } } @Configuration public class SimpleManualwireConfig { @Bean public SayAndPlayService sayAndPlayService() { return new PeopleSayAndPlayServiceImpl(); } } @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleManualwireConfig.class}) public class SimpleBeanManualwireTest { @Autowired private SayAndPlayService service; @Test public void testTestOneAutowiredService() { Assert.assertEquals(\"People Service implements say.\", service.say()); } @Test public void testAnnotationConfigAppContext() { ApplicationContext context = new AnnotationConfigApplicationContext(org.exfly.demo.config.SimpleManualwireConfig.class); SayAndPlayService service = (SayAndPlayService) context.getBean(\"sayAndPlayService\"); Assert.assertEquals(\"People Service implements say.\", service.say()); } } ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:1:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"自动装配 使用@ComponentScan 可以自动扫描，@ComponentScan 告诉 Spring 哪个 packages 的用注解标识的类 会被 spring 自动扫描并且装入 bean 容器。自动扫描，会扫描相应包以及子包，并为所有 bean 生成 name，name 命名规则为其类首字母变小写，如 interface UserService 被唯一的 UserServiceImpl 实现，则经过扫描，bean 被声明为 name 为 userServiceImpl。如果接口被多各类实现，需要转到下文消除歧义部分进行了解。 public interface SpeakService { String speak(); } @Service public class PeopleSpeakServiceImpl implements SpeakService { @Override public String speak() { return \"People speak\"; } } @Configuration @ComponentScan(basePackageClasses={org.exfly.demo.service.SpeakService.class}) public class SimpleConfigScanConfig {} @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleManualwireConfig.class}) public class SimpleBeanManualwireTest { @Test public void testAnnotationConfigAppContextAutoScan() { ApplicationContext context = new AnnotationConfigApplicationContext(org.exfly.demo.config.SimpleConfigScanConfig.class); SpeakService service = (SpeakService) context.getBean(\"peopleSpeakServiceImpl\"); Assert.assertEquals(\"People speak\", service.speak()); } } //如果希望使用@Autowired自动装配， @RunWith(SpringRunner.class) @ContextConfiguration(classes= {org.exfly.demo.config.SimpleConfigScanConfig.class}) public class SimpleAutoScanTest { @Autowired //根据类型进行自动注入 private SpeakService sservice; @Test public void testAnnotationConfigAppContextAutoScanAutoWire() { Assert.assertEquals(\"People speak\", sservice.speak()); } } ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:2:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"@Autowired 可以在属性、构造方法、set 函数中进行自动注入 ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:2:1","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"消除歧义 通过使用@Bean 注解方法，可以声明并注册一个以方法名为 name 的 bean。如果使用@Bean(name= {“sayAndPlayServiceNewName”, “sayAndPlayService”})对 bean 进行命名，可以用不同的名字取用（在@Autowired 处再加一个@Qualifier(“sayAndPlayServiceNewName”)）; 如果使用@ComponentScan，相应的 Bean 定义需要使用 Component 等进行注解，同时使用@Qualifier(“BeanId”)限定符，如下 @Configuration public class SimpleManualwireConfig { @Bean(name={\"sayAndPlayServiceNewName\", \"sayAndPlayService\"}) public SayAndPlayService sayAndPlayService() { return new PeopleSayAndPlayServiceImpl(); } } @Service @Qualifier(\"peopleSayServ\") public class PeopleSayServiceImpl implements SayService {} //or @Service(\"peopleSayServ\") public class PeopleSayServiceImpl implements SayService {} //如何使用 @Autowired @Qualifier(\"peopleSayServ\") SayService service; ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:3:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"条件生效 如下的解释：在当前上下文中，如果 Conditional 注解中的 MagitExistsCondition.matches 方法返回 true，则当前 bean：magicBean 生效。@Profile 和 springboot 自动配置都是基于此种原理实现的。 @Bean @Conditional(MagitExistsCondition.class) public MagicBean magicBean(){ return new MagitBean(); } public class MagitExistsCondition implements Condition { boolean matches(ConditionContext ctxt, AnnotatedTypeMetadat metadate){ Environment env = ctxt.getEnvironment(); return env.containsProperty(\"magic\"); } } ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:4:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"@Profile ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:5:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"bean 作用域 Singleton 默认 只创建一个实例 Prototype 每次创建新的实例 Session 每个会话创建一个实例 Request 每个请求创建一个实例 使用@Scope 进行配置即可 @Component @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) public class Notepad{} ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:6:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"运行时值注入 以后补充 ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:7:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"其他 项目代码 ","date":"2018-05-20","objectID":"/sprint-beans-waire-notebook/:8:0","tags":["Spring","SSM"],"title":"Spring Beans的装配规则总结","uri":"/sprint-beans-waire-notebook/"},{"categories":null,"content":"文章简介：学Java Collections集合，对其中一些知识进行整理 Collections结构 ","date":"2018-03-24","objectID":"/learn-java-collections/:0:0","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"使用例子 ","date":"2018-03-24","objectID":"/learn-java-collections/:1:0","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"Iterator public void testIterator(){ //创建一个集合 Collection books = new HashSet(); books.add(\"轻量级J2EE企业应用实战\"); books.add(\"Struts2权威指南\"); books.add(\"基于J2EE的Ajax宝典\"); //获取books集合对应的迭代器 Iterator\u003cString\u003e it = books.iterator(); while(it.hasNext()) { String book = it.next(); System.out.println(book); if (book.equals(\"Struts2权威指南\")) { it.remove(); //使用Iterator迭代过程中，不可修改集合元素,下面代码引发异常 //books.remove(book); } //对book变量赋值，不会改变集合元素本身 book = \"测试字符串\"; } System.out.println(books); } ","date":"2018-03-24","objectID":"/learn-java-collections/:1:1","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"List 实现List接口的常用类有LinkedList，ArrayList List\u003cString\u003e list = new LinkedList\u003c\u003e(); ","date":"2018-03-24","objectID":"/learn-java-collections/:1:2","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"Set Set接口有以下几种实现： HashSet : 为快速查找设计的Set，主要的特点是：不能存放重复元素，而且采用散列的存储方法，所以没有顺序。这里所说的没有顺序是指元素插入的顺序与输出的顺序不一致。 TreeSet : 保存次序的Set, 底层为树结构。使用它可以从Set中提取有序的序列。 LinkedHashSet : 具有HashSet的查询速度，且内部使用链表维护元素的顺序(插入的次序)。于是在使用迭代器遍历Set时，结果会按元素插入的次序显示。 Set\u003cString\u003e hs = new HashSet\u003c\u003e(); ","date":"2018-03-24","objectID":"/learn-java-collections/:1:3","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"Map Map接口有以下几种实现： HashMap、LinkedHashMap、HashTable和TreeMap Map\u003cString, String\u003e m1 = new HashMap\u003c\u003e(); m1.put(\"Zara\", \"8\"); m1.get(\"Zara\"); // 8 m1.containsKey(\"Zara\"); // true Java8的HashMap详解（存储结构，功能实现，扩容优化，线程安全，遍历方法） ","date":"2018-03-24","objectID":"/learn-java-collections/:1:4","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"Queue Queue\u003cString\u003e queue = new LinkedList\u003cString\u003e(); //添加元素 queue.offer(\"a\"); queue.offer(\"b\"); queue.offer(\"c\"); queue.offer(\"d\"); queue.offer(\"e\"); for(String q : queue){ System.out.println(q); } System.out.println(\"===\"); System.out.println(\"poll=\"+queue.poll()); //返回第一个元素，并在队列中删除 for(String q : queue){ System.out.println(q); } System.out.println(\"===\"); System.out.println(\"element=\"+queue.element()); //返回第一个元素 for(String q : queue){ System.out.println(q); } System.out.println(\"===\"); System.out.println(\"peek=\"+queue.peek()); //返回第一个元素 for(String q : queue){ System.out.println(q); } /* a b c d e === poll=a b c d e === element=b b c d e === peek=b b c d e */ ","date":"2018-03-24","objectID":"/learn-java-collections/:1:5","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"转成线程安全 List\u003cString\u003e list = Collections.synchronizedList(new LinkedList\u003c\u003e()); 资源 官方 Collections Api reference Java集合框架面试题 比较细致的讲解 面试整理-Java综合高级篇（吐血整理） 最全的BAT大厂面试题整理 ","date":"2018-03-24","objectID":"/learn-java-collections/:2:0","tags":["Java"],"title":"Collections知识整理","uri":"/learn-java-collections/"},{"categories":null,"content":"文章简介：你可以将 Wox 看作一个高效的本地快速搜索框，通过快捷键呼出（默认 alt+空格），然后输入关键字（支持拼音模糊查询）来搜索程序进行快速启动，或者搜索本地硬盘的文件，打开百度、Google 进行搜索，甚至是通过一些插件的功能实现单词翻译、关闭屏幕、查询剪贴板历史、查询编程文档、查询天气等更多功能。 ","date":"2018-02-24","objectID":"/wox_everything_production/:0:0","tags":["工具","生产力","windows"],"title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","uri":"/wox_everything_production/"},{"categories":null,"content":"软件准备 everything wox ","date":"2018-02-24","objectID":"/wox_everything_production/:1:0","tags":["工具","生产力","windows"],"title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","uri":"/wox_everything_production/"},{"categories":null,"content":"介绍一些软件的特性 直接使用搜索引擎搜索 可以搜索软件，直接回车即可运行 搜索文件、文件夹，回车后使用系统默认软件打开 配置系统命令，可以直接运行(类似 Win+R) 通过插件可以实现单词翻译等功能 ","date":"2018-02-24","objectID":"/wox_everything_production/:2:0","tags":["工具","生产力","windows"],"title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","uri":"/wox_everything_production/"},{"categories":null,"content":"安装方法 链接中有具体的使用方法。我更喜欢绿色软件，下载下来直接可以使用。 ","date":"2018-02-24","objectID":"/wox_everything_production/:3:0","tags":["工具","生产力","windows"],"title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","uri":"/wox_everything_production/"},{"categories":null,"content":"安装之后 看一下使用效果： ","date":"2018-02-24","objectID":"/wox_everything_production/:4:0","tags":["工具","生产力","windows"],"title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","uri":"/wox_everything_production/"},{"categories":null,"content":"引用 Wox 一款国产开源的快捷启动器辅助工具神器 具体怎么配置可以看这个 wox 程序开源地址 ","date":"2018-02-24","objectID":"/wox_everything_production/:5:0","tags":["工具","生产力","windows"],"title":"Wox+Everything改变日常使用电脑的流程神器，墙裂推荐","uri":"/wox_everything_production/"},{"categories":null,"content":"文章简介：为了写论文，使用 gensim 训练 word2vec 模型，如下记录了进行训练的过程 ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:0:0","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"准备 中文维基百科预料：zhwiki-latest-pages-articles.xml.bz2 python3 wiki_zh_word2vec 繁体转简体：opencc一定要下*-win32.7z,win64 的在我电脑上无法运行。如果使用我的wiki_zh_word2vec,则项目中包含可以直接使用的 opencc ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:1:0","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"TODO ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:0","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"依赖准备 下载中文维基百科预料 git clone https://github.com/ExFly/wiki_zh_word2vec.git 将 zhwiki-latest-pages-articles.xml.bz2 放到 build 文件夹下 cd path/to/wiki_zh_word2vec pip install pipenv pipenv install –dev ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:1","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"将 XML 的 Wiki 数据转换为 text 格式 pipenv run python 1_process.py build/zhwiki-latest-pages-articles.xml.bz2 build/wiki.zh.txt 31 分钟运行完成 282855 篇文章，得到一个 931M 的 txt 文件 ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:2","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"中文繁体替换成简体 opencc-1.0.1-win32/opencc -i build/wiki.zh.txt -o build/wiki.zh.simp.txt -c opencc-1.0.1-win32/t2s.json 大约使用了 15 分钟 ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:3","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"结巴分词 pipenv run python 2_jieba_participle.py 大约使用了 30 分钟 ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:4","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"Word2Vec 模型训练 pipenv run python 3_train_word2vec_model.py 大约使用了 30 分钟，且全程 cpu 使用率达到 90%+ ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:5","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"模型测试 pipenv run python 4_model_match.py d:\\Project\\wiki_zh_word2vec (develop) λ pipenv run python 4_model_match.py 国际足球 0.5256255865097046 足球队 0.5234458446502686 篮球 0.5108680725097656 足球运动 0.5033905506134033 国家足球队 0.494105726480484 足球比赛 0.4919792115688324 男子篮球 0.48382389545440674 足球联赛 0.4837716817855835 体育 0.4835757911205292 football 0.47945135831832886 ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:6","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"查看结果 可以使用 linux 的 head 或者 tail 命令查看运行的结果。 head -n 100 wiki.zh.simp.txt \u003e wiki.zh.simp_head_100.txt,直接查看 wiki.zh.simp_head_100.txt 即可 没有 head 命令，可以安装gow，或者直接下载cmder,进入就可以使用 head 命令了 ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:2:7","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"结果 至此，使用 python 对中文 wiki 语料的词向量建模就全部结束了，wiki.zh.text.vector 中是每个词对应的词向量，可以在此基础上作文本特征的提取以及分类。所有代码都已上传至本人 GitHub中，欢迎指教！ 感谢AimeeLee77,其代码为 Python2，我的项目exfly/wiki_zh_word2vec已经完全迁移到 python3,并向AimeeLee77提交了 pull request wiki_zh_word2vec ","date":"2018-02-14","objectID":"/wiki_zh_practice_word2vec/:3:0","tags":["word2vec","jieba","python","算法","NLP"],"title":"使用gensim训练word2vec模型--中文维基百科语料","uri":"/wiki_zh_practice_word2vec/"},{"categories":null,"content":"记录 vim 学习资源 ","date":"2018-01-10","objectID":"/share-vim-resources/:0:0","tags":["linux","工具","资源","生产力"],"title":"记录 Vim 学习资源","uri":"/share-vim-resources/"},{"categories":null,"content":"引言 想学一下 Vim 的键位，结合 sublime text 和 vscode 的 vim 插件加快编码速度编码 ","date":"2018-01-10","objectID":"/share-vim-resources/:1:0","tags":["linux","工具","资源","生产力"],"title":"记录 Vim 学习资源","uri":"/share-vim-resources/"},{"categories":null,"content":"正文 ","date":"2018-01-10","objectID":"/share-vim-resources/:2:0","tags":["linux","工具","资源","生产力"],"title":"记录 Vim 学习资源","uri":"/share-vim-resources/"},{"categories":null,"content":"资源 ","date":"2018-01-10","objectID":"/share-vim-resources/:3:0","tags":["linux","工具","资源","生产力"],"title":"记录 Vim 学习资源","uri":"/share-vim-resources/"},{"categories":null,"content":"使用方法 Vim 简体中文 ","date":"2018-01-10","objectID":"/share-vim-resources/:3:1","tags":["linux","工具","资源","生产力"],"title":"记录 Vim 学习资源","uri":"/share-vim-resources/"},{"categories":null,"content":"cheatsheet vim_cheat_sheet vim_表格形式 vim_脑图形式 vim_中文形式 ","date":"2018-01-10","objectID":"/share-vim-resources/:3:2","tags":["linux","工具","资源","生产力"],"title":"记录 Vim 学习资源","uri":"/share-vim-resources/"},{"categories":null,"content":"分享自己的密码管理体系:keepass+坚果云+keepass2Android，以及使用容器，搭建防止手机越用越卡的日常使用 app 体系，该应用不需要 root。 ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:0:0","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"个人密码管理 ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:1:0","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"准备 win:keepass 自助云存储:坚果云账号 Android:keepass2Android ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:1:1","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"开始 工具准备好就开始吧，按如下步骤： 电脑端 keepass 本地建一个密码文件（需要一个主密码，以后可以修改，主密码一定要复杂），后上传到坚果云里，（这时候就可以删除本地的密码文件了）； 坚果云中配置第三方应用授权。（坚果云记得开二步验证，这样每次登陆需要微信接收验证码才可以登陆，更安全一些）； 电脑 keepass 打开 url，以及 Android 手机 keepass2Android 打开 url； 完。 具体如何创建请看这个链接， 如我这般建密码维护基本不会出现问题。 到这里密码管理体系基本完成了。可是对于一个新手机，用好久就会卡，很卡，超级卡，为了解决这个问题，就要耍一些小手段，具体如下。 ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:1:2","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"Android 装机指南 ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:2:0","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"准备 容器开源产品，基于 virtualapp 框架，有点像 Docker；又是双开工具，他自己有自己的运行环境；又可以说是 Android 下的免安装应用的运行平台。之后会告诉你怎么用，超级棒 酷安应用商店 apkpure.com下载中国下载不了的应用，有一个没被墙的网址，下载好 app 后，app 不需要 vpn，懂了没？域名没找到，懒得找了。google 商店里所有的应用这里都可以下，自己想像吧 ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:2:1","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"开始 先截个图，看一看 最屌炸天的是，我把淘宝、王者荣耀和吃鸡都放到容器了，而且完全没有性能损失 基本思路：把必须装到手机里的（比如支付宝，微信等）装到手机里，非必需（比如淘宝，百度云等），都装容器里。 类比真机，软件需要有一个执行环境和临时文件，容器里的软件文件都存到了/virtual 中的。 使用思路：平时把可以放到容器中的软件放到里边，不用的时候直接关闭容器，容器里所有的软件会关闭。这样就防止软件的后台自动启动，浪费内存，手机越用越慢的现象。 具体看我平时常用的一些软件，我把他们分为主机（就是安装到真机中）和容器中的。地址分别如下 真机 容器 ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:2:2","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"最后 有什么问题请直接在我的酷安@我，或者邮箱我：exflycat@gmail.com。 Enjoy!!! ","date":"2017-12-19","objectID":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/:3:0","tags":["安全"],"title":"个人密码管理+Android装机指南","uri":"/2017.12.19-%E4%B8%AA%E4%BA%BA%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86-android%E8%A3%85%E6%9C%BA%E6%8C%87%E5%8D%97/"},{"categories":null,"content":"看了余弦大大的知乎 live 后，发现真的需要对自己的隐私安全做点什么了。一激动，淘宝 500 大洋买了一部 oneplus one，刷 kali。 这里记录使用 oneplus one 手机打造黑手全过程，以及一些使用到的资源，以及经验汇总。 ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:0:0","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"一 使用 oneplus one 搭建黑手还是比较简单的，因为对 Android 平台不熟悉，刷 kali 过程中踩了许多坑。比如二清、三清、四清等等。如下一步一步的说如何刷。 ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:1:0","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"二 ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:2:0","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"刷机四步走 第一步， 仔细阅读官方 Wiki 仔细研读仔细阅读官方 Wiki，可以减少刷机过程中各种坑。 第二步， 刷TWRP-oneplus1 一句话总结就是：解锁、刷进对应的 twrp.img。整个过程本质是一加 3T 开启了开发者模式，同时电脑上基于预配置好的 adb、fastboot 命令完成这一系列操作。这个\"预配置\"在 Windows 下，也可以参考\"Bacon Root Toolkit\"（这是专为一加打造的 GUI 工具集，当时还是一加 1 时，用这个很方便，虽然很久没更新了，但作为参考还是很好的）。 第三步，下载最新的 NetHunter，并进入 TWRP 的 recovery 模式刷入 kali kernel-nethunter-[device]-[os]-*.zip nethunter-generic-[arch]-kalifs-*.zip 对于 oneplus1 手机来说，其对应的[arch]为 armhf。 随后进行刷入操作。进入 TWRP，选择安装。先找到 CM13.0 刷到 oneplus 中，后进行默认的 WIPE。之后再 TWRP 安装中选择 kernel-nethunter-[device]-[os]-*.zip，安装结束后，在选择 nethunter-generic-[arch]-kalifs-*.zip，最后这个文件安装话费的时间比较久，大约 10 多分钟的样子。 第四步，都刷顺利后，开机进入 kali ，用已经“预装”上的 SuperSU App 来完成之后一系列的 Root 授权即可。 ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:2:1","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"三 如果安装 Kali Linux NetHunter,需要下载的文件如下： CM13.0 TWRP：第三方 recovery brt：oneplus 的解锁、root 工具 Kali Linux NetHunter：如果刷其他系统，可能需要的文件如下： TWRP：第三方 recovery lineageos rom：cm 的重生 supersu：root 工具 ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:3:0","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"四 Enjoy！ ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:4:0","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"最后 安装了 Kali Linux NetHunter，手机便有了完整的 python 环境。剩下的，你懂的。 ","date":"2017-09-17","objectID":"/2017.09.17-black-phone/:5:0","tags":["安全","kali","刷机"],"title":"oneplus one 刷Kali Linux NetHunter","uri":"/2017.09.17-black-phone/"},{"categories":null,"content":"总结一些常用的排序算法，如冒泡排序、插入排序、快速排序、计数排序、二分排序、归并排序等。 Source 排序算法比较-wiki visualgo 排序算法的动态图 排序算法复杂度 冒泡排序 ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:0:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"伪代码 function bubble_sort (array, length) { var i, j; for(i from 0 to length-1){ for(j from 0 to length-1-i){ if (array[j] \u003e array[j+1]) swap(array[j], array[j+1]) } } } 函数 冒泡排序 输入 一个数组名称为array 其长度为length i 从 0 到 (length - 1) j 从 0 到 (length - 1 - i) 如果 array[j] \u003e array[j + 1] 交换 array[j] 和 array[j + 1] 的值 如果结束 j循环结束 i循环结束 函数结束 ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:1:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"python实现 def bubble(List): for j in range(len(List)-1,0,-1): for i in range(0, j): if List[i] \u003e List[i+1]: List[i], List[i+1] = List[i+1], List[i] return List 插入排序 ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:2:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"python实现 def insert_sort(lst): n=len(lst) if n==1: return lst for i in range(1,n): for j in range(i,0,-1): if lst[j] \u003c lst[j-1]: lst[j], lst[j-1] = lst[j-1], lst[j] return lst 快速排序 ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:3:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"python实现 def quicksort(a): if len(a) == 1: return a[0] if len(a) \u003c 1: return 0 return quicksort([x for x in a[1:] if x \u003c a[0]]), [a[0]], quicksort([x for x in a[1:] if x \u003e a[0]]) ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:4:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"C语言实现 int partition(int arr[], int low, int high){ int key; key = arr[low]; while(low \u003c high){ while(low \u003c high \u0026\u0026 arr[high]\u003e= key ) high--; if(low \u003c high) arr[low++] = arr[high]; while( low \u003c high \u0026\u0026 arr[low]\u003c=key ) low++; if(low \u003c high) arr[high--] = arr[low]; } arr[low] = key; return low; } void quick_sort(int arr[], int start, int end){ int pos; if (start \u003c end){ pos = partition(arr, start, end); quick_sort(arr,start,pos-1); quick_sort(arr,pos+1,end); } return; } 归并排序 ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:5:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"python实现 from collections import deque def merge_sort(lst): if len(lst) \u003c= 1: return lst def merge(left, right): merged,left,right = deque(),deque(left),deque(right) while left and right: merged.append(left.popleft() if left[0] \u003c= right[0] else right.popleft()) # deque popleft is also O(1) merged.extend(right if right else left) return list(merged) middle = int(len(lst) // 2) left = merge_sort(lst[:middle]) right = merge_sort(lst[middle:]) return merge(left, right) 计数排序 ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:6:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"C实现 void counting_sort(int *ini_arr, int *sorted_arr, int n) { int *count_arr = (int *) malloc(sizeof(int) * 100); int i, j, k; for (k = 0; k \u003c 100; k++) count_arr[k] = 0; for (i = 0; i \u003c n; i++) count_arr[ini_arr[i]]++; for (k = 1; k \u003c 100; k++) count_arr[k] += count_arr[k - 1]; for (j = n; j \u003e 0; j--) sorted_arr[--count_arr[ini_arr[j - 1]]] = ini_arr[j - 1]; free(count_arr); } 二分查找 int binary_search(int array[],int n,int value){ int left=0; int right=n-1; while (left\u003c=right){ int middle=left + ((right-left)\u003e\u003e1); //防止溢出，移位也更高效。同时，每次循环都需要更新。 if (array[middle] \u003e value) { right =middle-1; } else if(array[middle] \u003c value) { left=middle+1; } else { return middle; } } return -1; } ","date":"2017-09-16","objectID":"/sort-algorithm-compare-and-impl/:7:0","tags":["算法","排序","实现"],"title":"各种排序算法实现方法","uri":"/sort-algorithm-compare-and-impl/"},{"categories":null,"content":"对 2016 年全年的总结，并为未来做一些安排。 年前一天，依旧忙碌 ","date":"2017-01-01","objectID":"/2017.01.01-2016-holiday-summary/:0:0","tags":["总结"],"title":"2016年终总结，未来规划","uri":"/2017.01.01-2016-holiday-summary/"},{"categories":null,"content":"往事 14 年十月至今天，从 0 到 1 的转变。 开始的时候，不断的学习各种软件的使用，像什么 PS 之类的东西玩了个遍，总感觉这些东西都是别人做出来的东西，便生出自己也搞出一些想这类软件。然而继续的瞎搞。 不知什么时候，知道通过学习 c 可以做出来好多有意思的小玩意，比如贪吃蛇，便开始学习高级语言。 之后，知道原来学习计算机，需要系统的学习计算机的理论知识，从此入坑，一发不可收拾。 网易云课堂是自己计算机启蒙课程。跟着上边的课程学了好久，懂了计算机需要如何入门。学了导论、计组、C 语言、算法和数据结构、操作系统、计算机网络、数据库原理等等。自己找视频看了 Linux，读了鸟哥的 linux 书，甚至直接吧自己的电脑系统换成了 Ubuntu/linuxmint，真正体会到 Linux 该如何使用。 假期用 Flask 完成了动态网站，部署到服务器里一段时间，后来因为网站太简单，给撤了。 后来深入 web，用了两个月的时间，把前端学了一下，同时完成了自己第一个网页。学了 Tornado,重构了几次 Project Generator，也算初步掌握全栈开发。同学科研训练，找了一个 Tornado 开源项目，改成了论坛系统，还帮这个项目修了几个 bug。 学了些东西。 ","date":"2017-01-01","objectID":"/2017.01.01-2016-holiday-summary/:1:0","tags":["总结"],"title":"2016年终总结，未来规划","uri":"/2017.01.01-2016-holiday-summary/"},{"categories":null,"content":"如今 如今，趁着假期，也是准备考研前的最后一个可以自由支配的假期，准备了些东西回来学。每天完成一个汇编的项目，( 假期汇编 )。准备好好学学数据结构和算法导论. ","date":"2017-01-01","objectID":"/2017.01.01-2016-holiday-summary/:2:0","tags":["总结"],"title":"2016年终总结，未来规划","uri":"/2017.01.01-2016-holiday-summary/"},{"categories":null,"content":"越努力越迷茫 越努力，知道的越多，不知到的更多。为了选择自己的技术方向，越接触越不知道该学什么，越着急，越急躁。 同时还有一些其他的事让我烦心。慢慢来吧，急不来。 ","date":"2017-01-01","objectID":"/2017.01.01-2016-holiday-summary/:3:0","tags":["总结"],"title":"2016年终总结，未来规划","uri":"/2017.01.01-2016-holiday-summary/"},{"categories":null,"content":"未来 以后该怎么如何进步？ ","date":"2017-01-01","objectID":"/2017.01.01-2016-holiday-summary/:4:0","tags":["总结"],"title":"2016年终总结，未来规划","uri":"/2017.01.01-2016-holiday-summary/"},{"categories":null,"content":"其他 继续学习，共勉！ ","date":"2017-01-01","objectID":"/2017.01.01-2016-holiday-summary/:5:0","tags":["总结"],"title":"2016年终总结，未来规划","uri":"/2017.01.01-2016-holiday-summary/"},{"categories":null,"content":"记录学习Linux操作系统实现时候使用过的资源，一部分笔记，以及调试中用到的技巧。内容有点乱，仅供个人使用。 linux history linux 0.11 linux 0.95 实现虚拟文件系统 linux 0.96 实现网络接口 linux linux采用分段+分页机制结合管理内存 linux 调试方法 linux0.11 调试方法 gdb tools/system target remote localhost:1234 gdb常用命令 b: 下中斷點 info b :u 列出目前中断点，也可简写成\"i b\" continue(c) 继续执行直到下一个中断点或结束 list(l): 列出目前上下文 step(s): 单步 (会进入 funciton) next(n) : 单步 (不会进入 funciton) until(u) 跳离一个 while for 循环 print(p): 显示某变量，如 p str info register(i r) : 显示 CPU 的 register GDB 打印出内存中的內容，格式為 x/nyz，其中 n: 要印出的數量 y: 显示的格式，可为C( char), d(整数), x(hex) z: 单位，可为 b(byte), h(16bit), w(32bit) cgdb 可显示为上半部分为代码，下半部分命令部分 cgdb tools/system* [linux-0.11启动过程描述](http://labrick.cc/2015/08/13/linux-0-11-boot/) * [Linux0.11启动过程](http://linux.chinaunix.net/techdoc/install/2007/04/10/954810.shtml) * [80386保护模式的本质](http://www.jianshu.com/p/1cea7dc5d6b7) * [linux虚拟地址到线性地址的转化](http://luodw.cc/2016/02/17/address/) * [Linux内存寻址之分段机制-linux回避了分段机制](http://blog.xiaohansong.com/2015/10/03/Linux%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6/) * [Linux内存寻址之分页机制/](http://blog.xiaohansong.com/2015/10/05/Linux%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%E4%B9%8B%E5%88%86%E9%A1%B5%E6%9C%BA%E5%88%B6/) * [逻辑地址、线性地址、物理地址和虚拟地址](http://www.voidcn.com/blog/will130/article/p-5705051.html) * [Intel 80386 程序员参考手册](http://www.kancloud.cn/wizardforcel/intel-80386-ref-manual/123838) * [linux0.11内核之文件系统](http://harpsword.leanote.com/post/Untitled-563d6103ab6441584f000164) source 80386内存访问公式 32位线性地址 = 段基地址(32位) + 段内偏移(32位) 48bit = 16 + 32 16位地段选择子 + 32虚拟地址 -\u003e 32线性地址 32线性地址 -\u003e 物理地址 ","date":"2016-10-30","objectID":"/linux0.11-src-code-dev-env/:0:0","tags":["Linux","资源"],"title":"Linux0.11源码学习环境配置与相关资源汇总","uri":"/linux0.11-src-code-dev-env/"},{"categories":null,"content":"bitnami-pg-upgrade This is a PoC for using pg_upgrade inside bitnami/postgres-ha from 14 to 15 – learn from it, adapt it for your needs; don’t expect it to work as-is! (Source for this image is available at https://github.com/exfly/bitnami-pg-upgrade .) ref blog ","date":"0001-01-01","objectID":"/readme/:0:0","tags":null,"title":"","uri":"/readme/"},{"categories":null,"content":"Usage Dependense on kind docker bash test.sh ","date":"0001-01-01","objectID":"/readme/:1:0","tags":null,"title":"","uri":"/readme/"},{"categories":null,"content":"Ref Better documentation needed on major version upgrades ","date":"0001-01-01","objectID":"/readme/:2:0","tags":null,"title":"","uri":"/readme/"}]